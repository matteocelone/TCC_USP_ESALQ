{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd2b9a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "dataset_process = pd.read_csv(\"PCA_banco_3.csv\", delimiter=';')\n",
    "X = dataset_process.loc[: , dataset_process.columns != 'Label'].values\n",
    "y = dataset_process[\"Label\"].values\n",
    "\n",
    "len(dataset_process.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f1cd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principal component 1</th>\n",
       "      <th>principal component 2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-31.03</td>\n",
       "      <td>-4.73</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-29.25</td>\n",
       "      <td>-4.44</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32.15</td>\n",
       "      <td>-4.31</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-24.41</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-29.33</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>-30.43</td>\n",
       "      <td>-4.60</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>-27.58</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>-26.51</td>\n",
       "      <td>-4.09</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>-18.52</td>\n",
       "      <td>-3.14</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>-26.50</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     principal component 1  principal component 2      Label\n",
       "0                   -31.03                  -4.73  BallFault\n",
       "1                   -29.25                  -4.44  BallFault\n",
       "2                   -32.15                  -4.31  BallFault\n",
       "3                   -24.41                  -3.71  BallFault\n",
       "4                   -29.33                  -3.99  BallFault\n",
       "..                     ...                    ...        ...\n",
       "475                 -30.43                  -4.60    Healthy\n",
       "476                 -27.58                  -4.33    Healthy\n",
       "477                 -26.51                  -4.09    Healthy\n",
       "478                 -18.52                  -3.14    Healthy\n",
       "479                 -26.50                  -3.99    Healthy\n",
       "\n",
       "[480 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d460522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f646be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195a1416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6225dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6813ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.13391659\n",
      "Iteration 2, loss = 2.60501090\n",
      "Iteration 3, loss = 2.11099593\n",
      "Iteration 4, loss = 1.65289413\n",
      "Iteration 5, loss = 1.36440416\n",
      "Iteration 6, loss = 1.27093629\n",
      "Iteration 7, loss = 1.26592845\n",
      "Iteration 8, loss = 1.25885554\n",
      "Iteration 9, loss = 1.26103217\n",
      "Iteration 10, loss = 1.24174920\n",
      "Iteration 11, loss = 1.20679058\n",
      "Iteration 12, loss = 1.15784028\n",
      "Iteration 13, loss = 1.10364062\n",
      "Iteration 14, loss = 1.05694549\n",
      "Iteration 15, loss = 0.99555702\n",
      "Iteration 16, loss = 0.95029567\n",
      "Iteration 17, loss = 0.91135136\n",
      "Iteration 18, loss = 0.89735995\n",
      "Iteration 19, loss = 0.89066045\n",
      "Iteration 20, loss = 0.89103231\n",
      "Iteration 21, loss = 0.89209712\n",
      "Iteration 22, loss = 0.87979107\n",
      "Iteration 23, loss = 0.85730034\n",
      "Iteration 24, loss = 0.83838074\n",
      "Iteration 25, loss = 0.82898800\n",
      "Iteration 26, loss = 0.82416180\n",
      "Iteration 27, loss = 0.82109550\n",
      "Iteration 28, loss = 0.81734253\n",
      "Iteration 29, loss = 0.80961030\n",
      "Iteration 30, loss = 0.80466848\n",
      "Iteration 31, loss = 0.79320437\n",
      "Iteration 32, loss = 0.78654253\n",
      "Iteration 33, loss = 0.77991247\n",
      "Iteration 34, loss = 0.77642771\n",
      "Iteration 35, loss = 0.77122561\n",
      "Iteration 36, loss = 0.76401141\n",
      "Iteration 37, loss = 0.75974485\n",
      "Iteration 38, loss = 0.75209827\n",
      "Iteration 39, loss = 0.74538117\n",
      "Iteration 40, loss = 0.74034052\n",
      "Iteration 41, loss = 0.73633997\n",
      "Iteration 42, loss = 0.73069263\n",
      "Iteration 43, loss = 0.72427184\n",
      "Iteration 44, loss = 0.71926150\n",
      "Iteration 45, loss = 0.71548984\n",
      "Iteration 46, loss = 0.71128919\n",
      "Iteration 47, loss = 0.70589531\n",
      "Iteration 48, loss = 0.70388414\n",
      "Iteration 49, loss = 0.69822538\n",
      "Iteration 50, loss = 0.69380744\n",
      "Iteration 51, loss = 0.68955728\n",
      "Iteration 52, loss = 0.68458325\n",
      "Iteration 53, loss = 0.68042409\n",
      "Iteration 54, loss = 0.67666977\n",
      "Iteration 55, loss = 0.67232057\n",
      "Iteration 56, loss = 0.66886090\n",
      "Iteration 57, loss = 0.66534253\n",
      "Iteration 58, loss = 0.66171887\n",
      "Iteration 59, loss = 0.65735934\n",
      "Iteration 60, loss = 0.65509793\n",
      "Iteration 61, loss = 0.65258737\n",
      "Iteration 62, loss = 0.64651054\n",
      "Iteration 63, loss = 0.64327515\n",
      "Iteration 64, loss = 0.64094058\n",
      "Iteration 65, loss = 0.63681091\n",
      "Iteration 66, loss = 0.63327546\n",
      "Iteration 67, loss = 0.62956012\n",
      "Iteration 68, loss = 0.62732893\n",
      "Iteration 69, loss = 0.62352083\n",
      "Iteration 70, loss = 0.61971197\n",
      "Iteration 71, loss = 0.61692510\n",
      "Iteration 72, loss = 0.61477008\n",
      "Iteration 73, loss = 0.61244381\n",
      "Iteration 74, loss = 0.60815563\n",
      "Iteration 75, loss = 0.60526387\n",
      "Iteration 76, loss = 0.60326205\n",
      "Iteration 77, loss = 0.59891039\n",
      "Iteration 78, loss = 0.59571410\n",
      "Iteration 79, loss = 0.59413324\n",
      "Iteration 80, loss = 0.59070194\n",
      "Iteration 81, loss = 0.58764097\n",
      "Iteration 82, loss = 0.58441768\n",
      "Iteration 83, loss = 0.58168323\n",
      "Iteration 84, loss = 0.57947477\n",
      "Iteration 85, loss = 0.57691247\n",
      "Iteration 86, loss = 0.57488720\n",
      "Iteration 87, loss = 0.57151668\n",
      "Iteration 88, loss = 0.56905973\n",
      "Iteration 89, loss = 0.56732349\n",
      "Iteration 90, loss = 0.56460369\n",
      "Iteration 91, loss = 0.56269342\n",
      "Iteration 92, loss = 0.56049779\n",
      "Iteration 93, loss = 0.55979186\n",
      "Iteration 94, loss = 0.55566010\n",
      "Iteration 95, loss = 0.55351999\n",
      "Iteration 96, loss = 0.55190021\n",
      "Iteration 97, loss = 0.54982878\n",
      "Iteration 98, loss = 0.54784909\n",
      "Iteration 99, loss = 0.54631622\n",
      "Iteration 100, loss = 0.54403637\n",
      "Iteration 101, loss = 0.54233087\n",
      "Iteration 102, loss = 0.53979872\n",
      "Iteration 103, loss = 0.53922274\n",
      "Iteration 104, loss = 0.53673356\n",
      "Iteration 105, loss = 0.53505587\n",
      "Iteration 106, loss = 0.53348245\n",
      "Iteration 107, loss = 0.53341667\n",
      "Iteration 108, loss = 0.53015281\n",
      "Iteration 109, loss = 0.52875709\n",
      "Iteration 110, loss = 0.52680268\n",
      "Iteration 111, loss = 0.52606789\n",
      "Iteration 112, loss = 0.52465328\n",
      "Iteration 113, loss = 0.52258985\n",
      "Iteration 114, loss = 0.52196854\n",
      "Iteration 115, loss = 0.51967521\n",
      "Iteration 116, loss = 0.51796522\n",
      "Iteration 117, loss = 0.51642014\n",
      "Iteration 118, loss = 0.51555236\n",
      "Iteration 119, loss = 0.51385927\n",
      "Iteration 120, loss = 0.51339698\n",
      "Iteration 121, loss = 0.51227466\n",
      "Iteration 122, loss = 0.51064069\n",
      "Iteration 123, loss = 0.50987985\n",
      "Iteration 124, loss = 0.50803749\n",
      "Iteration 125, loss = 0.50801007\n",
      "Iteration 126, loss = 0.50649169\n",
      "Iteration 127, loss = 0.50420922\n",
      "Iteration 128, loss = 0.50337289\n",
      "Iteration 129, loss = 0.50212358\n",
      "Iteration 130, loss = 0.50293130\n",
      "Iteration 131, loss = 0.49987863\n",
      "Iteration 132, loss = 0.50015526\n",
      "Iteration 133, loss = 0.49900997\n",
      "Iteration 134, loss = 0.49644497\n",
      "Iteration 135, loss = 0.49547590\n",
      "Iteration 136, loss = 0.49481454\n",
      "Iteration 137, loss = 0.49311991\n",
      "Iteration 138, loss = 0.49435719\n",
      "Iteration 139, loss = 0.49200568\n",
      "Iteration 140, loss = 0.49085634\n",
      "Iteration 141, loss = 0.48975762\n",
      "Iteration 142, loss = 0.48911786\n",
      "Iteration 143, loss = 0.48765196\n",
      "Iteration 144, loss = 0.48769409\n",
      "Iteration 145, loss = 0.48628581\n",
      "Iteration 146, loss = 0.48551171\n",
      "Iteration 147, loss = 0.48440720\n",
      "Iteration 148, loss = 0.48373338\n",
      "Iteration 149, loss = 0.48298493\n",
      "Iteration 150, loss = 0.48163793\n",
      "Iteration 151, loss = 0.48085540\n",
      "Iteration 152, loss = 0.48013362\n",
      "Iteration 153, loss = 0.47930623\n",
      "Iteration 154, loss = 0.47938424\n",
      "Iteration 155, loss = 0.47767095\n",
      "Iteration 156, loss = 0.47725727\n",
      "Iteration 157, loss = 0.47772651\n",
      "Iteration 158, loss = 0.47728602\n",
      "Iteration 159, loss = 0.47519385\n",
      "Iteration 160, loss = 0.47426982\n",
      "Iteration 161, loss = 0.47330552\n",
      "Iteration 162, loss = 0.47599029\n",
      "Iteration 163, loss = 0.47283989\n",
      "Iteration 164, loss = 0.47190570\n",
      "Iteration 165, loss = 0.47197822\n",
      "Iteration 166, loss = 0.47038089\n",
      "Iteration 167, loss = 0.46985066\n",
      "Iteration 168, loss = 0.46979734\n",
      "Iteration 169, loss = 0.46813048\n",
      "Iteration 170, loss = 0.47055850\n",
      "Iteration 171, loss = 0.46813431\n",
      "Iteration 172, loss = 0.46755139\n",
      "Iteration 173, loss = 0.46761112\n",
      "Iteration 174, loss = 0.46888120\n",
      "Iteration 175, loss = 0.46515362\n",
      "Iteration 176, loss = 0.46443829\n",
      "Iteration 177, loss = 0.46396651\n",
      "Iteration 178, loss = 0.46339140\n",
      "Iteration 179, loss = 0.46353939\n",
      "Iteration 180, loss = 0.46219084\n",
      "Iteration 181, loss = 0.46311650\n",
      "Iteration 182, loss = 0.46179667\n",
      "Iteration 183, loss = 0.46070765\n",
      "Iteration 184, loss = 0.46145633\n",
      "Iteration 185, loss = 0.46027711\n",
      "Iteration 186, loss = 0.45882187\n",
      "Iteration 187, loss = 0.45962856\n",
      "Iteration 188, loss = 0.45879171\n",
      "Iteration 189, loss = 0.45808917\n",
      "Iteration 190, loss = 0.45825556\n",
      "Iteration 191, loss = 0.45887716\n",
      "Iteration 192, loss = 0.45640986\n",
      "Iteration 193, loss = 0.45693544\n",
      "Iteration 194, loss = 0.45869545\n",
      "Iteration 195, loss = 0.45531442\n",
      "Iteration 196, loss = 0.45490128\n",
      "Iteration 197, loss = 0.45478711\n",
      "Iteration 198, loss = 0.45389057\n",
      "Iteration 199, loss = 0.45350559\n",
      "Iteration 200, loss = 0.45377101\n",
      "Iteration 201, loss = 0.45434928\n",
      "Iteration 202, loss = 0.45250145\n",
      "Iteration 203, loss = 0.45204664\n",
      "Iteration 204, loss = 0.45223231\n",
      "Iteration 205, loss = 0.45225070\n",
      "Iteration 206, loss = 0.45186050\n",
      "Iteration 207, loss = 0.45074173\n",
      "Iteration 208, loss = 0.45230253\n",
      "Iteration 209, loss = 0.45070966\n",
      "Iteration 210, loss = 0.45031350\n",
      "Iteration 211, loss = 0.44988306\n",
      "Iteration 212, loss = 0.44966281\n",
      "Iteration 213, loss = 0.44893465\n",
      "Iteration 214, loss = 0.44842933\n",
      "Iteration 215, loss = 0.44808445\n",
      "Iteration 216, loss = 0.44834968\n",
      "Iteration 217, loss = 0.44770768\n",
      "Iteration 218, loss = 0.44709385\n",
      "Iteration 219, loss = 0.44788413\n",
      "Iteration 220, loss = 0.44816158\n",
      "Iteration 221, loss = 0.44629441\n",
      "Iteration 222, loss = 0.44658812\n",
      "Iteration 223, loss = 0.44617848\n",
      "Iteration 224, loss = 0.44721006\n",
      "Iteration 225, loss = 0.44503229\n",
      "Iteration 226, loss = 0.44511269\n",
      "Iteration 227, loss = 0.44470666\n",
      "Iteration 228, loss = 0.44431930\n",
      "Iteration 229, loss = 0.44388841\n",
      "Iteration 230, loss = 0.44369063\n",
      "Iteration 231, loss = 0.44334613\n",
      "Iteration 232, loss = 0.44322602\n",
      "Iteration 233, loss = 0.44299431\n",
      "Iteration 234, loss = 0.44333072\n",
      "Iteration 235, loss = 0.44290221\n",
      "Iteration 236, loss = 0.44192732\n",
      "Iteration 237, loss = 0.44185723\n",
      "Iteration 238, loss = 0.44222025\n",
      "Iteration 239, loss = 0.44130315\n",
      "Iteration 240, loss = 0.44140069\n",
      "Iteration 241, loss = 0.44191080\n",
      "Iteration 242, loss = 0.44133272\n",
      "Iteration 243, loss = 0.44104961\n",
      "Iteration 244, loss = 0.44071069\n",
      "Iteration 245, loss = 0.43948821\n",
      "Iteration 246, loss = 0.43992652\n",
      "Iteration 247, loss = 0.44000166\n",
      "Iteration 248, loss = 0.43947898\n",
      "Iteration 249, loss = 0.43894886\n",
      "Iteration 250, loss = 0.44085592\n",
      "Iteration 251, loss = 0.43962629\n",
      "Iteration 252, loss = 0.43874131\n",
      "Iteration 253, loss = 0.43862949\n",
      "Iteration 254, loss = 0.43864123\n",
      "Iteration 255, loss = 0.43800620\n",
      "Iteration 256, loss = 0.43858369\n",
      "Iteration 257, loss = 0.43711119\n",
      "Iteration 258, loss = 0.43752357\n",
      "Iteration 259, loss = 0.43689700\n",
      "Iteration 260, loss = 0.43701023\n",
      "Iteration 261, loss = 0.43682624\n",
      "Iteration 262, loss = 0.43695409\n",
      "Iteration 263, loss = 0.43617787\n",
      "Iteration 264, loss = 0.43639957\n",
      "Iteration 265, loss = 0.43610283\n",
      "Iteration 266, loss = 0.43599870\n",
      "Iteration 267, loss = 0.43545674\n",
      "Iteration 268, loss = 0.43655957\n",
      "Iteration 269, loss = 0.43605545\n",
      "Iteration 270, loss = 0.43656597\n",
      "Iteration 271, loss = 0.43829578\n",
      "Iteration 272, loss = 0.43637952\n",
      "Iteration 273, loss = 0.43529638\n",
      "Iteration 274, loss = 0.43514002\n",
      "Iteration 275, loss = 0.43653686\n",
      "Iteration 276, loss = 0.43508531\n",
      "Iteration 277, loss = 0.43422110\n",
      "Iteration 278, loss = 0.43386685\n",
      "Iteration 279, loss = 0.43454945\n",
      "Iteration 280, loss = 0.43366252\n",
      "Iteration 281, loss = 0.43340437\n",
      "Iteration 282, loss = 0.43396959\n",
      "Iteration 283, loss = 0.43266613\n",
      "Iteration 284, loss = 0.43266351\n",
      "Iteration 285, loss = 0.43405701\n",
      "Iteration 286, loss = 0.43598770\n",
      "Iteration 287, loss = 0.43311216\n",
      "Iteration 288, loss = 0.43201894\n",
      "Iteration 289, loss = 0.43277047\n",
      "Iteration 290, loss = 0.43280462\n",
      "Iteration 291, loss = 0.43285565\n",
      "Iteration 292, loss = 0.43153524\n",
      "Iteration 293, loss = 0.43155137\n",
      "Iteration 294, loss = 0.43271660\n",
      "Iteration 295, loss = 0.43188406\n",
      "Iteration 296, loss = 0.43123192\n",
      "Iteration 297, loss = 0.43206380\n",
      "Iteration 298, loss = 0.43086244\n",
      "Iteration 299, loss = 0.43052360\n",
      "Iteration 300, loss = 0.43079898\n",
      "Iteration 301, loss = 0.43027262\n",
      "Iteration 302, loss = 0.42986695\n",
      "Iteration 303, loss = 0.43026208\n",
      "Iteration 304, loss = 0.42953730\n",
      "Iteration 305, loss = 0.42969549\n",
      "Iteration 306, loss = 0.43003561\n",
      "Iteration 307, loss = 0.42922503\n",
      "Iteration 308, loss = 0.42960683\n",
      "Iteration 309, loss = 0.42995303\n",
      "Iteration 310, loss = 0.42906051\n",
      "Iteration 311, loss = 0.42858627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 312, loss = 0.42870255\n",
      "Iteration 313, loss = 0.42885560\n",
      "Iteration 314, loss = 0.42852910\n",
      "Iteration 315, loss = 0.42908476\n",
      "Iteration 316, loss = 0.42854021\n",
      "Iteration 317, loss = 0.42792029\n",
      "Iteration 318, loss = 0.42875326\n",
      "Iteration 319, loss = 0.42795834\n",
      "Iteration 320, loss = 0.42801398\n",
      "Iteration 321, loss = 0.42743208\n",
      "Iteration 322, loss = 0.42743423\n",
      "Iteration 323, loss = 0.42731727\n",
      "Iteration 324, loss = 0.42754268\n",
      "Iteration 325, loss = 0.42753793\n",
      "Iteration 326, loss = 0.42733506\n",
      "Iteration 327, loss = 0.42688441\n",
      "Iteration 328, loss = 0.42731353\n",
      "Iteration 329, loss = 0.42751485\n",
      "Iteration 330, loss = 0.42649670\n",
      "Iteration 331, loss = 0.42701803\n",
      "Iteration 332, loss = 0.42622768\n",
      "Iteration 333, loss = 0.42709057\n",
      "Iteration 334, loss = 0.42674392\n",
      "Iteration 335, loss = 0.42609566\n",
      "Iteration 336, loss = 0.42668689\n",
      "Iteration 337, loss = 0.42560640\n",
      "Iteration 338, loss = 0.42655298\n",
      "Iteration 339, loss = 0.42634915\n",
      "Iteration 340, loss = 0.42536901\n",
      "Iteration 341, loss = 0.42517242\n",
      "Iteration 342, loss = 0.42528518\n",
      "Iteration 343, loss = 0.42686408\n",
      "Iteration 344, loss = 0.42468687\n",
      "Iteration 345, loss = 0.42528783\n",
      "Iteration 346, loss = 0.42560372\n",
      "Iteration 347, loss = 0.42452007\n",
      "Iteration 348, loss = 0.42556501\n",
      "Iteration 349, loss = 0.42558520\n",
      "Iteration 350, loss = 0.42402557\n",
      "Iteration 351, loss = 0.42758081\n",
      "Iteration 352, loss = 0.42670514\n",
      "Iteration 353, loss = 0.42415898\n",
      "Iteration 354, loss = 0.42845278\n",
      "Iteration 355, loss = 0.42538319\n",
      "Iteration 356, loss = 0.42289380\n",
      "Iteration 357, loss = 0.42613020\n",
      "Iteration 358, loss = 0.42501699\n",
      "Iteration 359, loss = 0.42282322\n",
      "Iteration 360, loss = 0.42433337\n",
      "Iteration 361, loss = 0.42433099\n",
      "Iteration 362, loss = 0.42361950\n",
      "Iteration 363, loss = 0.42406703\n",
      "Iteration 364, loss = 0.42369437\n",
      "Iteration 365, loss = 0.42235832\n",
      "Iteration 366, loss = 0.42373474\n",
      "Iteration 367, loss = 0.42375773\n",
      "Iteration 368, loss = 0.42260320\n",
      "Iteration 369, loss = 0.42329227\n",
      "Iteration 370, loss = 0.42313863\n",
      "Iteration 371, loss = 0.42310526\n",
      "Iteration 372, loss = 0.42220687\n",
      "Iteration 373, loss = 0.42253377\n",
      "Iteration 374, loss = 0.42216331\n",
      "Iteration 375, loss = 0.42198455\n",
      "Iteration 376, loss = 0.42225820\n",
      "Iteration 377, loss = 0.42316821\n",
      "Iteration 378, loss = 0.42412033\n",
      "Iteration 379, loss = 0.42166600\n",
      "Iteration 380, loss = 0.42218691\n",
      "Iteration 381, loss = 0.42306793\n",
      "Iteration 382, loss = 0.42156156\n",
      "Iteration 383, loss = 0.42201857\n",
      "Iteration 384, loss = 0.42266889\n",
      "Iteration 385, loss = 0.42061365\n",
      "Iteration 386, loss = 0.42323812\n",
      "Iteration 387, loss = 0.42419866\n",
      "Iteration 388, loss = 0.42160225\n",
      "Iteration 389, loss = 0.42611888\n",
      "Iteration 390, loss = 0.42294700\n",
      "Iteration 391, loss = 0.42278066\n",
      "Iteration 392, loss = 0.42343491\n",
      "Iteration 393, loss = 0.42084596\n",
      "Iteration 394, loss = 0.42023802\n",
      "Iteration 395, loss = 0.42231875\n",
      "Iteration 396, loss = 0.42272853\n",
      "Iteration 397, loss = 0.42043136\n",
      "Iteration 398, loss = 0.42186419\n",
      "Iteration 399, loss = 0.42063060\n",
      "Iteration 400, loss = 0.41987136\n",
      "Iteration 401, loss = 0.41959441\n",
      "Iteration 402, loss = 0.41963505\n",
      "Iteration 403, loss = 0.42017312\n",
      "Iteration 404, loss = 0.42059039\n",
      "Iteration 405, loss = 0.41950539\n",
      "Iteration 406, loss = 0.41916200\n",
      "Iteration 407, loss = 0.41956131\n",
      "Iteration 408, loss = 0.42169926\n",
      "Iteration 409, loss = 0.41935878\n",
      "Iteration 410, loss = 0.41941624\n",
      "Iteration 411, loss = 0.42040543\n",
      "Iteration 412, loss = 0.42017467\n",
      "Iteration 413, loss = 0.41887408\n",
      "Iteration 414, loss = 0.41862778\n",
      "Iteration 415, loss = 0.41878387\n",
      "Iteration 416, loss = 0.41867589\n",
      "Iteration 417, loss = 0.41921039\n",
      "Iteration 418, loss = 0.41820532\n",
      "Iteration 419, loss = 0.41863537\n",
      "Iteration 420, loss = 0.41880574\n",
      "Iteration 421, loss = 0.41840451\n",
      "Iteration 422, loss = 0.41933487\n",
      "Iteration 423, loss = 0.41799524\n",
      "Iteration 424, loss = 0.41824047\n",
      "Iteration 425, loss = 0.41882012\n",
      "Iteration 426, loss = 0.41789903\n",
      "Iteration 427, loss = 0.41786719\n",
      "Iteration 428, loss = 0.41787944\n",
      "Iteration 429, loss = 0.41780663\n",
      "Iteration 430, loss = 0.41829304\n",
      "Iteration 431, loss = 0.41759908\n",
      "Iteration 432, loss = 0.41786240\n",
      "Iteration 433, loss = 0.41723508\n",
      "Iteration 434, loss = 0.41730144\n",
      "Iteration 435, loss = 0.41739562\n",
      "Iteration 436, loss = 0.41721325\n",
      "Iteration 437, loss = 0.41691301\n",
      "Iteration 438, loss = 0.41792209\n",
      "Iteration 439, loss = 0.41732454\n",
      "Iteration 440, loss = 0.41723772\n",
      "Iteration 441, loss = 0.41757473\n",
      "Iteration 442, loss = 0.41661573\n",
      "Iteration 443, loss = 0.41763711\n",
      "Iteration 444, loss = 0.41691901\n",
      "Iteration 445, loss = 0.41642690\n",
      "Iteration 446, loss = 0.41686479\n",
      "Iteration 447, loss = 0.42195910\n",
      "Iteration 448, loss = 0.41763141\n",
      "Iteration 449, loss = 0.41764265\n",
      "Iteration 450, loss = 0.41808883\n",
      "Iteration 451, loss = 0.41793853\n",
      "Iteration 452, loss = 0.41685916\n",
      "Iteration 453, loss = 0.41643638\n",
      "Iteration 454, loss = 0.41616801\n",
      "Iteration 455, loss = 0.41617121\n",
      "Iteration 456, loss = 0.41578577\n",
      "Iteration 457, loss = 0.41659711\n",
      "Iteration 458, loss = 0.41617664\n",
      "Iteration 459, loss = 0.41671114\n",
      "Iteration 460, loss = 0.41613736\n",
      "Iteration 461, loss = 0.41613204\n",
      "Iteration 462, loss = 0.41524943\n",
      "Iteration 463, loss = 0.41561147\n",
      "Iteration 464, loss = 0.41581611\n",
      "Iteration 465, loss = 0.41494832\n",
      "Iteration 466, loss = 0.41788815\n",
      "Iteration 467, loss = 0.41652143\n",
      "Iteration 468, loss = 0.41583135\n",
      "Iteration 469, loss = 0.41597607\n",
      "Iteration 470, loss = 0.41477899\n",
      "Iteration 471, loss = 0.41512950\n",
      "Iteration 472, loss = 0.41735338\n",
      "Iteration 473, loss = 0.41462002\n",
      "Iteration 474, loss = 0.41448021\n",
      "Iteration 475, loss = 0.41478875\n",
      "Iteration 476, loss = 0.41441493\n",
      "Iteration 477, loss = 0.41450011\n",
      "Iteration 478, loss = 0.41540991\n",
      "Iteration 479, loss = 0.41499033\n",
      "Iteration 480, loss = 0.41483402\n",
      "Iteration 481, loss = 0.41524098\n",
      "Iteration 482, loss = 0.41457507\n",
      "Iteration 483, loss = 0.41463356\n",
      "Iteration 484, loss = 0.41500830\n",
      "Iteration 485, loss = 0.41430758\n",
      "Iteration 486, loss = 0.41409920\n",
      "Iteration 487, loss = 0.41440196\n",
      "Iteration 488, loss = 0.41392583\n",
      "Iteration 489, loss = 0.41376758\n",
      "Iteration 490, loss = 0.41426410\n",
      "Iteration 491, loss = 0.41361764\n",
      "Iteration 492, loss = 0.41386988\n",
      "Iteration 493, loss = 0.41410906\n",
      "Iteration 494, loss = 0.41350104\n",
      "Iteration 495, loss = 0.41351821\n",
      "Iteration 496, loss = 0.41351327\n",
      "Iteration 497, loss = 0.41426295\n",
      "Iteration 498, loss = 0.41324680\n",
      "Iteration 499, loss = 0.41481262\n",
      "Iteration 500, loss = 0.41298316\n",
      "Iteration 501, loss = 0.41291827\n",
      "Iteration 502, loss = 0.41334189\n",
      "Iteration 503, loss = 0.41334814\n",
      "Iteration 504, loss = 0.41518120\n",
      "Iteration 505, loss = 0.41502500\n",
      "Iteration 506, loss = 0.41241139\n",
      "Iteration 507, loss = 0.41387843\n",
      "Iteration 508, loss = 0.41291151\n",
      "Iteration 509, loss = 0.41206114\n",
      "Iteration 510, loss = 0.41412467\n",
      "Iteration 511, loss = 0.41313236\n",
      "Iteration 512, loss = 0.41246804\n",
      "Iteration 513, loss = 0.41506585\n",
      "Iteration 514, loss = 0.41301279\n",
      "Iteration 515, loss = 0.41195526\n",
      "Iteration 516, loss = 0.41265856\n",
      "Iteration 517, loss = 0.41319142\n",
      "Iteration 518, loss = 0.41165902\n",
      "Iteration 519, loss = 0.41310843\n",
      "Iteration 520, loss = 0.41254156\n",
      "Iteration 521, loss = 0.41236750\n",
      "Iteration 522, loss = 0.41242138\n",
      "Iteration 523, loss = 0.41171588\n",
      "Iteration 524, loss = 0.41316953\n",
      "Iteration 525, loss = 0.41225772\n",
      "Iteration 526, loss = 0.41225332\n",
      "Iteration 527, loss = 0.41546053\n",
      "Iteration 528, loss = 0.41271154\n",
      "Iteration 529, loss = 0.41103218\n",
      "Iteration 530, loss = 0.41407175\n",
      "Iteration 531, loss = 0.41271650\n",
      "Iteration 532, loss = 0.41138559\n",
      "Iteration 533, loss = 0.41181869\n",
      "Iteration 534, loss = 0.41082041\n",
      "Iteration 535, loss = 0.41197074\n",
      "Iteration 536, loss = 0.41394320\n",
      "Iteration 537, loss = 0.41192968\n",
      "Iteration 538, loss = 0.41135281\n",
      "Iteration 539, loss = 0.41139908\n",
      "Iteration 540, loss = 0.41127841\n",
      "Iteration 541, loss = 0.41135352\n",
      "Iteration 542, loss = 0.41101025\n",
      "Iteration 543, loss = 0.41147292\n",
      "Iteration 544, loss = 0.41087195\n",
      "Iteration 545, loss = 0.41082958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "rede_neural=MLPClassifier(max_iter=1000, verbose=True,tol=0.0001, activation='relu')\n",
    "rede_neural.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48aa1bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84375"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ea3b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  7,  1,  0],\n",
       "       [ 5, 19,  0,  0],\n",
       "       [ 0,  0, 16,  0],\n",
       "       [ 0,  2,  0, 23]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = rede_neural.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c9f33c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(42.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize=(7,5))\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfc066a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   BallFault       0.82      0.74      0.78        31\n",
      "     Healthy       0.68      0.79      0.73        24\n",
      "  InnerFault       0.94      1.00      0.97        16\n",
      "  OuterFault       1.00      0.92      0.96        25\n",
      "\n",
      "    accuracy                           0.84        96\n",
      "   macro avg       0.86      0.86      0.86        96\n",
      "weighted avg       0.85      0.84      0.85        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "966812a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OuterFault'], dtype='<U10')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural.predict([[-40,-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f2ccfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = dataset_process[\"Label\"]='Healthy'\n",
    "df1 = dataset_process[\"Label\"]='Fault'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d725bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25596/1762625770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#plt.scatter(X[:,0],X[:,2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'principal component 1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"green\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'principal component 1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"blue\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "#plt.scatter(X[:,0],X[:,2])\n",
    "plt.scatter(df0['principal component 1'], df0['Label'],color=\"green\",marker='+')\n",
    "plt.scatter(df1['principal component 1'], df1['Label'],color=\"blue\",marker='.')\n",
    "plt.ylabel('Label')\n",
    "plt.xlabel('principle component 1')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f0d4bd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "dataset_process = pd.read_csv(\"PCA_banco_3_normalizado_2_classes.csv\", delimiter=';')\n",
    "X = dataset_process.loc[: , dataset_process.columns != 'Label'].values\n",
    "y = dataset_process[\"Label\"].values\n",
    "\n",
    "len(dataset_process.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0400b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>principal component 1</th>\n",
       "      <th>principal component 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fault</td>\n",
       "      <td>-0.122303</td>\n",
       "      <td>-0.018642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fault</td>\n",
       "      <td>-0.115301</td>\n",
       "      <td>-0.017505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fault</td>\n",
       "      <td>-0.126719</td>\n",
       "      <td>-0.016976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fault</td>\n",
       "      <td>-0.096206</td>\n",
       "      <td>-0.014626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fault</td>\n",
       "      <td>-0.115627</td>\n",
       "      <td>-0.015725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Healthy</td>\n",
       "      <td>-0.119933</td>\n",
       "      <td>-0.018148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Healthy</td>\n",
       "      <td>-0.108706</td>\n",
       "      <td>-0.017053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Healthy</td>\n",
       "      <td>-0.104510</td>\n",
       "      <td>-0.016105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Healthy</td>\n",
       "      <td>-0.072984</td>\n",
       "      <td>-0.012387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Healthy</td>\n",
       "      <td>-0.104464</td>\n",
       "      <td>-0.015714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label  principal component 1  principal component 2\n",
       "0      Fault              -0.122303              -0.018642\n",
       "1      Fault              -0.115301              -0.017505\n",
       "2      Fault              -0.126719              -0.016976\n",
       "3      Fault              -0.096206              -0.014626\n",
       "4      Fault              -0.115627              -0.015725\n",
       "..       ...                    ...                    ...\n",
       "475  Healthy              -0.119933              -0.018148\n",
       "476  Healthy              -0.108706              -0.017053\n",
       "477  Healthy              -0.104510              -0.016105\n",
       "478  Healthy              -0.072984              -0.012387\n",
       "479  Healthy              -0.104464              -0.015714\n",
       "\n",
       "[480 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b75c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc2d3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b0a050a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f05ed9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64968646\n",
      "Iteration 2, loss = 0.64491965\n",
      "Iteration 3, loss = 0.63994788\n",
      "Iteration 4, loss = 0.63521658\n",
      "Iteration 5, loss = 0.63073016\n",
      "Iteration 6, loss = 0.62639921\n",
      "Iteration 7, loss = 0.62213087\n",
      "Iteration 8, loss = 0.61816410\n",
      "Iteration 9, loss = 0.61439175\n",
      "Iteration 10, loss = 0.61038618\n",
      "Iteration 11, loss = 0.60678004\n",
      "Iteration 12, loss = 0.60325805\n",
      "Iteration 13, loss = 0.59997850\n",
      "Iteration 14, loss = 0.59666649\n",
      "Iteration 15, loss = 0.59375669\n",
      "Iteration 16, loss = 0.59053873\n",
      "Iteration 17, loss = 0.58817042\n",
      "Iteration 18, loss = 0.58506688\n",
      "Iteration 19, loss = 0.58257217\n",
      "Iteration 20, loss = 0.58006687\n",
      "Iteration 21, loss = 0.57747517\n",
      "Iteration 22, loss = 0.57523746\n",
      "Iteration 23, loss = 0.57319808\n",
      "Iteration 24, loss = 0.57091605\n",
      "Iteration 25, loss = 0.56887284\n",
      "Iteration 26, loss = 0.56715841\n",
      "Iteration 27, loss = 0.56507083\n",
      "Iteration 28, loss = 0.56339462\n",
      "Iteration 29, loss = 0.56169847\n",
      "Iteration 30, loss = 0.56003593\n",
      "Iteration 31, loss = 0.55845268\n",
      "Iteration 32, loss = 0.55691889\n",
      "Iteration 33, loss = 0.55567106\n",
      "Iteration 34, loss = 0.55417160\n",
      "Iteration 35, loss = 0.55274613\n",
      "Iteration 36, loss = 0.55155605\n",
      "Iteration 37, loss = 0.55045688\n",
      "Iteration 38, loss = 0.54927601\n",
      "Iteration 39, loss = 0.54824397\n",
      "Iteration 40, loss = 0.54735965\n",
      "Iteration 41, loss = 0.54635192\n",
      "Iteration 42, loss = 0.54531172\n",
      "Iteration 43, loss = 0.54444534\n",
      "Iteration 44, loss = 0.54354225\n",
      "Iteration 45, loss = 0.54264146\n",
      "Iteration 46, loss = 0.54191207\n",
      "Iteration 47, loss = 0.54104390\n",
      "Iteration 48, loss = 0.54014904\n",
      "Iteration 49, loss = 0.53933890\n",
      "Iteration 50, loss = 0.53858274\n",
      "Iteration 51, loss = 0.53775376\n",
      "Iteration 52, loss = 0.53700424\n",
      "Iteration 53, loss = 0.53618922\n",
      "Iteration 54, loss = 0.53544783\n",
      "Iteration 55, loss = 0.53458431\n",
      "Iteration 56, loss = 0.53371523\n",
      "Iteration 57, loss = 0.53282638\n",
      "Iteration 58, loss = 0.53196473\n",
      "Iteration 59, loss = 0.53106493\n",
      "Iteration 60, loss = 0.53015809\n",
      "Iteration 61, loss = 0.52920183\n",
      "Iteration 62, loss = 0.52826212\n",
      "Iteration 63, loss = 0.52730753\n",
      "Iteration 64, loss = 0.52632201\n",
      "Iteration 65, loss = 0.52533566\n",
      "Iteration 66, loss = 0.52433923\n",
      "Iteration 67, loss = 0.52330149\n",
      "Iteration 68, loss = 0.52232215\n",
      "Iteration 69, loss = 0.52128539\n",
      "Iteration 70, loss = 0.52027590\n",
      "Iteration 71, loss = 0.51926936\n",
      "Iteration 72, loss = 0.51828377\n",
      "Iteration 73, loss = 0.51729934\n",
      "Iteration 74, loss = 0.51636272\n",
      "Iteration 75, loss = 0.51546591\n",
      "Iteration 76, loss = 0.51458569\n",
      "Iteration 77, loss = 0.51375059\n",
      "Iteration 78, loss = 0.51291778\n",
      "Iteration 79, loss = 0.51208623\n",
      "Iteration 80, loss = 0.51126017\n",
      "Iteration 81, loss = 0.51043879\n",
      "Iteration 82, loss = 0.50963002\n",
      "Iteration 83, loss = 0.50880441\n",
      "Iteration 84, loss = 0.50801588\n",
      "Iteration 85, loss = 0.50717130\n",
      "Iteration 86, loss = 0.50639032\n",
      "Iteration 87, loss = 0.50557969\n",
      "Iteration 88, loss = 0.50475353\n",
      "Iteration 89, loss = 0.50397627\n",
      "Iteration 90, loss = 0.50313189\n",
      "Iteration 91, loss = 0.50237530\n",
      "Iteration 92, loss = 0.50153873\n",
      "Iteration 93, loss = 0.50075189\n",
      "Iteration 94, loss = 0.49994655\n",
      "Iteration 95, loss = 0.49914797\n",
      "Iteration 96, loss = 0.49833963\n",
      "Iteration 97, loss = 0.49755694\n",
      "Iteration 98, loss = 0.49675441\n",
      "Iteration 99, loss = 0.49600090\n",
      "Iteration 100, loss = 0.49518961\n",
      "Iteration 101, loss = 0.49440183\n",
      "Iteration 102, loss = 0.49364933\n",
      "Iteration 103, loss = 0.49284732\n",
      "Iteration 104, loss = 0.49203844\n",
      "Iteration 105, loss = 0.49128533\n",
      "Iteration 106, loss = 0.49049043\n",
      "Iteration 107, loss = 0.48972486\n",
      "Iteration 108, loss = 0.48893548\n",
      "Iteration 109, loss = 0.48816606\n",
      "Iteration 110, loss = 0.48735942\n",
      "Iteration 111, loss = 0.48662267\n",
      "Iteration 112, loss = 0.48584492\n",
      "Iteration 113, loss = 0.48503792\n",
      "Iteration 114, loss = 0.48428886\n",
      "Iteration 115, loss = 0.48353226\n",
      "Iteration 116, loss = 0.48278742\n",
      "Iteration 117, loss = 0.48199691\n",
      "Iteration 118, loss = 0.48121811\n",
      "Iteration 119, loss = 0.48048244\n",
      "Iteration 120, loss = 0.47970298\n",
      "Iteration 121, loss = 0.47895902\n",
      "Iteration 122, loss = 0.47821012\n",
      "Iteration 123, loss = 0.47758224\n",
      "Iteration 124, loss = 0.47669784\n",
      "Iteration 125, loss = 0.47597822\n",
      "Iteration 126, loss = 0.47522842\n",
      "Iteration 127, loss = 0.47449270\n",
      "Iteration 128, loss = 0.47375342\n",
      "Iteration 129, loss = 0.47300421\n",
      "Iteration 130, loss = 0.47228455\n",
      "Iteration 131, loss = 0.47153452\n",
      "Iteration 132, loss = 0.47083231\n",
      "Iteration 133, loss = 0.47009191\n",
      "Iteration 134, loss = 0.46935498\n",
      "Iteration 135, loss = 0.46863665\n",
      "Iteration 136, loss = 0.46799200\n",
      "Iteration 137, loss = 0.46728676\n",
      "Iteration 138, loss = 0.46647847\n",
      "Iteration 139, loss = 0.46578281\n",
      "Iteration 140, loss = 0.46507545\n",
      "Iteration 141, loss = 0.46438621\n",
      "Iteration 142, loss = 0.46366264\n",
      "Iteration 143, loss = 0.46303627\n",
      "Iteration 144, loss = 0.46229582\n",
      "Iteration 145, loss = 0.46159673\n",
      "Iteration 146, loss = 0.46088129\n",
      "Iteration 147, loss = 0.46022364\n",
      "Iteration 148, loss = 0.45953021\n",
      "Iteration 149, loss = 0.45887619\n",
      "Iteration 150, loss = 0.45821010\n",
      "Iteration 151, loss = 0.45751859\n",
      "Iteration 152, loss = 0.45686365\n",
      "Iteration 153, loss = 0.45626042\n",
      "Iteration 154, loss = 0.45559839\n",
      "Iteration 155, loss = 0.45497398\n",
      "Iteration 156, loss = 0.45422255\n",
      "Iteration 157, loss = 0.45357937\n",
      "Iteration 158, loss = 0.45305264\n",
      "Iteration 159, loss = 0.45229647\n",
      "Iteration 160, loss = 0.45164608\n",
      "Iteration 161, loss = 0.45107183\n",
      "Iteration 162, loss = 0.45040445\n",
      "Iteration 163, loss = 0.44975989\n",
      "Iteration 164, loss = 0.44912172\n",
      "Iteration 165, loss = 0.44852724\n",
      "Iteration 166, loss = 0.44790384\n",
      "Iteration 167, loss = 0.44725175\n",
      "Iteration 168, loss = 0.44666042\n",
      "Iteration 169, loss = 0.44603675\n",
      "Iteration 170, loss = 0.44545142\n",
      "Iteration 171, loss = 0.44481412\n",
      "Iteration 172, loss = 0.44421371\n",
      "Iteration 173, loss = 0.44360095\n",
      "Iteration 174, loss = 0.44303839\n",
      "Iteration 175, loss = 0.44241567\n",
      "Iteration 176, loss = 0.44191798\n",
      "Iteration 177, loss = 0.44123906\n",
      "Iteration 178, loss = 0.44064423\n",
      "Iteration 179, loss = 0.44008333\n",
      "Iteration 180, loss = 0.43950175\n",
      "Iteration 181, loss = 0.43892859\n",
      "Iteration 182, loss = 0.43835090\n",
      "Iteration 183, loss = 0.43779174\n",
      "Iteration 184, loss = 0.43726622\n",
      "Iteration 185, loss = 0.43667603\n",
      "Iteration 186, loss = 0.43610723\n",
      "Iteration 187, loss = 0.43556461\n",
      "Iteration 188, loss = 0.43502058\n",
      "Iteration 189, loss = 0.43461364\n",
      "Iteration 190, loss = 0.43392848\n",
      "Iteration 191, loss = 0.43338926\n",
      "Iteration 192, loss = 0.43286518\n",
      "Iteration 193, loss = 0.43232768\n",
      "Iteration 194, loss = 0.43177360\n",
      "Iteration 195, loss = 0.43130709\n",
      "Iteration 196, loss = 0.43075595\n",
      "Iteration 197, loss = 0.43027281\n",
      "Iteration 198, loss = 0.42970717\n",
      "Iteration 199, loss = 0.42921293\n",
      "Iteration 200, loss = 0.42866570\n",
      "Iteration 201, loss = 0.42819640\n",
      "Iteration 202, loss = 0.42768525\n",
      "Iteration 203, loss = 0.42724003\n",
      "Iteration 204, loss = 0.42670436\n",
      "Iteration 205, loss = 0.42621761\n",
      "Iteration 206, loss = 0.42574705\n",
      "Iteration 207, loss = 0.42524996\n",
      "Iteration 208, loss = 0.42474794\n",
      "Iteration 209, loss = 0.42428326\n",
      "Iteration 210, loss = 0.42382831\n",
      "Iteration 211, loss = 0.42335519\n",
      "Iteration 212, loss = 0.42289298\n",
      "Iteration 213, loss = 0.42241070\n",
      "Iteration 214, loss = 0.42193221\n",
      "Iteration 215, loss = 0.42146812\n",
      "Iteration 216, loss = 0.42102084\n",
      "Iteration 217, loss = 0.42055870\n",
      "Iteration 218, loss = 0.42013818\n",
      "Iteration 219, loss = 0.41967040\n",
      "Iteration 220, loss = 0.41923642\n",
      "Iteration 221, loss = 0.41879938\n",
      "Iteration 222, loss = 0.41835015\n",
      "Iteration 223, loss = 0.41793132\n",
      "Iteration 224, loss = 0.41750692\n",
      "Iteration 225, loss = 0.41710609\n",
      "Iteration 226, loss = 0.41667609\n",
      "Iteration 227, loss = 0.41625492\n",
      "Iteration 228, loss = 0.41583608\n",
      "Iteration 229, loss = 0.41540213\n",
      "Iteration 230, loss = 0.41501047\n",
      "Iteration 231, loss = 0.41461604\n",
      "Iteration 232, loss = 0.41421428\n",
      "Iteration 233, loss = 0.41385771\n",
      "Iteration 234, loss = 0.41340812\n",
      "Iteration 235, loss = 0.41301963\n",
      "Iteration 236, loss = 0.41263708\n",
      "Iteration 237, loss = 0.41225634\n",
      "Iteration 238, loss = 0.41185930\n",
      "Iteration 239, loss = 0.41151732\n",
      "Iteration 240, loss = 0.41111517\n",
      "Iteration 241, loss = 0.41075295\n",
      "Iteration 242, loss = 0.41039198\n",
      "Iteration 243, loss = 0.41002014\n",
      "Iteration 244, loss = 0.40963038\n",
      "Iteration 245, loss = 0.40927391\n",
      "Iteration 246, loss = 0.40893776\n",
      "Iteration 247, loss = 0.40858680\n",
      "Iteration 248, loss = 0.40822666\n",
      "Iteration 249, loss = 0.40804361\n",
      "Iteration 250, loss = 0.40755202\n",
      "Iteration 251, loss = 0.40721728\n",
      "Iteration 252, loss = 0.40698127\n",
      "Iteration 253, loss = 0.40658684\n",
      "Iteration 254, loss = 0.40622415\n",
      "Iteration 255, loss = 0.40589231\n",
      "Iteration 256, loss = 0.40560095\n",
      "Iteration 257, loss = 0.40524712\n",
      "Iteration 258, loss = 0.40494086\n",
      "Iteration 259, loss = 0.40462091\n",
      "Iteration 260, loss = 0.40430876\n",
      "Iteration 261, loss = 0.40399533\n",
      "Iteration 262, loss = 0.40371030\n",
      "Iteration 263, loss = 0.40339063\n",
      "Iteration 264, loss = 0.40309089\n",
      "Iteration 265, loss = 0.40288062\n",
      "Iteration 266, loss = 0.40250172\n",
      "Iteration 267, loss = 0.40220673\n",
      "Iteration 268, loss = 0.40190552\n",
      "Iteration 269, loss = 0.40163656\n",
      "Iteration 270, loss = 0.40135893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 271, loss = 0.40106690\n",
      "Iteration 272, loss = 0.40079344\n",
      "Iteration 273, loss = 0.40053254\n",
      "Iteration 274, loss = 0.40022789\n",
      "Iteration 275, loss = 0.39995200\n",
      "Iteration 276, loss = 0.39968018\n",
      "Iteration 277, loss = 0.39941998\n",
      "Iteration 278, loss = 0.39917761\n",
      "Iteration 279, loss = 0.39892326\n",
      "Iteration 280, loss = 0.39862513\n",
      "Iteration 281, loss = 0.39838587\n",
      "Iteration 282, loss = 0.39815502\n",
      "Iteration 283, loss = 0.39788135\n",
      "Iteration 284, loss = 0.39773747\n",
      "Iteration 285, loss = 0.39734321\n",
      "Iteration 286, loss = 0.39711992\n",
      "Iteration 287, loss = 0.39693825\n",
      "Iteration 288, loss = 0.39664443\n",
      "Iteration 289, loss = 0.39638343\n",
      "Iteration 290, loss = 0.39624789\n",
      "Iteration 291, loss = 0.39591518\n",
      "Iteration 292, loss = 0.39577198\n",
      "Iteration 293, loss = 0.39556192\n",
      "Iteration 294, loss = 0.39524026\n",
      "Iteration 295, loss = 0.39498590\n",
      "Iteration 296, loss = 0.39475565\n",
      "Iteration 297, loss = 0.39454929\n",
      "Iteration 298, loss = 0.39433024\n",
      "Iteration 299, loss = 0.39409855\n",
      "Iteration 300, loss = 0.39388723\n",
      "Iteration 301, loss = 0.39372724\n",
      "Iteration 302, loss = 0.39348561\n",
      "Iteration 303, loss = 0.39325156\n",
      "Iteration 304, loss = 0.39303975\n",
      "Iteration 305, loss = 0.39284734\n",
      "Iteration 306, loss = 0.39265794\n",
      "Iteration 307, loss = 0.39245986\n",
      "Iteration 308, loss = 0.39218329\n",
      "Iteration 309, loss = 0.39195411\n",
      "Iteration 310, loss = 0.39176568\n",
      "Iteration 311, loss = 0.39152658\n",
      "Iteration 312, loss = 0.39138824\n",
      "Iteration 313, loss = 0.39119375\n",
      "Iteration 314, loss = 0.39092552\n",
      "Iteration 315, loss = 0.39073388\n",
      "Iteration 316, loss = 0.39056070\n",
      "Iteration 317, loss = 0.39040409\n",
      "Iteration 318, loss = 0.39023991\n",
      "Iteration 319, loss = 0.38998910\n",
      "Iteration 320, loss = 0.38981933\n",
      "Iteration 321, loss = 0.38961999\n",
      "Iteration 322, loss = 0.38945714\n",
      "Iteration 323, loss = 0.38926943\n",
      "Iteration 324, loss = 0.38910890\n",
      "Iteration 325, loss = 0.38892510\n",
      "Iteration 326, loss = 0.38873031\n",
      "Iteration 327, loss = 0.38858549\n",
      "Iteration 328, loss = 0.38847288\n",
      "Iteration 329, loss = 0.38827375\n",
      "Iteration 330, loss = 0.38807439\n",
      "Iteration 331, loss = 0.38794588\n",
      "Iteration 332, loss = 0.38772374\n",
      "Iteration 333, loss = 0.38757483\n",
      "Iteration 334, loss = 0.38740175\n",
      "Iteration 335, loss = 0.38726959\n",
      "Iteration 336, loss = 0.38718730\n",
      "Iteration 337, loss = 0.38697569\n",
      "Iteration 338, loss = 0.38679681\n",
      "Iteration 339, loss = 0.38675180\n",
      "Iteration 340, loss = 0.38646124\n",
      "Iteration 341, loss = 0.38636259\n",
      "Iteration 342, loss = 0.38615923\n",
      "Iteration 343, loss = 0.38603732\n",
      "Iteration 344, loss = 0.38585777\n",
      "Iteration 345, loss = 0.38578046\n",
      "Iteration 346, loss = 0.38556819\n",
      "Iteration 347, loss = 0.38544415\n",
      "Iteration 348, loss = 0.38530698\n",
      "Iteration 349, loss = 0.38508004\n",
      "Iteration 350, loss = 0.38497664\n",
      "Iteration 351, loss = 0.38475127\n",
      "Iteration 352, loss = 0.38468294\n",
      "Iteration 353, loss = 0.38443944\n",
      "Iteration 354, loss = 0.38428601\n",
      "Iteration 355, loss = 0.38409931\n",
      "Iteration 356, loss = 0.38402255\n",
      "Iteration 357, loss = 0.38377362\n",
      "Iteration 358, loss = 0.38363639\n",
      "Iteration 359, loss = 0.38345980\n",
      "Iteration 360, loss = 0.38330327\n",
      "Iteration 361, loss = 0.38315630\n",
      "Iteration 362, loss = 0.38306330\n",
      "Iteration 363, loss = 0.38291147\n",
      "Iteration 364, loss = 0.38276563\n",
      "Iteration 365, loss = 0.38273519\n",
      "Iteration 366, loss = 0.38255509\n",
      "Iteration 367, loss = 0.38239459\n",
      "Iteration 368, loss = 0.38225209\n",
      "Iteration 369, loss = 0.38211990\n",
      "Iteration 370, loss = 0.38201993\n",
      "Iteration 371, loss = 0.38186989\n",
      "Iteration 372, loss = 0.38187821\n",
      "Iteration 373, loss = 0.38164352\n",
      "Iteration 374, loss = 0.38151144\n",
      "Iteration 375, loss = 0.38137648\n",
      "Iteration 376, loss = 0.38125291\n",
      "Iteration 377, loss = 0.38112894\n",
      "Iteration 378, loss = 0.38104401\n",
      "Iteration 379, loss = 0.38090799\n",
      "Iteration 380, loss = 0.38078566\n",
      "Iteration 381, loss = 0.38066888\n",
      "Iteration 382, loss = 0.38059652\n",
      "Iteration 383, loss = 0.38045395\n",
      "Iteration 384, loss = 0.38034281\n",
      "Iteration 385, loss = 0.38023381\n",
      "Iteration 386, loss = 0.38020215\n",
      "Iteration 387, loss = 0.38001564\n",
      "Iteration 388, loss = 0.37992551\n",
      "Iteration 389, loss = 0.37990748\n",
      "Iteration 390, loss = 0.37969895\n",
      "Iteration 391, loss = 0.37965050\n",
      "Iteration 392, loss = 0.37949082\n",
      "Iteration 393, loss = 0.37938324\n",
      "Iteration 394, loss = 0.37930961\n",
      "Iteration 395, loss = 0.37929274\n",
      "Iteration 396, loss = 0.37906793\n",
      "Iteration 397, loss = 0.37908044\n",
      "Iteration 398, loss = 0.37886717\n",
      "Iteration 399, loss = 0.37877549\n",
      "Iteration 400, loss = 0.37867960\n",
      "Iteration 401, loss = 0.37857488\n",
      "Iteration 402, loss = 0.37849722\n",
      "Iteration 403, loss = 0.37841135\n",
      "Iteration 404, loss = 0.37829800\n",
      "Iteration 405, loss = 0.37819736\n",
      "Iteration 406, loss = 0.37807381\n",
      "Iteration 407, loss = 0.37799158\n",
      "Iteration 408, loss = 0.37785864\n",
      "Iteration 409, loss = 0.37771246\n",
      "Iteration 410, loss = 0.37769279\n",
      "Iteration 411, loss = 0.37751963\n",
      "Iteration 412, loss = 0.37739162\n",
      "Iteration 413, loss = 0.37729385\n",
      "Iteration 414, loss = 0.37714218\n",
      "Iteration 415, loss = 0.37706951\n",
      "Iteration 416, loss = 0.37697591\n",
      "Iteration 417, loss = 0.37688406\n",
      "Iteration 418, loss = 0.37684658\n",
      "Iteration 419, loss = 0.37670732\n",
      "Iteration 420, loss = 0.37664693\n",
      "Iteration 421, loss = 0.37656943\n",
      "Iteration 422, loss = 0.37644738\n",
      "Iteration 423, loss = 0.37642179\n",
      "Iteration 424, loss = 0.37628635\n",
      "Iteration 425, loss = 0.37629723\n",
      "Iteration 426, loss = 0.37611966\n",
      "Iteration 427, loss = 0.37612646\n",
      "Iteration 428, loss = 0.37601621\n",
      "Iteration 429, loss = 0.37586845\n",
      "Iteration 430, loss = 0.37582164\n",
      "Iteration 431, loss = 0.37571038\n",
      "Iteration 432, loss = 0.37568569\n",
      "Iteration 433, loss = 0.37557520\n",
      "Iteration 434, loss = 0.37549395\n",
      "Iteration 435, loss = 0.37540746\n",
      "Iteration 436, loss = 0.37540901\n",
      "Iteration 437, loss = 0.37529697\n",
      "Iteration 438, loss = 0.37520493\n",
      "Iteration 439, loss = 0.37509292\n",
      "Iteration 440, loss = 0.37509757\n",
      "Iteration 441, loss = 0.37494754\n",
      "Iteration 442, loss = 0.37486958\n",
      "Iteration 443, loss = 0.37479656\n",
      "Iteration 444, loss = 0.37472478\n",
      "Iteration 445, loss = 0.37468939\n",
      "Iteration 446, loss = 0.37458248\n",
      "Iteration 447, loss = 0.37455187\n",
      "Iteration 448, loss = 0.37451936\n",
      "Iteration 449, loss = 0.37438199\n",
      "Iteration 450, loss = 0.37428597\n",
      "Iteration 451, loss = 0.37420978\n",
      "Iteration 452, loss = 0.37413309\n",
      "Iteration 453, loss = 0.37407793\n",
      "Iteration 454, loss = 0.37394054\n",
      "Iteration 455, loss = 0.37383273\n",
      "Iteration 456, loss = 0.37375140\n",
      "Iteration 457, loss = 0.37370958\n",
      "Iteration 458, loss = 0.37355949\n",
      "Iteration 459, loss = 0.37341721\n",
      "Iteration 460, loss = 0.37337148\n",
      "Iteration 461, loss = 0.37333834\n",
      "Iteration 462, loss = 0.37327555\n",
      "Iteration 463, loss = 0.37322561\n",
      "Iteration 464, loss = 0.37306715\n",
      "Iteration 465, loss = 0.37308828\n",
      "Iteration 466, loss = 0.37293078\n",
      "Iteration 467, loss = 0.37287452\n",
      "Iteration 468, loss = 0.37281333\n",
      "Iteration 469, loss = 0.37274965\n",
      "Iteration 470, loss = 0.37268075\n",
      "Iteration 471, loss = 0.37262887\n",
      "Iteration 472, loss = 0.37255302\n",
      "Iteration 473, loss = 0.37251256\n",
      "Iteration 474, loss = 0.37240518\n",
      "Iteration 475, loss = 0.37233695\n",
      "Iteration 476, loss = 0.37229143\n",
      "Iteration 477, loss = 0.37226837\n",
      "Iteration 478, loss = 0.37215980\n",
      "Iteration 479, loss = 0.37211493\n",
      "Iteration 480, loss = 0.37205989\n",
      "Iteration 481, loss = 0.37199320\n",
      "Iteration 482, loss = 0.37200691\n",
      "Iteration 483, loss = 0.37185880\n",
      "Iteration 484, loss = 0.37179775\n",
      "Iteration 485, loss = 0.37177207\n",
      "Iteration 486, loss = 0.37176736\n",
      "Iteration 487, loss = 0.37165297\n",
      "Iteration 488, loss = 0.37158418\n",
      "Iteration 489, loss = 0.37154304\n",
      "Iteration 490, loss = 0.37155238\n",
      "Iteration 491, loss = 0.37145934\n",
      "Iteration 492, loss = 0.37134576\n",
      "Iteration 493, loss = 0.37135994\n",
      "Iteration 494, loss = 0.37128531\n",
      "Iteration 495, loss = 0.37118952\n",
      "Iteration 496, loss = 0.37115405\n",
      "Iteration 497, loss = 0.37106519\n",
      "Iteration 498, loss = 0.37101865\n",
      "Iteration 499, loss = 0.37099933\n",
      "Iteration 500, loss = 0.37091377\n",
      "Iteration 501, loss = 0.37086666\n",
      "Iteration 502, loss = 0.37081434\n",
      "Iteration 503, loss = 0.37077281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural=MLPClassifier(max_iter=1000, verbose=True,tol=0.0001, activation='relu')\n",
    "rede_neural.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c2649c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8229166666666666"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "20c9786a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67,  5],\n",
       "       [12, 12]], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = rede_neural.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b4f35b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(42.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize=(7,5))\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c5e31d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fault       0.85      0.93      0.89        72\n",
      "     Healthy       0.71      0.50      0.59        24\n",
      "\n",
      "    accuracy                           0.82        96\n",
      "   macro avg       0.78      0.72      0.74        96\n",
      "weighted avg       0.81      0.82      0.81        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7dba152a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Healthy'], dtype='<U7')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural.predict([[-0.12,-0.02]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41e2973e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Healthy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df0 = dataset_process[\"Label\"]='Healthy'\n",
    "df1 = dataset_process[\"Label\"]='Fault'\n",
    "display(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c147a54",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25596/607048014.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#plt.scatter(X[:,0],X[:,2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'principal component 1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"green\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'principal component 1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"blue\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "#plt.scatter(X[:,0],X[:,2])\n",
    "plt.scatter(df0['principal component 1'], df0['Label'],color=\"green\",marker='+')\n",
    "plt.scatter(df1['principal component 1'], df1['Label'],color=\"blue\",marker='.')\n",
    "plt.ylabel('Label')\n",
    "plt.xlabel('principle component 1')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79c23a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "dataset_process = pd.read_csv(\"PCA_banco_3_normalizado_4_classes.csv\", delimiter=';')\n",
    "X = dataset_process.loc[: , dataset_process.columns != 'Label'].values\n",
    "y = dataset_process[\"Label\"].values\n",
    "\n",
    "len(dataset_process.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "851ca6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principal component 1</th>\n",
       "      <th>principal component 2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.122303</td>\n",
       "      <td>-0.018642</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.115301</td>\n",
       "      <td>-0.017505</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.126719</td>\n",
       "      <td>-0.016976</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.096206</td>\n",
       "      <td>-0.014626</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.115627</td>\n",
       "      <td>-0.015725</td>\n",
       "      <td>BallFault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>-0.119933</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>-0.108706</td>\n",
       "      <td>-0.017053</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>-0.104510</td>\n",
       "      <td>-0.016105</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>-0.072984</td>\n",
       "      <td>-0.012387</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>-0.104464</td>\n",
       "      <td>-0.015714</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     principal component 1  principal component 2      Label\n",
       "0                -0.122303              -0.018642  BallFault\n",
       "1                -0.115301              -0.017505  BallFault\n",
       "2                -0.126719              -0.016976  BallFault\n",
       "3                -0.096206              -0.014626  BallFault\n",
       "4                -0.115627              -0.015725  BallFault\n",
       "..                     ...                    ...        ...\n",
       "475              -0.119933              -0.018148    Healthy\n",
       "476              -0.108706              -0.017053    Healthy\n",
       "477              -0.104510              -0.016105    Healthy\n",
       "478              -0.072984              -0.012387    Healthy\n",
       "479              -0.104464              -0.015714    Healthy\n",
       "\n",
       "[480 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98377ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b594db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6b7be04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7c4b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40779871\n",
      "Iteration 2, loss = 1.40263238\n",
      "Iteration 3, loss = 1.39833773\n",
      "Iteration 4, loss = 1.39478613\n",
      "Iteration 5, loss = 1.39105707\n",
      "Iteration 6, loss = 1.38763323\n",
      "Iteration 7, loss = 1.38517550\n",
      "Iteration 8, loss = 1.38243883\n",
      "Iteration 9, loss = 1.37983091\n",
      "Iteration 10, loss = 1.37777182\n",
      "Iteration 11, loss = 1.37618904\n",
      "Iteration 12, loss = 1.37390255\n",
      "Iteration 13, loss = 1.37271127\n",
      "Iteration 14, loss = 1.37100532\n",
      "Iteration 15, loss = 1.36950607\n",
      "Iteration 16, loss = 1.36822419\n",
      "Iteration 17, loss = 1.36680806\n",
      "Iteration 18, loss = 1.36542972\n",
      "Iteration 19, loss = 1.36401864\n",
      "Iteration 20, loss = 1.36262442\n",
      "Iteration 21, loss = 1.36129208\n",
      "Iteration 22, loss = 1.35995873\n",
      "Iteration 23, loss = 1.35854366\n",
      "Iteration 24, loss = 1.35715380\n",
      "Iteration 25, loss = 1.35572179\n",
      "Iteration 26, loss = 1.35428926\n",
      "Iteration 27, loss = 1.35287087\n",
      "Iteration 28, loss = 1.35142870\n",
      "Iteration 29, loss = 1.34995640\n",
      "Iteration 30, loss = 1.34850922\n",
      "Iteration 31, loss = 1.34696950\n",
      "Iteration 32, loss = 1.34553312\n",
      "Iteration 33, loss = 1.34399183\n",
      "Iteration 34, loss = 1.34238915\n",
      "Iteration 35, loss = 1.34084775\n",
      "Iteration 36, loss = 1.33928765\n",
      "Iteration 37, loss = 1.33768039\n",
      "Iteration 38, loss = 1.33611770\n",
      "Iteration 39, loss = 1.33450168\n",
      "Iteration 40, loss = 1.33284023\n",
      "Iteration 41, loss = 1.33121853\n",
      "Iteration 42, loss = 1.32949908\n",
      "Iteration 43, loss = 1.32777766\n",
      "Iteration 44, loss = 1.32611387\n",
      "Iteration 45, loss = 1.32434353\n",
      "Iteration 46, loss = 1.32255756\n",
      "Iteration 47, loss = 1.32074436\n",
      "Iteration 48, loss = 1.31909573\n",
      "Iteration 49, loss = 1.31718797\n",
      "Iteration 50, loss = 1.31528904\n",
      "Iteration 51, loss = 1.31343118\n",
      "Iteration 52, loss = 1.31160750\n",
      "Iteration 53, loss = 1.30973682\n",
      "Iteration 54, loss = 1.30774107\n",
      "Iteration 55, loss = 1.30585302\n",
      "Iteration 56, loss = 1.30381914\n",
      "Iteration 57, loss = 1.30186614\n",
      "Iteration 58, loss = 1.29996914\n",
      "Iteration 59, loss = 1.29785892\n",
      "Iteration 60, loss = 1.29581247\n",
      "Iteration 61, loss = 1.29374263\n",
      "Iteration 62, loss = 1.29173457\n",
      "Iteration 63, loss = 1.28960635\n",
      "Iteration 64, loss = 1.28749439\n",
      "Iteration 65, loss = 1.28545373\n",
      "Iteration 66, loss = 1.28326910\n",
      "Iteration 67, loss = 1.28114792\n",
      "Iteration 68, loss = 1.27893195\n",
      "Iteration 69, loss = 1.27681764\n",
      "Iteration 70, loss = 1.27453712\n",
      "Iteration 71, loss = 1.27230837\n",
      "Iteration 72, loss = 1.27013227\n",
      "Iteration 73, loss = 1.26785118\n",
      "Iteration 74, loss = 1.26564617\n",
      "Iteration 75, loss = 1.26335370\n",
      "Iteration 76, loss = 1.26109569\n",
      "Iteration 77, loss = 1.25882015\n",
      "Iteration 78, loss = 1.25645935\n",
      "Iteration 79, loss = 1.25415235\n",
      "Iteration 80, loss = 1.25199983\n",
      "Iteration 81, loss = 1.24947417\n",
      "Iteration 82, loss = 1.24714020\n",
      "Iteration 83, loss = 1.24476979\n",
      "Iteration 84, loss = 1.24249435\n",
      "Iteration 85, loss = 1.24018309\n",
      "Iteration 86, loss = 1.23771350\n",
      "Iteration 87, loss = 1.23532220\n",
      "Iteration 88, loss = 1.23307162\n",
      "Iteration 89, loss = 1.23056802\n",
      "Iteration 90, loss = 1.22819708\n",
      "Iteration 91, loss = 1.22591302\n",
      "Iteration 92, loss = 1.22338661\n",
      "Iteration 93, loss = 1.22109434\n",
      "Iteration 94, loss = 1.21863632\n",
      "Iteration 95, loss = 1.21613818\n",
      "Iteration 96, loss = 1.21378926\n",
      "Iteration 97, loss = 1.21133914\n",
      "Iteration 98, loss = 1.20897785\n",
      "Iteration 99, loss = 1.20652028\n",
      "Iteration 100, loss = 1.20404776\n",
      "Iteration 101, loss = 1.20169977\n",
      "Iteration 102, loss = 1.19935889\n",
      "Iteration 103, loss = 1.19684290\n",
      "Iteration 104, loss = 1.19449428\n",
      "Iteration 105, loss = 1.19202447\n",
      "Iteration 106, loss = 1.18958500\n",
      "Iteration 107, loss = 1.18730654\n",
      "Iteration 108, loss = 1.18487925\n",
      "Iteration 109, loss = 1.18249947\n",
      "Iteration 110, loss = 1.18020412\n",
      "Iteration 111, loss = 1.17776135\n",
      "Iteration 112, loss = 1.17543699\n",
      "Iteration 113, loss = 1.17303303\n",
      "Iteration 114, loss = 1.17076711\n",
      "Iteration 115, loss = 1.16833195\n",
      "Iteration 116, loss = 1.16606164\n",
      "Iteration 117, loss = 1.16374050\n",
      "Iteration 118, loss = 1.16152968\n",
      "Iteration 119, loss = 1.15914730\n",
      "Iteration 120, loss = 1.15681940\n",
      "Iteration 121, loss = 1.15449016\n",
      "Iteration 122, loss = 1.15220314\n",
      "Iteration 123, loss = 1.14994650\n",
      "Iteration 124, loss = 1.14768644\n",
      "Iteration 125, loss = 1.14554464\n",
      "Iteration 126, loss = 1.14314870\n",
      "Iteration 127, loss = 1.14094552\n",
      "Iteration 128, loss = 1.13873699\n",
      "Iteration 129, loss = 1.13642296\n",
      "Iteration 130, loss = 1.13417035\n",
      "Iteration 131, loss = 1.13195271\n",
      "Iteration 132, loss = 1.12976568\n",
      "Iteration 133, loss = 1.12770233\n",
      "Iteration 134, loss = 1.12547701\n",
      "Iteration 135, loss = 1.12321791\n",
      "Iteration 136, loss = 1.12106116\n",
      "Iteration 137, loss = 1.11893258\n",
      "Iteration 138, loss = 1.11677355\n",
      "Iteration 139, loss = 1.11469417\n",
      "Iteration 140, loss = 1.11251494\n",
      "Iteration 141, loss = 1.11041938\n",
      "Iteration 142, loss = 1.10831788\n",
      "Iteration 143, loss = 1.10621252\n",
      "Iteration 144, loss = 1.10414407\n",
      "Iteration 145, loss = 1.10202567\n",
      "Iteration 146, loss = 1.10005752\n",
      "Iteration 147, loss = 1.09793061\n",
      "Iteration 148, loss = 1.09584899\n",
      "Iteration 149, loss = 1.09388295\n",
      "Iteration 150, loss = 1.09177001\n",
      "Iteration 151, loss = 1.08972428\n",
      "Iteration 152, loss = 1.08767904\n",
      "Iteration 153, loss = 1.08566859\n",
      "Iteration 154, loss = 1.08373805\n",
      "Iteration 155, loss = 1.08178124\n",
      "Iteration 156, loss = 1.07980537\n",
      "Iteration 157, loss = 1.07779638\n",
      "Iteration 158, loss = 1.07578706\n",
      "Iteration 159, loss = 1.07380907\n",
      "Iteration 160, loss = 1.07193595\n",
      "Iteration 161, loss = 1.07016032\n",
      "Iteration 162, loss = 1.06801593\n",
      "Iteration 163, loss = 1.06612040\n",
      "Iteration 164, loss = 1.06419146\n",
      "Iteration 165, loss = 1.06234052\n",
      "Iteration 166, loss = 1.06043265\n",
      "Iteration 167, loss = 1.05851388\n",
      "Iteration 168, loss = 1.05660269\n",
      "Iteration 169, loss = 1.05470178\n",
      "Iteration 170, loss = 1.05285302\n",
      "Iteration 171, loss = 1.05097560\n",
      "Iteration 172, loss = 1.04917359\n",
      "Iteration 173, loss = 1.04726335\n",
      "Iteration 174, loss = 1.04548043\n",
      "Iteration 175, loss = 1.04360264\n",
      "Iteration 176, loss = 1.04173921\n",
      "Iteration 177, loss = 1.03988073\n",
      "Iteration 178, loss = 1.03807645\n",
      "Iteration 179, loss = 1.03627540\n",
      "Iteration 180, loss = 1.03445920\n",
      "Iteration 181, loss = 1.03262012\n",
      "Iteration 182, loss = 1.03087362\n",
      "Iteration 183, loss = 1.02914461\n",
      "Iteration 184, loss = 1.02728708\n",
      "Iteration 185, loss = 1.02555884\n",
      "Iteration 186, loss = 1.02368469\n",
      "Iteration 187, loss = 1.02191591\n",
      "Iteration 188, loss = 1.02019851\n",
      "Iteration 189, loss = 1.01835097\n",
      "Iteration 190, loss = 1.01669143\n",
      "Iteration 191, loss = 1.01490179\n",
      "Iteration 192, loss = 1.01314633\n",
      "Iteration 193, loss = 1.01146334\n",
      "Iteration 194, loss = 1.00963403\n",
      "Iteration 195, loss = 1.00787501\n",
      "Iteration 196, loss = 1.00617975\n",
      "Iteration 197, loss = 1.00443875\n",
      "Iteration 198, loss = 1.00276544\n",
      "Iteration 199, loss = 1.00100941\n",
      "Iteration 200, loss = 0.99947102\n",
      "Iteration 201, loss = 0.99761393\n",
      "Iteration 202, loss = 0.99596520\n",
      "Iteration 203, loss = 0.99428940\n",
      "Iteration 204, loss = 0.99257221\n",
      "Iteration 205, loss = 0.99098867\n",
      "Iteration 206, loss = 0.98932902\n",
      "Iteration 207, loss = 0.98769336\n",
      "Iteration 208, loss = 0.98595872\n",
      "Iteration 209, loss = 0.98433211\n",
      "Iteration 210, loss = 0.98269555\n",
      "Iteration 211, loss = 0.98115327\n",
      "Iteration 212, loss = 0.97942481\n",
      "Iteration 213, loss = 0.97783340\n",
      "Iteration 214, loss = 0.97619638\n",
      "Iteration 215, loss = 0.97456288\n",
      "Iteration 216, loss = 0.97294578\n",
      "Iteration 217, loss = 0.97142051\n",
      "Iteration 218, loss = 0.96978108\n",
      "Iteration 219, loss = 0.96817620\n",
      "Iteration 220, loss = 0.96663798\n",
      "Iteration 221, loss = 0.96506239\n",
      "Iteration 222, loss = 0.96355725\n",
      "Iteration 223, loss = 0.96191486\n",
      "Iteration 224, loss = 0.96029350\n",
      "Iteration 225, loss = 0.95875771\n",
      "Iteration 226, loss = 0.95728866\n",
      "Iteration 227, loss = 0.95580547\n",
      "Iteration 228, loss = 0.95408662\n",
      "Iteration 229, loss = 0.95254734\n",
      "Iteration 230, loss = 0.95106147\n",
      "Iteration 231, loss = 0.94951235\n",
      "Iteration 232, loss = 0.94817454\n",
      "Iteration 233, loss = 0.94651242\n",
      "Iteration 234, loss = 0.94515063\n",
      "Iteration 235, loss = 0.94354123\n",
      "Iteration 236, loss = 0.94214117\n",
      "Iteration 237, loss = 0.94047446\n",
      "Iteration 238, loss = 0.93908051\n",
      "Iteration 239, loss = 0.93766581\n",
      "Iteration 240, loss = 0.93615257\n",
      "Iteration 241, loss = 0.93468102\n",
      "Iteration 242, loss = 0.93319885\n",
      "Iteration 243, loss = 0.93172509\n",
      "Iteration 244, loss = 0.93035521\n",
      "Iteration 245, loss = 0.92881465\n",
      "Iteration 246, loss = 0.92735875\n",
      "Iteration 247, loss = 0.92599956\n",
      "Iteration 248, loss = 0.92453200\n",
      "Iteration 249, loss = 0.92312057\n",
      "Iteration 250, loss = 0.92166422\n",
      "Iteration 251, loss = 0.92027081\n",
      "Iteration 252, loss = 0.91887575\n",
      "Iteration 253, loss = 0.91749387\n",
      "Iteration 254, loss = 0.91607577\n",
      "Iteration 255, loss = 0.91474101\n",
      "Iteration 256, loss = 0.91335285\n",
      "Iteration 257, loss = 0.91190852\n",
      "Iteration 258, loss = 0.91057931\n",
      "Iteration 259, loss = 0.90920530\n",
      "Iteration 260, loss = 0.90783395\n",
      "Iteration 261, loss = 0.90644218\n",
      "Iteration 262, loss = 0.90516782\n",
      "Iteration 263, loss = 0.90375054\n",
      "Iteration 264, loss = 0.90241875\n",
      "Iteration 265, loss = 0.90117691\n",
      "Iteration 266, loss = 0.89984710\n",
      "Iteration 267, loss = 0.89841443\n",
      "Iteration 268, loss = 0.89714168\n",
      "Iteration 269, loss = 0.89582272\n",
      "Iteration 270, loss = 0.89453555\n",
      "Iteration 271, loss = 0.89322373\n",
      "Iteration 272, loss = 0.89191239\n",
      "Iteration 273, loss = 0.89073604\n",
      "Iteration 274, loss = 0.88932122\n",
      "Iteration 275, loss = 0.88809087\n",
      "Iteration 276, loss = 0.88682911\n",
      "Iteration 277, loss = 0.88548241\n",
      "Iteration 278, loss = 0.88428927\n",
      "Iteration 279, loss = 0.88299649\n",
      "Iteration 280, loss = 0.88176788\n",
      "Iteration 281, loss = 0.88052515\n",
      "Iteration 282, loss = 0.87932252\n",
      "Iteration 283, loss = 0.87812026\n",
      "Iteration 284, loss = 0.87676301\n",
      "Iteration 285, loss = 0.87550834\n",
      "Iteration 286, loss = 0.87429298\n",
      "Iteration 287, loss = 0.87311737\n",
      "Iteration 288, loss = 0.87191396\n",
      "Iteration 289, loss = 0.87069557\n",
      "Iteration 290, loss = 0.86948276\n",
      "Iteration 291, loss = 0.86834820\n",
      "Iteration 292, loss = 0.86710786\n",
      "Iteration 293, loss = 0.86587253\n",
      "Iteration 294, loss = 0.86472516\n",
      "Iteration 295, loss = 0.86351012\n",
      "Iteration 296, loss = 0.86234608\n",
      "Iteration 297, loss = 0.86119404\n",
      "Iteration 298, loss = 0.85999726\n",
      "Iteration 299, loss = 0.85906271\n",
      "Iteration 300, loss = 0.85766430\n",
      "Iteration 301, loss = 0.85656642\n",
      "Iteration 302, loss = 0.85538103\n",
      "Iteration 303, loss = 0.85428183\n",
      "Iteration 304, loss = 0.85318195\n",
      "Iteration 305, loss = 0.85199669\n",
      "Iteration 306, loss = 0.85096162\n",
      "Iteration 307, loss = 0.84980125\n",
      "Iteration 308, loss = 0.84868590\n",
      "Iteration 309, loss = 0.84756836\n",
      "Iteration 310, loss = 0.84643937\n",
      "Iteration 311, loss = 0.84535569\n",
      "Iteration 312, loss = 0.84422471\n",
      "Iteration 313, loss = 0.84319197\n",
      "Iteration 314, loss = 0.84205301\n",
      "Iteration 315, loss = 0.84099525\n",
      "Iteration 316, loss = 0.83987109\n",
      "Iteration 317, loss = 0.83892458\n",
      "Iteration 318, loss = 0.83771247\n",
      "Iteration 319, loss = 0.83671954\n",
      "Iteration 320, loss = 0.83561939\n",
      "Iteration 321, loss = 0.83460743\n",
      "Iteration 322, loss = 0.83351429\n",
      "Iteration 323, loss = 0.83245251\n",
      "Iteration 324, loss = 0.83141864\n",
      "Iteration 325, loss = 0.83040072\n",
      "Iteration 326, loss = 0.82936550\n",
      "Iteration 327, loss = 0.82835941\n",
      "Iteration 328, loss = 0.82726382\n",
      "Iteration 329, loss = 0.82634315\n",
      "Iteration 330, loss = 0.82525255\n",
      "Iteration 331, loss = 0.82431605\n",
      "Iteration 332, loss = 0.82335102\n",
      "Iteration 333, loss = 0.82232326\n",
      "Iteration 334, loss = 0.82135880\n",
      "Iteration 335, loss = 0.82025590\n",
      "Iteration 336, loss = 0.81926242\n",
      "Iteration 337, loss = 0.81832210\n",
      "Iteration 338, loss = 0.81731627\n",
      "Iteration 339, loss = 0.81637141\n",
      "Iteration 340, loss = 0.81531918\n",
      "Iteration 341, loss = 0.81434919\n",
      "Iteration 342, loss = 0.81343140\n",
      "Iteration 343, loss = 0.81246162\n",
      "Iteration 344, loss = 0.81162572\n",
      "Iteration 345, loss = 0.81058207\n",
      "Iteration 346, loss = 0.80967073\n",
      "Iteration 347, loss = 0.80871269\n",
      "Iteration 348, loss = 0.80770166\n",
      "Iteration 349, loss = 0.80678563\n",
      "Iteration 350, loss = 0.80589257\n",
      "Iteration 351, loss = 0.80492000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 352, loss = 0.80407505\n",
      "Iteration 353, loss = 0.80317440\n",
      "Iteration 354, loss = 0.80215309\n",
      "Iteration 355, loss = 0.80131485\n",
      "Iteration 356, loss = 0.80035601\n",
      "Iteration 357, loss = 0.79949019\n",
      "Iteration 358, loss = 0.79864139\n",
      "Iteration 359, loss = 0.79763725\n",
      "Iteration 360, loss = 0.79686270\n",
      "Iteration 361, loss = 0.79596349\n",
      "Iteration 362, loss = 0.79502338\n",
      "Iteration 363, loss = 0.79411108\n",
      "Iteration 364, loss = 0.79321717\n",
      "Iteration 365, loss = 0.79234873\n",
      "Iteration 366, loss = 0.79150976\n",
      "Iteration 367, loss = 0.79066254\n",
      "Iteration 368, loss = 0.78981841\n",
      "Iteration 369, loss = 0.78894729\n",
      "Iteration 370, loss = 0.78833534\n",
      "Iteration 371, loss = 0.78720170\n",
      "Iteration 372, loss = 0.78641268\n",
      "Iteration 373, loss = 0.78555960\n",
      "Iteration 374, loss = 0.78469517\n",
      "Iteration 375, loss = 0.78384287\n",
      "Iteration 376, loss = 0.78305826\n",
      "Iteration 377, loss = 0.78230910\n",
      "Iteration 378, loss = 0.78137288\n",
      "Iteration 379, loss = 0.78054629\n",
      "Iteration 380, loss = 0.77976588\n",
      "Iteration 381, loss = 0.77898214\n",
      "Iteration 382, loss = 0.77808650\n",
      "Iteration 383, loss = 0.77730293\n",
      "Iteration 384, loss = 0.77648707\n",
      "Iteration 385, loss = 0.77568134\n",
      "Iteration 386, loss = 0.77488983\n",
      "Iteration 387, loss = 0.77413423\n",
      "Iteration 388, loss = 0.77332931\n",
      "Iteration 389, loss = 0.77261204\n",
      "Iteration 390, loss = 0.77177970\n",
      "Iteration 391, loss = 0.77118618\n",
      "Iteration 392, loss = 0.77019003\n",
      "Iteration 393, loss = 0.76942467\n",
      "Iteration 394, loss = 0.76871180\n",
      "Iteration 395, loss = 0.76790341\n",
      "Iteration 396, loss = 0.76711573\n",
      "Iteration 397, loss = 0.76641509\n",
      "Iteration 398, loss = 0.76569179\n",
      "Iteration 399, loss = 0.76485204\n",
      "Iteration 400, loss = 0.76408002\n",
      "Iteration 401, loss = 0.76334882\n",
      "Iteration 402, loss = 0.76264824\n",
      "Iteration 403, loss = 0.76193395\n",
      "Iteration 404, loss = 0.76116018\n",
      "Iteration 405, loss = 0.76036141\n",
      "Iteration 406, loss = 0.75968476\n",
      "Iteration 407, loss = 0.75900561\n",
      "Iteration 408, loss = 0.75816223\n",
      "Iteration 409, loss = 0.75745598\n",
      "Iteration 410, loss = 0.75671454\n",
      "Iteration 411, loss = 0.75601467\n",
      "Iteration 412, loss = 0.75531530\n",
      "Iteration 413, loss = 0.75469034\n",
      "Iteration 414, loss = 0.75394185\n",
      "Iteration 415, loss = 0.75321996\n",
      "Iteration 416, loss = 0.75259835\n",
      "Iteration 417, loss = 0.75174223\n",
      "Iteration 418, loss = 0.75109994\n",
      "Iteration 419, loss = 0.75039957\n",
      "Iteration 420, loss = 0.74964760\n",
      "Iteration 421, loss = 0.74902379\n",
      "Iteration 422, loss = 0.74834287\n",
      "Iteration 423, loss = 0.74754005\n",
      "Iteration 424, loss = 0.74693929\n",
      "Iteration 425, loss = 0.74620997\n",
      "Iteration 426, loss = 0.74558255\n",
      "Iteration 427, loss = 0.74482199\n",
      "Iteration 428, loss = 0.74413649\n",
      "Iteration 429, loss = 0.74349562\n",
      "Iteration 430, loss = 0.74288963\n",
      "Iteration 431, loss = 0.74220378\n",
      "Iteration 432, loss = 0.74153465\n",
      "Iteration 433, loss = 0.74083471\n",
      "Iteration 434, loss = 0.74013654\n",
      "Iteration 435, loss = 0.73959115\n",
      "Iteration 436, loss = 0.73898595\n",
      "Iteration 437, loss = 0.73838025\n",
      "Iteration 438, loss = 0.73752136\n",
      "Iteration 439, loss = 0.73702953\n",
      "Iteration 440, loss = 0.73622827\n",
      "Iteration 441, loss = 0.73577957\n",
      "Iteration 442, loss = 0.73503087\n",
      "Iteration 443, loss = 0.73437281\n",
      "Iteration 444, loss = 0.73372785\n",
      "Iteration 445, loss = 0.73303472\n",
      "Iteration 446, loss = 0.73240467\n",
      "Iteration 447, loss = 0.73180721\n",
      "Iteration 448, loss = 0.73116690\n",
      "Iteration 449, loss = 0.73058192\n",
      "Iteration 450, loss = 0.73002722\n",
      "Iteration 451, loss = 0.72938093\n",
      "Iteration 452, loss = 0.72869947\n",
      "Iteration 453, loss = 0.72803329\n",
      "Iteration 454, loss = 0.72754000\n",
      "Iteration 455, loss = 0.72703366\n",
      "Iteration 456, loss = 0.72624070\n",
      "Iteration 457, loss = 0.72558903\n",
      "Iteration 458, loss = 0.72499368\n",
      "Iteration 459, loss = 0.72453396\n",
      "Iteration 460, loss = 0.72382480\n",
      "Iteration 461, loss = 0.72326926\n",
      "Iteration 462, loss = 0.72270283\n",
      "Iteration 463, loss = 0.72207717\n",
      "Iteration 464, loss = 0.72148028\n",
      "Iteration 465, loss = 0.72087813\n",
      "Iteration 466, loss = 0.72020187\n",
      "Iteration 467, loss = 0.71962131\n",
      "Iteration 468, loss = 0.71916156\n",
      "Iteration 469, loss = 0.71845437\n",
      "Iteration 470, loss = 0.71787366\n",
      "Iteration 471, loss = 0.71736032\n",
      "Iteration 472, loss = 0.71672701\n",
      "Iteration 473, loss = 0.71612983\n",
      "Iteration 474, loss = 0.71578424\n",
      "Iteration 475, loss = 0.71501715\n",
      "Iteration 476, loss = 0.71439980\n",
      "Iteration 477, loss = 0.71385878\n",
      "Iteration 478, loss = 0.71325631\n",
      "Iteration 479, loss = 0.71273666\n",
      "Iteration 480, loss = 0.71213242\n",
      "Iteration 481, loss = 0.71171490\n",
      "Iteration 482, loss = 0.71113988\n",
      "Iteration 483, loss = 0.71052282\n",
      "Iteration 484, loss = 0.71000205\n",
      "Iteration 485, loss = 0.70932152\n",
      "Iteration 486, loss = 0.70882160\n",
      "Iteration 487, loss = 0.70830021\n",
      "Iteration 488, loss = 0.70769849\n",
      "Iteration 489, loss = 0.70711397\n",
      "Iteration 490, loss = 0.70670550\n",
      "Iteration 491, loss = 0.70606232\n",
      "Iteration 492, loss = 0.70551847\n",
      "Iteration 493, loss = 0.70498675\n",
      "Iteration 494, loss = 0.70450095\n",
      "Iteration 495, loss = 0.70396165\n",
      "Iteration 496, loss = 0.70340432\n",
      "Iteration 497, loss = 0.70283303\n",
      "Iteration 498, loss = 0.70230319\n",
      "Iteration 499, loss = 0.70175337\n",
      "Iteration 500, loss = 0.70125611\n",
      "Iteration 501, loss = 0.70072063\n",
      "Iteration 502, loss = 0.70033012\n",
      "Iteration 503, loss = 0.69976271\n",
      "Iteration 504, loss = 0.69918078\n",
      "Iteration 505, loss = 0.69864037\n",
      "Iteration 506, loss = 0.69820375\n",
      "Iteration 507, loss = 0.69769420\n",
      "Iteration 508, loss = 0.69712025\n",
      "Iteration 509, loss = 0.69658196\n",
      "Iteration 510, loss = 0.69607276\n",
      "Iteration 511, loss = 0.69563960\n",
      "Iteration 512, loss = 0.69517355\n",
      "Iteration 513, loss = 0.69464183\n",
      "Iteration 514, loss = 0.69420986\n",
      "Iteration 515, loss = 0.69365091\n",
      "Iteration 516, loss = 0.69305757\n",
      "Iteration 517, loss = 0.69281943\n",
      "Iteration 518, loss = 0.69227879\n",
      "Iteration 519, loss = 0.69163387\n",
      "Iteration 520, loss = 0.69113536\n",
      "Iteration 521, loss = 0.69068956\n",
      "Iteration 522, loss = 0.69013100\n",
      "Iteration 523, loss = 0.68960819\n",
      "Iteration 524, loss = 0.68914589\n",
      "Iteration 525, loss = 0.68882315\n",
      "Iteration 526, loss = 0.68834386\n",
      "Iteration 527, loss = 0.68776233\n",
      "Iteration 528, loss = 0.68723381\n",
      "Iteration 529, loss = 0.68681077\n",
      "Iteration 530, loss = 0.68629614\n",
      "Iteration 531, loss = 0.68583554\n",
      "Iteration 532, loss = 0.68530444\n",
      "Iteration 533, loss = 0.68490981\n",
      "Iteration 534, loss = 0.68462122\n",
      "Iteration 535, loss = 0.68393451\n",
      "Iteration 536, loss = 0.68346542\n",
      "Iteration 537, loss = 0.68300323\n",
      "Iteration 538, loss = 0.68252444\n",
      "Iteration 539, loss = 0.68214601\n",
      "Iteration 540, loss = 0.68175468\n",
      "Iteration 541, loss = 0.68115292\n",
      "Iteration 542, loss = 0.68074288\n",
      "Iteration 543, loss = 0.68020414\n",
      "Iteration 544, loss = 0.67981050\n",
      "Iteration 545, loss = 0.67953658\n",
      "Iteration 546, loss = 0.67894977\n",
      "Iteration 547, loss = 0.67842497\n",
      "Iteration 548, loss = 0.67798907\n",
      "Iteration 549, loss = 0.67748601\n",
      "Iteration 550, loss = 0.67711735\n",
      "Iteration 551, loss = 0.67662345\n",
      "Iteration 552, loss = 0.67621563\n",
      "Iteration 553, loss = 0.67575140\n",
      "Iteration 554, loss = 0.67532555\n",
      "Iteration 555, loss = 0.67486685\n",
      "Iteration 556, loss = 0.67444869\n",
      "Iteration 557, loss = 0.67396815\n",
      "Iteration 558, loss = 0.67351766\n",
      "Iteration 559, loss = 0.67312089\n",
      "Iteration 560, loss = 0.67262221\n",
      "Iteration 561, loss = 0.67220445\n",
      "Iteration 562, loss = 0.67177218\n",
      "Iteration 563, loss = 0.67140616\n",
      "Iteration 564, loss = 0.67099644\n",
      "Iteration 565, loss = 0.67057209\n",
      "Iteration 566, loss = 0.67011671\n",
      "Iteration 567, loss = 0.66969220\n",
      "Iteration 568, loss = 0.66937369\n",
      "Iteration 569, loss = 0.66886680\n",
      "Iteration 570, loss = 0.66835511\n",
      "Iteration 571, loss = 0.66804546\n",
      "Iteration 572, loss = 0.66764194\n",
      "Iteration 573, loss = 0.66719065\n",
      "Iteration 574, loss = 0.66667074\n",
      "Iteration 575, loss = 0.66633418\n",
      "Iteration 576, loss = 0.66591288\n",
      "Iteration 577, loss = 0.66558038\n",
      "Iteration 578, loss = 0.66510116\n",
      "Iteration 579, loss = 0.66474276\n",
      "Iteration 580, loss = 0.66420142\n",
      "Iteration 581, loss = 0.66380287\n",
      "Iteration 582, loss = 0.66342439\n",
      "Iteration 583, loss = 0.66298853\n",
      "Iteration 584, loss = 0.66261800\n",
      "Iteration 585, loss = 0.66228874\n",
      "Iteration 586, loss = 0.66178765\n",
      "Iteration 587, loss = 0.66149049\n",
      "Iteration 588, loss = 0.66112143\n",
      "Iteration 589, loss = 0.66063689\n",
      "Iteration 590, loss = 0.66016870\n",
      "Iteration 591, loss = 0.65977332\n",
      "Iteration 592, loss = 0.65936966\n",
      "Iteration 593, loss = 0.65902586\n",
      "Iteration 594, loss = 0.65870890\n",
      "Iteration 595, loss = 0.65826544\n",
      "Iteration 596, loss = 0.65779433\n",
      "Iteration 597, loss = 0.65774024\n",
      "Iteration 598, loss = 0.65700659\n",
      "Iteration 599, loss = 0.65663288\n",
      "Iteration 600, loss = 0.65625006\n",
      "Iteration 601, loss = 0.65591770\n",
      "Iteration 602, loss = 0.65555970\n",
      "Iteration 603, loss = 0.65511674\n",
      "Iteration 604, loss = 0.65468230\n",
      "Iteration 605, loss = 0.65429409\n",
      "Iteration 606, loss = 0.65391879\n",
      "Iteration 607, loss = 0.65354814\n",
      "Iteration 608, loss = 0.65336987\n",
      "Iteration 609, loss = 0.65283009\n",
      "Iteration 610, loss = 0.65257062\n",
      "Iteration 611, loss = 0.65213830\n",
      "Iteration 612, loss = 0.65175323\n",
      "Iteration 613, loss = 0.65148103\n",
      "Iteration 614, loss = 0.65109904\n",
      "Iteration 615, loss = 0.65056811\n",
      "Iteration 616, loss = 0.65020964\n",
      "Iteration 617, loss = 0.64992334\n",
      "Iteration 618, loss = 0.64953216\n",
      "Iteration 619, loss = 0.64916377\n",
      "Iteration 620, loss = 0.64869830\n",
      "Iteration 621, loss = 0.64836203\n",
      "Iteration 622, loss = 0.64801251\n",
      "Iteration 623, loss = 0.64782684\n",
      "Iteration 624, loss = 0.64728814\n",
      "Iteration 625, loss = 0.64694342\n",
      "Iteration 626, loss = 0.64654812\n",
      "Iteration 627, loss = 0.64619421\n",
      "Iteration 628, loss = 0.64599608\n",
      "Iteration 629, loss = 0.64546133\n",
      "Iteration 630, loss = 0.64518804\n",
      "Iteration 631, loss = 0.64480780\n",
      "Iteration 632, loss = 0.64447113\n",
      "Iteration 633, loss = 0.64407010\n",
      "Iteration 634, loss = 0.64374455\n",
      "Iteration 635, loss = 0.64354159\n",
      "Iteration 636, loss = 0.64309800\n",
      "Iteration 637, loss = 0.64272061\n",
      "Iteration 638, loss = 0.64236680\n",
      "Iteration 639, loss = 0.64201620\n",
      "Iteration 640, loss = 0.64183426\n",
      "Iteration 641, loss = 0.64131628\n",
      "Iteration 642, loss = 0.64105957\n",
      "Iteration 643, loss = 0.64067101\n",
      "Iteration 644, loss = 0.64030765\n",
      "Iteration 645, loss = 0.64000398\n",
      "Iteration 646, loss = 0.63972936\n",
      "Iteration 647, loss = 0.63932935\n",
      "Iteration 648, loss = 0.63906739\n",
      "Iteration 649, loss = 0.63863193\n",
      "Iteration 650, loss = 0.63837994\n",
      "Iteration 651, loss = 0.63801540\n",
      "Iteration 652, loss = 0.63763082\n",
      "Iteration 653, loss = 0.63732143\n",
      "Iteration 654, loss = 0.63717389\n",
      "Iteration 655, loss = 0.63666426\n",
      "Iteration 656, loss = 0.63634833\n",
      "Iteration 657, loss = 0.63611471\n",
      "Iteration 658, loss = 0.63568023\n",
      "Iteration 659, loss = 0.63540115\n",
      "Iteration 660, loss = 0.63504112\n",
      "Iteration 661, loss = 0.63486400\n",
      "Iteration 662, loss = 0.63449420\n",
      "Iteration 663, loss = 0.63407461\n",
      "Iteration 664, loss = 0.63395455\n",
      "Iteration 665, loss = 0.63359697\n",
      "Iteration 666, loss = 0.63323150\n",
      "Iteration 667, loss = 0.63288630\n",
      "Iteration 668, loss = 0.63252976\n",
      "Iteration 669, loss = 0.63221797\n",
      "Iteration 670, loss = 0.63191874\n",
      "Iteration 671, loss = 0.63163522\n",
      "Iteration 672, loss = 0.63129294\n",
      "Iteration 673, loss = 0.63097255\n",
      "Iteration 674, loss = 0.63073775\n",
      "Iteration 675, loss = 0.63051844\n",
      "Iteration 676, loss = 0.63004042\n",
      "Iteration 677, loss = 0.62973281\n",
      "Iteration 678, loss = 0.62947785\n",
      "Iteration 679, loss = 0.62918767\n",
      "Iteration 680, loss = 0.62887049\n",
      "Iteration 681, loss = 0.62856021\n",
      "Iteration 682, loss = 0.62827028\n",
      "Iteration 683, loss = 0.62796958\n",
      "Iteration 684, loss = 0.62767413\n",
      "Iteration 685, loss = 0.62738374\n",
      "Iteration 686, loss = 0.62707117\n",
      "Iteration 687, loss = 0.62679667\n",
      "Iteration 688, loss = 0.62656197\n",
      "Iteration 689, loss = 0.62618269\n",
      "Iteration 690, loss = 0.62600806\n",
      "Iteration 691, loss = 0.62557059\n",
      "Iteration 692, loss = 0.62527954\n",
      "Iteration 693, loss = 0.62512921\n",
      "Iteration 694, loss = 0.62472468\n",
      "Iteration 695, loss = 0.62452573\n",
      "Iteration 696, loss = 0.62454745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 697, loss = 0.62399098\n",
      "Iteration 698, loss = 0.62357356\n",
      "Iteration 699, loss = 0.62347539\n",
      "Iteration 700, loss = 0.62304015\n",
      "Iteration 701, loss = 0.62273924\n",
      "Iteration 702, loss = 0.62245836\n",
      "Iteration 703, loss = 0.62223219\n",
      "Iteration 704, loss = 0.62198183\n",
      "Iteration 705, loss = 0.62161873\n",
      "Iteration 706, loss = 0.62133905\n",
      "Iteration 707, loss = 0.62116144\n",
      "Iteration 708, loss = 0.62080867\n",
      "Iteration 709, loss = 0.62057939\n",
      "Iteration 710, loss = 0.62029988\n",
      "Iteration 711, loss = 0.62008489\n",
      "Iteration 712, loss = 0.61976895\n",
      "Iteration 713, loss = 0.61957353\n",
      "Iteration 714, loss = 0.61926121\n",
      "Iteration 715, loss = 0.61890757\n",
      "Iteration 716, loss = 0.61865990\n",
      "Iteration 717, loss = 0.61853809\n",
      "Iteration 718, loss = 0.61810617\n",
      "Iteration 719, loss = 0.61796697\n",
      "Iteration 720, loss = 0.61756227\n",
      "Iteration 721, loss = 0.61756155\n",
      "Iteration 722, loss = 0.61713652\n",
      "Iteration 723, loss = 0.61678281\n",
      "Iteration 724, loss = 0.61654843\n",
      "Iteration 725, loss = 0.61628697\n",
      "Iteration 726, loss = 0.61601386\n",
      "Iteration 727, loss = 0.61575807\n",
      "Iteration 728, loss = 0.61562026\n",
      "Iteration 729, loss = 0.61534758\n",
      "Iteration 730, loss = 0.61509078\n",
      "Iteration 731, loss = 0.61475404\n",
      "Iteration 732, loss = 0.61451083\n",
      "Iteration 733, loss = 0.61441171\n",
      "Iteration 734, loss = 0.61411956\n",
      "Iteration 735, loss = 0.61380990\n",
      "Iteration 736, loss = 0.61356407\n",
      "Iteration 737, loss = 0.61325179\n",
      "Iteration 738, loss = 0.61299402\n",
      "Iteration 739, loss = 0.61279348\n",
      "Iteration 740, loss = 0.61250460\n",
      "Iteration 741, loss = 0.61233457\n",
      "Iteration 742, loss = 0.61205627\n",
      "Iteration 743, loss = 0.61184061\n",
      "Iteration 744, loss = 0.61155771\n",
      "Iteration 745, loss = 0.61130379\n",
      "Iteration 746, loss = 0.61109165\n",
      "Iteration 747, loss = 0.61081250\n",
      "Iteration 748, loss = 0.61068674\n",
      "Iteration 749, loss = 0.61034203\n",
      "Iteration 750, loss = 0.61018411\n",
      "Iteration 751, loss = 0.60987353\n",
      "Iteration 752, loss = 0.60982804\n",
      "Iteration 753, loss = 0.60943527\n",
      "Iteration 754, loss = 0.60916562\n",
      "Iteration 755, loss = 0.60900159\n",
      "Iteration 756, loss = 0.60875291\n",
      "Iteration 757, loss = 0.60847093\n",
      "Iteration 758, loss = 0.60834497\n",
      "Iteration 759, loss = 0.60821982\n",
      "Iteration 760, loss = 0.60797815\n",
      "Iteration 761, loss = 0.60758177\n",
      "Iteration 762, loss = 0.60731826\n",
      "Iteration 763, loss = 0.60711154\n",
      "Iteration 764, loss = 0.60697204\n",
      "Iteration 765, loss = 0.60665732\n",
      "Iteration 766, loss = 0.60654701\n",
      "Iteration 767, loss = 0.60624854\n",
      "Iteration 768, loss = 0.60607225\n",
      "Iteration 769, loss = 0.60623927\n",
      "Iteration 770, loss = 0.60558397\n",
      "Iteration 771, loss = 0.60555996\n",
      "Iteration 772, loss = 0.60512482\n",
      "Iteration 773, loss = 0.60502576\n",
      "Iteration 774, loss = 0.60466573\n",
      "Iteration 775, loss = 0.60458432\n",
      "Iteration 776, loss = 0.60421822\n",
      "Iteration 777, loss = 0.60407794\n",
      "Iteration 778, loss = 0.60380238\n",
      "Iteration 779, loss = 0.60377172\n",
      "Iteration 780, loss = 0.60365944\n",
      "Iteration 781, loss = 0.60318459\n",
      "Iteration 782, loss = 0.60295304\n",
      "Iteration 783, loss = 0.60276304\n",
      "Iteration 784, loss = 0.60254990\n",
      "Iteration 785, loss = 0.60231850\n",
      "Iteration 786, loss = 0.60214447\n",
      "Iteration 787, loss = 0.60198011\n",
      "Iteration 788, loss = 0.60170950\n",
      "Iteration 789, loss = 0.60154360\n",
      "Iteration 790, loss = 0.60137629\n",
      "Iteration 791, loss = 0.60127094\n",
      "Iteration 792, loss = 0.60092132\n",
      "Iteration 793, loss = 0.60088464\n",
      "Iteration 794, loss = 0.60053544\n",
      "Iteration 795, loss = 0.60041276\n",
      "Iteration 796, loss = 0.60024791\n",
      "Iteration 797, loss = 0.59988715\n",
      "Iteration 798, loss = 0.59994134\n",
      "Iteration 799, loss = 0.59948912\n",
      "Iteration 800, loss = 0.59929621\n",
      "Iteration 801, loss = 0.59933391\n",
      "Iteration 802, loss = 0.59898285\n",
      "Iteration 803, loss = 0.59868497\n",
      "Iteration 804, loss = 0.59852392\n",
      "Iteration 805, loss = 0.59828654\n",
      "Iteration 806, loss = 0.59820231\n",
      "Iteration 807, loss = 0.59792608\n",
      "Iteration 808, loss = 0.59776020\n",
      "Iteration 809, loss = 0.59751671\n",
      "Iteration 810, loss = 0.59735872\n",
      "Iteration 811, loss = 0.59713049\n",
      "Iteration 812, loss = 0.59692558\n",
      "Iteration 813, loss = 0.59677288\n",
      "Iteration 814, loss = 0.59674857\n",
      "Iteration 815, loss = 0.59642452\n",
      "Iteration 816, loss = 0.59627317\n",
      "Iteration 817, loss = 0.59608110\n",
      "Iteration 818, loss = 0.59582755\n",
      "Iteration 819, loss = 0.59570193\n",
      "Iteration 820, loss = 0.59558748\n",
      "Iteration 821, loss = 0.59531680\n",
      "Iteration 822, loss = 0.59523073\n",
      "Iteration 823, loss = 0.59489500\n",
      "Iteration 824, loss = 0.59474190\n",
      "Iteration 825, loss = 0.59452575\n",
      "Iteration 826, loss = 0.59438081\n",
      "Iteration 827, loss = 0.59416809\n",
      "Iteration 828, loss = 0.59395965\n",
      "Iteration 829, loss = 0.59400208\n",
      "Iteration 830, loss = 0.59363792\n",
      "Iteration 831, loss = 0.59341103\n",
      "Iteration 832, loss = 0.59329623\n",
      "Iteration 833, loss = 0.59307058\n",
      "Iteration 834, loss = 0.59314141\n",
      "Iteration 835, loss = 0.59281315\n",
      "Iteration 836, loss = 0.59255292\n",
      "Iteration 837, loss = 0.59250301\n",
      "Iteration 838, loss = 0.59221838\n",
      "Iteration 839, loss = 0.59207024\n",
      "Iteration 840, loss = 0.59186964\n",
      "Iteration 841, loss = 0.59164526\n",
      "Iteration 842, loss = 0.59153774\n",
      "Iteration 843, loss = 0.59131451\n",
      "Iteration 844, loss = 0.59114288\n",
      "Iteration 845, loss = 0.59096622\n",
      "Iteration 846, loss = 0.59095006\n",
      "Iteration 847, loss = 0.59062777\n",
      "Iteration 848, loss = 0.59050399\n",
      "Iteration 849, loss = 0.59031381\n",
      "Iteration 850, loss = 0.59018883\n",
      "Iteration 851, loss = 0.58990758\n",
      "Iteration 852, loss = 0.58972902\n",
      "Iteration 853, loss = 0.58964370\n",
      "Iteration 854, loss = 0.58957293\n",
      "Iteration 855, loss = 0.58924420\n",
      "Iteration 856, loss = 0.58907474\n",
      "Iteration 857, loss = 0.58887184\n",
      "Iteration 858, loss = 0.58895021\n",
      "Iteration 859, loss = 0.58865112\n",
      "Iteration 860, loss = 0.58863508\n",
      "Iteration 861, loss = 0.58824283\n",
      "Iteration 862, loss = 0.58810512\n",
      "Iteration 863, loss = 0.58799924\n",
      "Iteration 864, loss = 0.58792884\n",
      "Iteration 865, loss = 0.58761830\n",
      "Iteration 866, loss = 0.58748256\n",
      "Iteration 867, loss = 0.58739195\n",
      "Iteration 868, loss = 0.58714642\n",
      "Iteration 869, loss = 0.58701035\n",
      "Iteration 870, loss = 0.58677426\n",
      "Iteration 871, loss = 0.58668910\n",
      "Iteration 872, loss = 0.58646353\n",
      "Iteration 873, loss = 0.58638210\n",
      "Iteration 874, loss = 0.58613305\n",
      "Iteration 875, loss = 0.58614719\n",
      "Iteration 876, loss = 0.58589562\n",
      "Iteration 877, loss = 0.58573594\n",
      "Iteration 878, loss = 0.58550511\n",
      "Iteration 879, loss = 0.58543546\n",
      "Iteration 880, loss = 0.58532659\n",
      "Iteration 881, loss = 0.58510041\n",
      "Iteration 882, loss = 0.58494059\n",
      "Iteration 883, loss = 0.58487217\n",
      "Iteration 884, loss = 0.58456376\n",
      "Iteration 885, loss = 0.58446553\n",
      "Iteration 886, loss = 0.58432838\n",
      "Iteration 887, loss = 0.58423306\n",
      "Iteration 888, loss = 0.58405207\n",
      "Iteration 889, loss = 0.58383581\n",
      "Iteration 890, loss = 0.58373662\n",
      "Iteration 891, loss = 0.58376438\n",
      "Iteration 892, loss = 0.58338455\n",
      "Iteration 893, loss = 0.58323546\n",
      "Iteration 894, loss = 0.58309834\n",
      "Iteration 895, loss = 0.58295252\n",
      "Iteration 896, loss = 0.58276646\n",
      "Iteration 897, loss = 0.58262308\n",
      "Iteration 898, loss = 0.58254152\n",
      "Iteration 899, loss = 0.58234279\n",
      "Iteration 900, loss = 0.58224055\n",
      "Iteration 901, loss = 0.58204514\n",
      "Iteration 902, loss = 0.58190653\n",
      "Iteration 903, loss = 0.58187462\n",
      "Iteration 904, loss = 0.58159772\n",
      "Iteration 905, loss = 0.58144552\n",
      "Iteration 906, loss = 0.58129780\n",
      "Iteration 907, loss = 0.58113056\n",
      "Iteration 908, loss = 0.58111065\n",
      "Iteration 909, loss = 0.58085007\n",
      "Iteration 910, loss = 0.58070062\n",
      "Iteration 911, loss = 0.58065580\n",
      "Iteration 912, loss = 0.58046672\n",
      "Iteration 913, loss = 0.58027994\n",
      "Iteration 914, loss = 0.58015973\n",
      "Iteration 915, loss = 0.58000078\n",
      "Iteration 916, loss = 0.58007989\n",
      "Iteration 917, loss = 0.57985435\n",
      "Iteration 918, loss = 0.57965683\n",
      "Iteration 919, loss = 0.57958899\n",
      "Iteration 920, loss = 0.57936479\n",
      "Iteration 921, loss = 0.57926276\n",
      "Iteration 922, loss = 0.57907845\n",
      "Iteration 923, loss = 0.57893559\n",
      "Iteration 924, loss = 0.57876501\n",
      "Iteration 925, loss = 0.57871644\n",
      "Iteration 926, loss = 0.57852624\n",
      "Iteration 927, loss = 0.57835149\n",
      "Iteration 928, loss = 0.57817202\n",
      "Iteration 929, loss = 0.57812845\n",
      "Iteration 930, loss = 0.57799233\n",
      "Iteration 931, loss = 0.57784003\n",
      "Iteration 932, loss = 0.57765830\n",
      "Iteration 933, loss = 0.57768063\n",
      "Iteration 934, loss = 0.57740333\n",
      "Iteration 935, loss = 0.57730173\n",
      "Iteration 936, loss = 0.57720648\n",
      "Iteration 937, loss = 0.57702233\n",
      "Iteration 938, loss = 0.57685234\n",
      "Iteration 939, loss = 0.57680234\n",
      "Iteration 940, loss = 0.57666941\n",
      "Iteration 941, loss = 0.57644435\n",
      "Iteration 942, loss = 0.57637147\n",
      "Iteration 943, loss = 0.57618348\n",
      "Iteration 944, loss = 0.57603821\n",
      "Iteration 945, loss = 0.57591647\n",
      "Iteration 946, loss = 0.57580184\n",
      "Iteration 947, loss = 0.57572295\n",
      "Iteration 948, loss = 0.57567932\n",
      "Iteration 949, loss = 0.57564195\n",
      "Iteration 950, loss = 0.57549781\n",
      "Iteration 951, loss = 0.57511344\n",
      "Iteration 952, loss = 0.57498746\n",
      "Iteration 953, loss = 0.57491420\n",
      "Iteration 954, loss = 0.57499157\n",
      "Iteration 955, loss = 0.57481279\n",
      "Iteration 956, loss = 0.57448433\n",
      "Iteration 957, loss = 0.57436640\n",
      "Iteration 958, loss = 0.57424483\n",
      "Iteration 959, loss = 0.57421293\n",
      "Iteration 960, loss = 0.57399302\n",
      "Iteration 961, loss = 0.57388776\n",
      "Iteration 962, loss = 0.57377220\n",
      "Iteration 963, loss = 0.57366367\n",
      "Iteration 964, loss = 0.57348894\n",
      "Iteration 965, loss = 0.57340773\n",
      "Iteration 966, loss = 0.57322289\n",
      "Iteration 967, loss = 0.57325635\n",
      "Iteration 968, loss = 0.57295975\n",
      "Iteration 969, loss = 0.57287395\n",
      "Iteration 970, loss = 0.57277748\n",
      "Iteration 971, loss = 0.57276309\n",
      "Iteration 972, loss = 0.57251488\n",
      "Iteration 973, loss = 0.57235437\n",
      "Iteration 974, loss = 0.57229579\n",
      "Iteration 975, loss = 0.57210118\n",
      "Iteration 976, loss = 0.57209709\n",
      "Iteration 977, loss = 0.57191680\n",
      "Iteration 978, loss = 0.57179549\n",
      "Iteration 979, loss = 0.57165276\n",
      "Iteration 980, loss = 0.57154945\n",
      "Iteration 981, loss = 0.57149157\n",
      "Iteration 982, loss = 0.57128519\n",
      "Iteration 983, loss = 0.57115804\n",
      "Iteration 984, loss = 0.57100183\n",
      "Iteration 985, loss = 0.57094615\n",
      "Iteration 986, loss = 0.57081104\n",
      "Iteration 987, loss = 0.57092721\n",
      "Iteration 988, loss = 0.57075376\n",
      "Iteration 989, loss = 0.57046649\n",
      "Iteration 990, loss = 0.57031246\n",
      "Iteration 991, loss = 0.57020580\n",
      "Iteration 992, loss = 0.57005145\n",
      "Iteration 993, loss = 0.57007320\n",
      "Iteration 994, loss = 0.56983979\n",
      "Iteration 995, loss = 0.56981676\n",
      "Iteration 996, loss = 0.56974437\n",
      "Iteration 997, loss = 0.56955332\n",
      "Iteration 998, loss = 0.56936017\n",
      "Iteration 999, loss = 0.56934064\n",
      "Iteration 1000, loss = 0.56945824\n",
      "Iteration 1001, loss = 0.56904884\n",
      "Iteration 1002, loss = 0.56894920\n",
      "Iteration 1003, loss = 0.56883489\n",
      "Iteration 1004, loss = 0.56875694\n",
      "Iteration 1005, loss = 0.56856563\n",
      "Iteration 1006, loss = 0.56847637\n",
      "Iteration 1007, loss = 0.56868838\n",
      "Iteration 1008, loss = 0.56838268\n",
      "Iteration 1009, loss = 0.56818340\n",
      "Iteration 1010, loss = 0.56798631\n",
      "Iteration 1011, loss = 0.56802304\n",
      "Iteration 1012, loss = 0.56775283\n",
      "Iteration 1013, loss = 0.56771290\n",
      "Iteration 1014, loss = 0.56764258\n",
      "Iteration 1015, loss = 0.56753332\n",
      "Iteration 1016, loss = 0.56730381\n",
      "Iteration 1017, loss = 0.56719999\n",
      "Iteration 1018, loss = 0.56722137\n",
      "Iteration 1019, loss = 0.56705139\n",
      "Iteration 1020, loss = 0.56685762\n",
      "Iteration 1021, loss = 0.56686787\n",
      "Iteration 1022, loss = 0.56668002\n",
      "Iteration 1023, loss = 0.56657481\n",
      "Iteration 1024, loss = 0.56643150\n",
      "Iteration 1025, loss = 0.56636640\n",
      "Iteration 1026, loss = 0.56625589\n",
      "Iteration 1027, loss = 0.56612993\n",
      "Iteration 1028, loss = 0.56604099\n",
      "Iteration 1029, loss = 0.56596497\n",
      "Iteration 1030, loss = 0.56590188\n",
      "Iteration 1031, loss = 0.56604086\n",
      "Iteration 1032, loss = 0.56561854\n",
      "Iteration 1033, loss = 0.56547680\n",
      "Iteration 1034, loss = 0.56537205\n",
      "Iteration 1035, loss = 0.56523436\n",
      "Iteration 1036, loss = 0.56534454\n",
      "Iteration 1037, loss = 0.56523882\n",
      "Iteration 1038, loss = 0.56504493\n",
      "Iteration 1039, loss = 0.56477289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1040, loss = 0.56472170\n",
      "Iteration 1041, loss = 0.56464398\n",
      "Iteration 1042, loss = 0.56451961\n",
      "Iteration 1043, loss = 0.56437264\n",
      "Iteration 1044, loss = 0.56429774\n",
      "Iteration 1045, loss = 0.56423243\n",
      "Iteration 1046, loss = 0.56403319\n",
      "Iteration 1047, loss = 0.56394682\n",
      "Iteration 1048, loss = 0.56393218\n",
      "Iteration 1049, loss = 0.56372161\n",
      "Iteration 1050, loss = 0.56373615\n",
      "Iteration 1051, loss = 0.56351500\n",
      "Iteration 1052, loss = 0.56347367\n",
      "Iteration 1053, loss = 0.56351150\n",
      "Iteration 1054, loss = 0.56322340\n",
      "Iteration 1055, loss = 0.56313986\n",
      "Iteration 1056, loss = 0.56296649\n",
      "Iteration 1057, loss = 0.56302236\n",
      "Iteration 1058, loss = 0.56288670\n",
      "Iteration 1059, loss = 0.56270187\n",
      "Iteration 1060, loss = 0.56265291\n",
      "Iteration 1061, loss = 0.56255770\n",
      "Iteration 1062, loss = 0.56240591\n",
      "Iteration 1063, loss = 0.56226887\n",
      "Iteration 1064, loss = 0.56214932\n",
      "Iteration 1065, loss = 0.56206814\n",
      "Iteration 1066, loss = 0.56203205\n",
      "Iteration 1067, loss = 0.56191015\n",
      "Iteration 1068, loss = 0.56177468\n",
      "Iteration 1069, loss = 0.56190528\n",
      "Iteration 1070, loss = 0.56171008\n",
      "Iteration 1071, loss = 0.56159206\n",
      "Iteration 1072, loss = 0.56140286\n",
      "Iteration 1073, loss = 0.56140573\n",
      "Iteration 1074, loss = 0.56135614\n",
      "Iteration 1075, loss = 0.56111696\n",
      "Iteration 1076, loss = 0.56092299\n",
      "Iteration 1077, loss = 0.56084045\n",
      "Iteration 1078, loss = 0.56071813\n",
      "Iteration 1079, loss = 0.56069751\n",
      "Iteration 1080, loss = 0.56069433\n",
      "Iteration 1081, loss = 0.56051396\n",
      "Iteration 1082, loss = 0.56054270\n",
      "Iteration 1083, loss = 0.56040617\n",
      "Iteration 1084, loss = 0.56010393\n",
      "Iteration 1085, loss = 0.56007789\n",
      "Iteration 1086, loss = 0.56000012\n",
      "Iteration 1087, loss = 0.55996654\n",
      "Iteration 1088, loss = 0.55982628\n",
      "Iteration 1089, loss = 0.55971334\n",
      "Iteration 1090, loss = 0.55964828\n",
      "Iteration 1091, loss = 0.55953318\n",
      "Iteration 1092, loss = 0.55936054\n",
      "Iteration 1093, loss = 0.55943887\n",
      "Iteration 1094, loss = 0.55919965\n",
      "Iteration 1095, loss = 0.55906386\n",
      "Iteration 1096, loss = 0.55897261\n",
      "Iteration 1097, loss = 0.55892441\n",
      "Iteration 1098, loss = 0.55877379\n",
      "Iteration 1099, loss = 0.55880479\n",
      "Iteration 1100, loss = 0.55865561\n",
      "Iteration 1101, loss = 0.55852845\n",
      "Iteration 1102, loss = 0.55849614\n",
      "Iteration 1103, loss = 0.55826795\n",
      "Iteration 1104, loss = 0.55823443\n",
      "Iteration 1105, loss = 0.55813170\n",
      "Iteration 1106, loss = 0.55801828\n",
      "Iteration 1107, loss = 0.55795498\n",
      "Iteration 1108, loss = 0.55786641\n",
      "Iteration 1109, loss = 0.55771485\n",
      "Iteration 1110, loss = 0.55771380\n",
      "Iteration 1111, loss = 0.55757553\n",
      "Iteration 1112, loss = 0.55745890\n",
      "Iteration 1113, loss = 0.55738373\n",
      "Iteration 1114, loss = 0.55729911\n",
      "Iteration 1115, loss = 0.55746435\n",
      "Iteration 1116, loss = 0.55707982\n",
      "Iteration 1117, loss = 0.55700112\n",
      "Iteration 1118, loss = 0.55690601\n",
      "Iteration 1119, loss = 0.55681338\n",
      "Iteration 1120, loss = 0.55678842\n",
      "Iteration 1121, loss = 0.55669689\n",
      "Iteration 1122, loss = 0.55650561\n",
      "Iteration 1123, loss = 0.55643811\n",
      "Iteration 1124, loss = 0.55631741\n",
      "Iteration 1125, loss = 0.55625278\n",
      "Iteration 1126, loss = 0.55618233\n",
      "Iteration 1127, loss = 0.55602967\n",
      "Iteration 1128, loss = 0.55601287\n",
      "Iteration 1129, loss = 0.55598791\n",
      "Iteration 1130, loss = 0.55577359\n",
      "Iteration 1131, loss = 0.55575861\n",
      "Iteration 1132, loss = 0.55562994\n",
      "Iteration 1133, loss = 0.55557283\n",
      "Iteration 1134, loss = 0.55556828\n",
      "Iteration 1135, loss = 0.55533085\n",
      "Iteration 1136, loss = 0.55523474\n",
      "Iteration 1137, loss = 0.55560209\n",
      "Iteration 1138, loss = 0.55510917\n",
      "Iteration 1139, loss = 0.55520002\n",
      "Iteration 1140, loss = 0.55485540\n",
      "Iteration 1141, loss = 0.55474930\n",
      "Iteration 1142, loss = 0.55496882\n",
      "Iteration 1143, loss = 0.55466006\n",
      "Iteration 1144, loss = 0.55453695\n",
      "Iteration 1145, loss = 0.55452816\n",
      "Iteration 1146, loss = 0.55431972\n",
      "Iteration 1147, loss = 0.55426231\n",
      "Iteration 1148, loss = 0.55417949\n",
      "Iteration 1149, loss = 0.55403349\n",
      "Iteration 1150, loss = 0.55395116\n",
      "Iteration 1151, loss = 0.55393804\n",
      "Iteration 1152, loss = 0.55376597\n",
      "Iteration 1153, loss = 0.55375084\n",
      "Iteration 1154, loss = 0.55363811\n",
      "Iteration 1155, loss = 0.55355780\n",
      "Iteration 1156, loss = 0.55351564\n",
      "Iteration 1157, loss = 0.55339352\n",
      "Iteration 1158, loss = 0.55332096\n",
      "Iteration 1159, loss = 0.55325605\n",
      "Iteration 1160, loss = 0.55315475\n",
      "Iteration 1161, loss = 0.55305731\n",
      "Iteration 1162, loss = 0.55316388\n",
      "Iteration 1163, loss = 0.55287018\n",
      "Iteration 1164, loss = 0.55281894\n",
      "Iteration 1165, loss = 0.55272205\n",
      "Iteration 1166, loss = 0.55262301\n",
      "Iteration 1167, loss = 0.55244263\n",
      "Iteration 1168, loss = 0.55242947\n",
      "Iteration 1169, loss = 0.55250791\n",
      "Iteration 1170, loss = 0.55235893\n",
      "Iteration 1171, loss = 0.55215981\n",
      "Iteration 1172, loss = 0.55207205\n",
      "Iteration 1173, loss = 0.55207073\n",
      "Iteration 1174, loss = 0.55195697\n",
      "Iteration 1175, loss = 0.55190775\n",
      "Iteration 1176, loss = 0.55178437\n",
      "Iteration 1177, loss = 0.55168679\n",
      "Iteration 1178, loss = 0.55159331\n",
      "Iteration 1179, loss = 0.55148703\n",
      "Iteration 1180, loss = 0.55136696\n",
      "Iteration 1181, loss = 0.55154387\n",
      "Iteration 1182, loss = 0.55124533\n",
      "Iteration 1183, loss = 0.55117318\n",
      "Iteration 1184, loss = 0.55102900\n",
      "Iteration 1185, loss = 0.55092388\n",
      "Iteration 1186, loss = 0.55084591\n",
      "Iteration 1187, loss = 0.55081037\n",
      "Iteration 1188, loss = 0.55083952\n",
      "Iteration 1189, loss = 0.55066994\n",
      "Iteration 1190, loss = 0.55051944\n",
      "Iteration 1191, loss = 0.55057671\n",
      "Iteration 1192, loss = 0.55031790\n",
      "Iteration 1193, loss = 0.55023166\n",
      "Iteration 1194, loss = 0.55018729\n",
      "Iteration 1195, loss = 0.55023014\n",
      "Iteration 1196, loss = 0.55005877\n",
      "Iteration 1197, loss = 0.54993250\n",
      "Iteration 1198, loss = 0.54982239\n",
      "Iteration 1199, loss = 0.54979040\n",
      "Iteration 1200, loss = 0.54968929\n",
      "Iteration 1201, loss = 0.54958335\n",
      "Iteration 1202, loss = 0.54948365\n",
      "Iteration 1203, loss = 0.54945428\n",
      "Iteration 1204, loss = 0.54935093\n",
      "Iteration 1205, loss = 0.54936435\n",
      "Iteration 1206, loss = 0.54916701\n",
      "Iteration 1207, loss = 0.54910983\n",
      "Iteration 1208, loss = 0.54907232\n",
      "Iteration 1209, loss = 0.54892592\n",
      "Iteration 1210, loss = 0.54881390\n",
      "Iteration 1211, loss = 0.54876789\n",
      "Iteration 1212, loss = 0.54864521\n",
      "Iteration 1213, loss = 0.54873507\n",
      "Iteration 1214, loss = 0.54876804\n",
      "Iteration 1215, loss = 0.54857823\n",
      "Iteration 1216, loss = 0.54836061\n",
      "Iteration 1217, loss = 0.54834336\n",
      "Iteration 1218, loss = 0.54836989\n",
      "Iteration 1219, loss = 0.54826724\n",
      "Iteration 1220, loss = 0.54823919\n",
      "Iteration 1221, loss = 0.54809800\n",
      "Iteration 1222, loss = 0.54813151\n",
      "Iteration 1223, loss = 0.54777655\n",
      "Iteration 1224, loss = 0.54778847\n",
      "Iteration 1225, loss = 0.54779207\n",
      "Iteration 1226, loss = 0.54762348\n",
      "Iteration 1227, loss = 0.54751020\n",
      "Iteration 1228, loss = 0.54751979\n",
      "Iteration 1229, loss = 0.54734090\n",
      "Iteration 1230, loss = 0.54728017\n",
      "Iteration 1231, loss = 0.54715914\n",
      "Iteration 1232, loss = 0.54723027\n",
      "Iteration 1233, loss = 0.54717791\n",
      "Iteration 1234, loss = 0.54691844\n",
      "Iteration 1235, loss = 0.54680939\n",
      "Iteration 1236, loss = 0.54669474\n",
      "Iteration 1237, loss = 0.54661057\n",
      "Iteration 1238, loss = 0.54660934\n",
      "Iteration 1239, loss = 0.54663171\n",
      "Iteration 1240, loss = 0.54677076\n",
      "Iteration 1241, loss = 0.54634133\n",
      "Iteration 1242, loss = 0.54631169\n",
      "Iteration 1243, loss = 0.54626910\n",
      "Iteration 1244, loss = 0.54611230\n",
      "Iteration 1245, loss = 0.54602473\n",
      "Iteration 1246, loss = 0.54618343\n",
      "Iteration 1247, loss = 0.54586901\n",
      "Iteration 1248, loss = 0.54589644\n",
      "Iteration 1249, loss = 0.54566776\n",
      "Iteration 1250, loss = 0.54567793\n",
      "Iteration 1251, loss = 0.54554978\n",
      "Iteration 1252, loss = 0.54558150\n",
      "Iteration 1253, loss = 0.54547325\n",
      "Iteration 1254, loss = 0.54534009\n",
      "Iteration 1255, loss = 0.54545613\n",
      "Iteration 1256, loss = 0.54513803\n",
      "Iteration 1257, loss = 0.54509208\n",
      "Iteration 1258, loss = 0.54508838\n",
      "Iteration 1259, loss = 0.54498650\n",
      "Iteration 1260, loss = 0.54492658\n",
      "Iteration 1261, loss = 0.54475543\n",
      "Iteration 1262, loss = 0.54473108\n",
      "Iteration 1263, loss = 0.54466768\n",
      "Iteration 1264, loss = 0.54454568\n",
      "Iteration 1265, loss = 0.54463389\n",
      "Iteration 1266, loss = 0.54457053\n",
      "Iteration 1267, loss = 0.54433334\n",
      "Iteration 1268, loss = 0.54423179\n",
      "Iteration 1269, loss = 0.54416675\n",
      "Iteration 1270, loss = 0.54419686\n",
      "Iteration 1271, loss = 0.54400437\n",
      "Iteration 1272, loss = 0.54393982\n",
      "Iteration 1273, loss = 0.54401582\n",
      "Iteration 1274, loss = 0.54377436\n",
      "Iteration 1275, loss = 0.54369475\n",
      "Iteration 1276, loss = 0.54356681\n",
      "Iteration 1277, loss = 0.54356669\n",
      "Iteration 1278, loss = 0.54352344\n",
      "Iteration 1279, loss = 0.54337412\n",
      "Iteration 1280, loss = 0.54342543\n",
      "Iteration 1281, loss = 0.54340668\n",
      "Iteration 1282, loss = 0.54328527\n",
      "Iteration 1283, loss = 0.54304358\n",
      "Iteration 1284, loss = 0.54301523\n",
      "Iteration 1285, loss = 0.54292325\n",
      "Iteration 1286, loss = 0.54281805\n",
      "Iteration 1287, loss = 0.54274179\n",
      "Iteration 1288, loss = 0.54272262\n",
      "Iteration 1289, loss = 0.54273551\n",
      "Iteration 1290, loss = 0.54258068\n",
      "Iteration 1291, loss = 0.54244103\n",
      "Iteration 1292, loss = 0.54234929\n",
      "Iteration 1293, loss = 0.54231245\n",
      "Iteration 1294, loss = 0.54220524\n",
      "Iteration 1295, loss = 0.54210576\n",
      "Iteration 1296, loss = 0.54225868\n",
      "Iteration 1297, loss = 0.54196475\n",
      "Iteration 1298, loss = 0.54194143\n",
      "Iteration 1299, loss = 0.54190707\n",
      "Iteration 1300, loss = 0.54179466\n",
      "Iteration 1301, loss = 0.54174245\n",
      "Iteration 1302, loss = 0.54160328\n",
      "Iteration 1303, loss = 0.54161264\n",
      "Iteration 1304, loss = 0.54154674\n",
      "Iteration 1305, loss = 0.54135290\n",
      "Iteration 1306, loss = 0.54130000\n",
      "Iteration 1307, loss = 0.54126053\n",
      "Iteration 1308, loss = 0.54113335\n",
      "Iteration 1309, loss = 0.54105900\n",
      "Iteration 1310, loss = 0.54137640\n",
      "Iteration 1311, loss = 0.54096515\n",
      "Iteration 1312, loss = 0.54087730\n",
      "Iteration 1313, loss = 0.54079025\n",
      "Iteration 1314, loss = 0.54071032\n",
      "Iteration 1315, loss = 0.54069137\n",
      "Iteration 1316, loss = 0.54079714\n",
      "Iteration 1317, loss = 0.54053466\n",
      "Iteration 1318, loss = 0.54039336\n",
      "Iteration 1319, loss = 0.54035684\n",
      "Iteration 1320, loss = 0.54020284\n",
      "Iteration 1321, loss = 0.54018414\n",
      "Iteration 1322, loss = 0.54024020\n",
      "Iteration 1323, loss = 0.54003923\n",
      "Iteration 1324, loss = 0.54001631\n",
      "Iteration 1325, loss = 0.53987910\n",
      "Iteration 1326, loss = 0.53979159\n",
      "Iteration 1327, loss = 0.53977150\n",
      "Iteration 1328, loss = 0.53959149\n",
      "Iteration 1329, loss = 0.53957840\n",
      "Iteration 1330, loss = 0.53961895\n",
      "Iteration 1331, loss = 0.53948384\n",
      "Iteration 1332, loss = 0.53951648\n",
      "Iteration 1333, loss = 0.53926737\n",
      "Iteration 1334, loss = 0.53920714\n",
      "Iteration 1335, loss = 0.53911882\n",
      "Iteration 1336, loss = 0.53909629\n",
      "Iteration 1337, loss = 0.53914727\n",
      "Iteration 1338, loss = 0.53908969\n",
      "Iteration 1339, loss = 0.53889544\n",
      "Iteration 1340, loss = 0.53878326\n",
      "Iteration 1341, loss = 0.53862829\n",
      "Iteration 1342, loss = 0.53857442\n",
      "Iteration 1343, loss = 0.53880380\n",
      "Iteration 1344, loss = 0.53841128\n",
      "Iteration 1345, loss = 0.53838123\n",
      "Iteration 1346, loss = 0.53826677\n",
      "Iteration 1347, loss = 0.53828381\n",
      "Iteration 1348, loss = 0.53820574\n",
      "Iteration 1349, loss = 0.53803291\n",
      "Iteration 1350, loss = 0.53804056\n",
      "Iteration 1351, loss = 0.53788101\n",
      "Iteration 1352, loss = 0.53784008\n",
      "Iteration 1353, loss = 0.53802359\n",
      "Iteration 1354, loss = 0.53787623\n",
      "Iteration 1355, loss = 0.53772271\n",
      "Iteration 1356, loss = 0.53756889\n",
      "Iteration 1357, loss = 0.53758826\n",
      "Iteration 1358, loss = 0.53736490\n",
      "Iteration 1359, loss = 0.53749609\n",
      "Iteration 1360, loss = 0.53730722\n",
      "Iteration 1361, loss = 0.53718396\n",
      "Iteration 1362, loss = 0.53710364\n",
      "Iteration 1363, loss = 0.53712616\n",
      "Iteration 1364, loss = 0.53711627\n",
      "Iteration 1365, loss = 0.53700407\n",
      "Iteration 1366, loss = 0.53681337\n",
      "Iteration 1367, loss = 0.53669071\n",
      "Iteration 1368, loss = 0.53675418\n",
      "Iteration 1369, loss = 0.53656874\n",
      "Iteration 1370, loss = 0.53662658\n",
      "Iteration 1371, loss = 0.53642833\n",
      "Iteration 1372, loss = 0.53646971\n",
      "Iteration 1373, loss = 0.53635361\n",
      "Iteration 1374, loss = 0.53634415\n",
      "Iteration 1375, loss = 0.53612806\n",
      "Iteration 1376, loss = 0.53621551\n",
      "Iteration 1377, loss = 0.53614234\n",
      "Iteration 1378, loss = 0.53588326\n",
      "Iteration 1379, loss = 0.53580987\n",
      "Iteration 1380, loss = 0.53584302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1381, loss = 0.53571761\n",
      "Iteration 1382, loss = 0.53565063\n",
      "Iteration 1383, loss = 0.53571089\n",
      "Iteration 1384, loss = 0.53546754\n",
      "Iteration 1385, loss = 0.53542606\n",
      "Iteration 1386, loss = 0.53556206\n",
      "Iteration 1387, loss = 0.53525209\n",
      "Iteration 1388, loss = 0.53517858\n",
      "Iteration 1389, loss = 0.53509550\n",
      "Iteration 1390, loss = 0.53503900\n",
      "Iteration 1391, loss = 0.53497666\n",
      "Iteration 1392, loss = 0.53492393\n",
      "Iteration 1393, loss = 0.53496374\n",
      "Iteration 1394, loss = 0.53483203\n",
      "Iteration 1395, loss = 0.53471392\n",
      "Iteration 1396, loss = 0.53458561\n",
      "Iteration 1397, loss = 0.53466696\n",
      "Iteration 1398, loss = 0.53466939\n",
      "Iteration 1399, loss = 0.53446301\n",
      "Iteration 1400, loss = 0.53431071\n",
      "Iteration 1401, loss = 0.53434245\n",
      "Iteration 1402, loss = 0.53423791\n",
      "Iteration 1403, loss = 0.53434421\n",
      "Iteration 1404, loss = 0.53423979\n",
      "Iteration 1405, loss = 0.53409149\n",
      "Iteration 1406, loss = 0.53388862\n",
      "Iteration 1407, loss = 0.53379254\n",
      "Iteration 1408, loss = 0.53374606\n",
      "Iteration 1409, loss = 0.53393848\n",
      "Iteration 1410, loss = 0.53373456\n",
      "Iteration 1411, loss = 0.53350631\n",
      "Iteration 1412, loss = 0.53358361\n",
      "Iteration 1413, loss = 0.53338657\n",
      "Iteration 1414, loss = 0.53331233\n",
      "Iteration 1415, loss = 0.53332863\n",
      "Iteration 1416, loss = 0.53314584\n",
      "Iteration 1417, loss = 0.53309395\n",
      "Iteration 1418, loss = 0.53310796\n",
      "Iteration 1419, loss = 0.53296245\n",
      "Iteration 1420, loss = 0.53285699\n",
      "Iteration 1421, loss = 0.53286992\n",
      "Iteration 1422, loss = 0.53289385\n",
      "Iteration 1423, loss = 0.53278152\n",
      "Iteration 1424, loss = 0.53286374\n",
      "Iteration 1425, loss = 0.53255486\n",
      "Iteration 1426, loss = 0.53254763\n",
      "Iteration 1427, loss = 0.53233773\n",
      "Iteration 1428, loss = 0.53257107\n",
      "Iteration 1429, loss = 0.53244528\n",
      "Iteration 1430, loss = 0.53218429\n",
      "Iteration 1431, loss = 0.53211003\n",
      "Iteration 1432, loss = 0.53208245\n",
      "Iteration 1433, loss = 0.53197197\n",
      "Iteration 1434, loss = 0.53189629\n",
      "Iteration 1435, loss = 0.53182521\n",
      "Iteration 1436, loss = 0.53177666\n",
      "Iteration 1437, loss = 0.53176188\n",
      "Iteration 1438, loss = 0.53165936\n",
      "Iteration 1439, loss = 0.53153350\n",
      "Iteration 1440, loss = 0.53141718\n",
      "Iteration 1441, loss = 0.53143196\n",
      "Iteration 1442, loss = 0.53148321\n",
      "Iteration 1443, loss = 0.53135844\n",
      "Iteration 1444, loss = 0.53120479\n",
      "Iteration 1445, loss = 0.53108685\n",
      "Iteration 1446, loss = 0.53098885\n",
      "Iteration 1447, loss = 0.53091460\n",
      "Iteration 1448, loss = 0.53089035\n",
      "Iteration 1449, loss = 0.53082351\n",
      "Iteration 1450, loss = 0.53076512\n",
      "Iteration 1451, loss = 0.53068249\n",
      "Iteration 1452, loss = 0.53062682\n",
      "Iteration 1453, loss = 0.53068604\n",
      "Iteration 1454, loss = 0.53045483\n",
      "Iteration 1455, loss = 0.53040381\n",
      "Iteration 1456, loss = 0.53041159\n",
      "Iteration 1457, loss = 0.53021414\n",
      "Iteration 1458, loss = 0.53019311\n",
      "Iteration 1459, loss = 0.53024827\n",
      "Iteration 1460, loss = 0.53014861\n",
      "Iteration 1461, loss = 0.53003800\n",
      "Iteration 1462, loss = 0.52995791\n",
      "Iteration 1463, loss = 0.52980330\n",
      "Iteration 1464, loss = 0.52969436\n",
      "Iteration 1465, loss = 0.52968464\n",
      "Iteration 1466, loss = 0.52962469\n",
      "Iteration 1467, loss = 0.52965749\n",
      "Iteration 1468, loss = 0.52946812\n",
      "Iteration 1469, loss = 0.52937773\n",
      "Iteration 1470, loss = 0.52932681\n",
      "Iteration 1471, loss = 0.52927121\n",
      "Iteration 1472, loss = 0.52937819\n",
      "Iteration 1473, loss = 0.52919894\n",
      "Iteration 1474, loss = 0.52905225\n",
      "Iteration 1475, loss = 0.52908332\n",
      "Iteration 1476, loss = 0.52893725\n",
      "Iteration 1477, loss = 0.52891650\n",
      "Iteration 1478, loss = 0.52891107\n",
      "Iteration 1479, loss = 0.52869659\n",
      "Iteration 1480, loss = 0.52870108\n",
      "Iteration 1481, loss = 0.52855076\n",
      "Iteration 1482, loss = 0.52843909\n",
      "Iteration 1483, loss = 0.52838695\n",
      "Iteration 1484, loss = 0.52834296\n",
      "Iteration 1485, loss = 0.52825261\n",
      "Iteration 1486, loss = 0.52819228\n",
      "Iteration 1487, loss = 0.52823779\n",
      "Iteration 1488, loss = 0.52815483\n",
      "Iteration 1489, loss = 0.52795137\n",
      "Iteration 1490, loss = 0.52792991\n",
      "Iteration 1491, loss = 0.52782512\n",
      "Iteration 1492, loss = 0.52781020\n",
      "Iteration 1493, loss = 0.52768734\n",
      "Iteration 1494, loss = 0.52766792\n",
      "Iteration 1495, loss = 0.52751307\n",
      "Iteration 1496, loss = 0.52779378\n",
      "Iteration 1497, loss = 0.52749999\n",
      "Iteration 1498, loss = 0.52741451\n",
      "Iteration 1499, loss = 0.52723247\n",
      "Iteration 1500, loss = 0.52724289\n",
      "Iteration 1501, loss = 0.52711723\n",
      "Iteration 1502, loss = 0.52721863\n",
      "Iteration 1503, loss = 0.52711901\n",
      "Iteration 1504, loss = 0.52692857\n",
      "Iteration 1505, loss = 0.52683074\n",
      "Iteration 1506, loss = 0.52687556\n",
      "Iteration 1507, loss = 0.52682025\n",
      "Iteration 1508, loss = 0.52666412\n",
      "Iteration 1509, loss = 0.52660303\n",
      "Iteration 1510, loss = 0.52660207\n",
      "Iteration 1511, loss = 0.52639307\n",
      "Iteration 1512, loss = 0.52642723\n",
      "Iteration 1513, loss = 0.52628246\n",
      "Iteration 1514, loss = 0.52660772\n",
      "Iteration 1515, loss = 0.52613866\n",
      "Iteration 1516, loss = 0.52610683\n",
      "Iteration 1517, loss = 0.52611110\n",
      "Iteration 1518, loss = 0.52600932\n",
      "Iteration 1519, loss = 0.52598402\n",
      "Iteration 1520, loss = 0.52604678\n",
      "Iteration 1521, loss = 0.52580732\n",
      "Iteration 1522, loss = 0.52561792\n",
      "Iteration 1523, loss = 0.52555598\n",
      "Iteration 1524, loss = 0.52553054\n",
      "Iteration 1525, loss = 0.52539643\n",
      "Iteration 1526, loss = 0.52532279\n",
      "Iteration 1527, loss = 0.52541947\n",
      "Iteration 1528, loss = 0.52558389\n",
      "Iteration 1529, loss = 0.52519847\n",
      "Iteration 1530, loss = 0.52525447\n",
      "Iteration 1531, loss = 0.52543355\n",
      "Iteration 1532, loss = 0.52514212\n",
      "Iteration 1533, loss = 0.52489497\n",
      "Iteration 1534, loss = 0.52496507\n",
      "Iteration 1535, loss = 0.52476223\n",
      "Iteration 1536, loss = 0.52470544\n",
      "Iteration 1537, loss = 0.52462035\n",
      "Iteration 1538, loss = 0.52453892\n",
      "Iteration 1539, loss = 0.52472505\n",
      "Iteration 1540, loss = 0.52440121\n",
      "Iteration 1541, loss = 0.52431372\n",
      "Iteration 1542, loss = 0.52423097\n",
      "Iteration 1543, loss = 0.52418462\n",
      "Iteration 1544, loss = 0.52421293\n",
      "Iteration 1545, loss = 0.52415960\n",
      "Iteration 1546, loss = 0.52401496\n",
      "Iteration 1547, loss = 0.52389607\n",
      "Iteration 1548, loss = 0.52376747\n",
      "Iteration 1549, loss = 0.52382826\n",
      "Iteration 1550, loss = 0.52375762\n",
      "Iteration 1551, loss = 0.52356611\n",
      "Iteration 1552, loss = 0.52350834\n",
      "Iteration 1553, loss = 0.52343711\n",
      "Iteration 1554, loss = 0.52338597\n",
      "Iteration 1555, loss = 0.52330087\n",
      "Iteration 1556, loss = 0.52323572\n",
      "Iteration 1557, loss = 0.52317547\n",
      "Iteration 1558, loss = 0.52325283\n",
      "Iteration 1559, loss = 0.52304730\n",
      "Iteration 1560, loss = 0.52310921\n",
      "Iteration 1561, loss = 0.52292453\n",
      "Iteration 1562, loss = 0.52280387\n",
      "Iteration 1563, loss = 0.52273449\n",
      "Iteration 1564, loss = 0.52286911\n",
      "Iteration 1565, loss = 0.52257266\n",
      "Iteration 1566, loss = 0.52254478\n",
      "Iteration 1567, loss = 0.52238630\n",
      "Iteration 1568, loss = 0.52237679\n",
      "Iteration 1569, loss = 0.52232384\n",
      "Iteration 1570, loss = 0.52228070\n",
      "Iteration 1571, loss = 0.52219863\n",
      "Iteration 1572, loss = 0.52227440\n",
      "Iteration 1573, loss = 0.52214642\n",
      "Iteration 1574, loss = 0.52224685\n",
      "Iteration 1575, loss = 0.52188694\n",
      "Iteration 1576, loss = 0.52192929\n",
      "Iteration 1577, loss = 0.52174039\n",
      "Iteration 1578, loss = 0.52164725\n",
      "Iteration 1579, loss = 0.52155992\n",
      "Iteration 1580, loss = 0.52156763\n",
      "Iteration 1581, loss = 0.52151793\n",
      "Iteration 1582, loss = 0.52146742\n",
      "Iteration 1583, loss = 0.52130334\n",
      "Iteration 1584, loss = 0.52124764\n",
      "Iteration 1585, loss = 0.52131285\n",
      "Iteration 1586, loss = 0.52114996\n",
      "Iteration 1587, loss = 0.52109897\n",
      "Iteration 1588, loss = 0.52118532\n",
      "Iteration 1589, loss = 0.52089936\n",
      "Iteration 1590, loss = 0.52093130\n",
      "Iteration 1591, loss = 0.52074589\n",
      "Iteration 1592, loss = 0.52072578\n",
      "Iteration 1593, loss = 0.52063100\n",
      "Iteration 1594, loss = 0.52053972\n",
      "Iteration 1595, loss = 0.52043033\n",
      "Iteration 1596, loss = 0.52052590\n",
      "Iteration 1597, loss = 0.52024625\n",
      "Iteration 1598, loss = 0.52016425\n",
      "Iteration 1599, loss = 0.52021097\n",
      "Iteration 1600, loss = 0.52005866\n",
      "Iteration 1601, loss = 0.52001045\n",
      "Iteration 1602, loss = 0.52004000\n",
      "Iteration 1603, loss = 0.51985667\n",
      "Iteration 1604, loss = 0.51975392\n",
      "Iteration 1605, loss = 0.51982678\n",
      "Iteration 1606, loss = 0.51963371\n",
      "Iteration 1607, loss = 0.51950979\n",
      "Iteration 1608, loss = 0.51949271\n",
      "Iteration 1609, loss = 0.51938723\n",
      "Iteration 1610, loss = 0.51926596\n",
      "Iteration 1611, loss = 0.51942680\n",
      "Iteration 1612, loss = 0.51914464\n",
      "Iteration 1613, loss = 0.51909177\n",
      "Iteration 1614, loss = 0.51910791\n",
      "Iteration 1615, loss = 0.51907309\n",
      "Iteration 1616, loss = 0.51885875\n",
      "Iteration 1617, loss = 0.51897843\n",
      "Iteration 1618, loss = 0.51870787\n",
      "Iteration 1619, loss = 0.51881460\n",
      "Iteration 1620, loss = 0.51862637\n",
      "Iteration 1621, loss = 0.51849432\n",
      "Iteration 1622, loss = 0.51843292\n",
      "Iteration 1623, loss = 0.51840239\n",
      "Iteration 1624, loss = 0.51835968\n",
      "Iteration 1625, loss = 0.51829403\n",
      "Iteration 1626, loss = 0.51816955\n",
      "Iteration 1627, loss = 0.51804994\n",
      "Iteration 1628, loss = 0.51828478\n",
      "Iteration 1629, loss = 0.51801955\n",
      "Iteration 1630, loss = 0.51785436\n",
      "Iteration 1631, loss = 0.51786288\n",
      "Iteration 1632, loss = 0.51770113\n",
      "Iteration 1633, loss = 0.51763540\n",
      "Iteration 1634, loss = 0.51761609\n",
      "Iteration 1635, loss = 0.51749452\n",
      "Iteration 1636, loss = 0.51746716\n",
      "Iteration 1637, loss = 0.51742343\n",
      "Iteration 1638, loss = 0.51730302\n",
      "Iteration 1639, loss = 0.51724774\n",
      "Iteration 1640, loss = 0.51726157\n",
      "Iteration 1641, loss = 0.51715961\n",
      "Iteration 1642, loss = 0.51702175\n",
      "Iteration 1643, loss = 0.51707436\n",
      "Iteration 1644, loss = 0.51691824\n",
      "Iteration 1645, loss = 0.51691321\n",
      "Iteration 1646, loss = 0.51679003\n",
      "Iteration 1647, loss = 0.51664380\n",
      "Iteration 1648, loss = 0.51679935\n",
      "Iteration 1649, loss = 0.51654077\n",
      "Iteration 1650, loss = 0.51653707\n",
      "Iteration 1651, loss = 0.51659136\n",
      "Iteration 1652, loss = 0.51642246\n",
      "Iteration 1653, loss = 0.51630074\n",
      "Iteration 1654, loss = 0.51624513\n",
      "Iteration 1655, loss = 0.51633513\n",
      "Iteration 1656, loss = 0.51600753\n",
      "Iteration 1657, loss = 0.51599191\n",
      "Iteration 1658, loss = 0.51583800\n",
      "Iteration 1659, loss = 0.51578770\n",
      "Iteration 1660, loss = 0.51572202\n",
      "Iteration 1661, loss = 0.51582870\n",
      "Iteration 1662, loss = 0.51586535\n",
      "Iteration 1663, loss = 0.51559373\n",
      "Iteration 1664, loss = 0.51540282\n",
      "Iteration 1665, loss = 0.51570655\n",
      "Iteration 1666, loss = 0.51547087\n",
      "Iteration 1667, loss = 0.51534748\n",
      "Iteration 1668, loss = 0.51515248\n",
      "Iteration 1669, loss = 0.51503281\n",
      "Iteration 1670, loss = 0.51506952\n",
      "Iteration 1671, loss = 0.51493220\n",
      "Iteration 1672, loss = 0.51481841\n",
      "Iteration 1673, loss = 0.51476565\n",
      "Iteration 1674, loss = 0.51468622\n",
      "Iteration 1675, loss = 0.51470466\n",
      "Iteration 1676, loss = 0.51463177\n",
      "Iteration 1677, loss = 0.51461651\n",
      "Iteration 1678, loss = 0.51452820\n",
      "Iteration 1679, loss = 0.51436203\n",
      "Iteration 1680, loss = 0.51432646\n",
      "Iteration 1681, loss = 0.51431422\n",
      "Iteration 1682, loss = 0.51425630\n",
      "Iteration 1683, loss = 0.51429696\n",
      "Iteration 1684, loss = 0.51403694\n",
      "Iteration 1685, loss = 0.51386969\n",
      "Iteration 1686, loss = 0.51396195\n",
      "Iteration 1687, loss = 0.51378701\n",
      "Iteration 1688, loss = 0.51368864\n",
      "Iteration 1689, loss = 0.51364922\n",
      "Iteration 1690, loss = 0.51347774\n",
      "Iteration 1691, loss = 0.51341680\n",
      "Iteration 1692, loss = 0.51343176\n",
      "Iteration 1693, loss = 0.51345538\n",
      "Iteration 1694, loss = 0.51328050\n",
      "Iteration 1695, loss = 0.51318998\n",
      "Iteration 1696, loss = 0.51305755\n",
      "Iteration 1697, loss = 0.51303342\n",
      "Iteration 1698, loss = 0.51293429\n",
      "Iteration 1699, loss = 0.51293784\n",
      "Iteration 1700, loss = 0.51302140\n",
      "Iteration 1701, loss = 0.51272376\n",
      "Iteration 1702, loss = 0.51290660\n",
      "Iteration 1703, loss = 0.51264463\n",
      "Iteration 1704, loss = 0.51248650\n",
      "Iteration 1705, loss = 0.51252349\n",
      "Iteration 1706, loss = 0.51237176\n",
      "Iteration 1707, loss = 0.51229396\n",
      "Iteration 1708, loss = 0.51226633\n",
      "Iteration 1709, loss = 0.51240072\n",
      "Iteration 1710, loss = 0.51214883\n",
      "Iteration 1711, loss = 0.51201871\n",
      "Iteration 1712, loss = 0.51196325\n",
      "Iteration 1713, loss = 0.51217342\n",
      "Iteration 1714, loss = 0.51177302\n",
      "Iteration 1715, loss = 0.51176490\n",
      "Iteration 1716, loss = 0.51157394\n",
      "Iteration 1717, loss = 0.51166867\n",
      "Iteration 1718, loss = 0.51150604\n",
      "Iteration 1719, loss = 0.51143305\n",
      "Iteration 1720, loss = 0.51154967\n",
      "Iteration 1721, loss = 0.51124909\n",
      "Iteration 1722, loss = 0.51117821\n",
      "Iteration 1723, loss = 0.51119691\n",
      "Iteration 1724, loss = 0.51101951\n",
      "Iteration 1725, loss = 0.51101799\n",
      "Iteration 1726, loss = 0.51086293\n",
      "Iteration 1727, loss = 0.51084979\n",
      "Iteration 1728, loss = 0.51086858\n",
      "Iteration 1729, loss = 0.51088502\n",
      "Iteration 1730, loss = 0.51057898\n",
      "Iteration 1731, loss = 0.51049403\n",
      "Iteration 1732, loss = 0.51057913\n",
      "Iteration 1733, loss = 0.51039741\n",
      "Iteration 1734, loss = 0.51046719\n",
      "Iteration 1735, loss = 0.51036353\n",
      "Iteration 1736, loss = 0.51016355\n",
      "Iteration 1737, loss = 0.51008817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1738, loss = 0.50998966\n",
      "Iteration 1739, loss = 0.50993573\n",
      "Iteration 1740, loss = 0.50982741\n",
      "Iteration 1741, loss = 0.50976259\n",
      "Iteration 1742, loss = 0.50979928\n",
      "Iteration 1743, loss = 0.50970732\n",
      "Iteration 1744, loss = 0.50964411\n",
      "Iteration 1745, loss = 0.50958514\n",
      "Iteration 1746, loss = 0.50941142\n",
      "Iteration 1747, loss = 0.50934764\n",
      "Iteration 1748, loss = 0.50938198\n",
      "Iteration 1749, loss = 0.50931197\n",
      "Iteration 1750, loss = 0.50915747\n",
      "Iteration 1751, loss = 0.50910749\n",
      "Iteration 1752, loss = 0.50899632\n",
      "Iteration 1753, loss = 0.50890727\n",
      "Iteration 1754, loss = 0.50889933\n",
      "Iteration 1755, loss = 0.50884953\n",
      "Iteration 1756, loss = 0.50869231\n",
      "Iteration 1757, loss = 0.50870770\n",
      "Iteration 1758, loss = 0.50849296\n",
      "Iteration 1759, loss = 0.50859173\n",
      "Iteration 1760, loss = 0.50855389\n",
      "Iteration 1761, loss = 0.50831465\n",
      "Iteration 1762, loss = 0.50823515\n",
      "Iteration 1763, loss = 0.50822187\n",
      "Iteration 1764, loss = 0.50812383\n",
      "Iteration 1765, loss = 0.50804421\n",
      "Iteration 1766, loss = 0.50814309\n",
      "Iteration 1767, loss = 0.50791195\n",
      "Iteration 1768, loss = 0.50775577\n",
      "Iteration 1769, loss = 0.50768192\n",
      "Iteration 1770, loss = 0.50789500\n",
      "Iteration 1771, loss = 0.50758406\n",
      "Iteration 1772, loss = 0.50758156\n",
      "Iteration 1773, loss = 0.50747232\n",
      "Iteration 1774, loss = 0.50748446\n",
      "Iteration 1775, loss = 0.50776671\n",
      "Iteration 1776, loss = 0.50736280\n",
      "Iteration 1777, loss = 0.50725627\n",
      "Iteration 1778, loss = 0.50717966\n",
      "Iteration 1779, loss = 0.50702451\n",
      "Iteration 1780, loss = 0.50720406\n",
      "Iteration 1781, loss = 0.50685139\n",
      "Iteration 1782, loss = 0.50674178\n",
      "Iteration 1783, loss = 0.50675762\n",
      "Iteration 1784, loss = 0.50673327\n",
      "Iteration 1785, loss = 0.50663505\n",
      "Iteration 1786, loss = 0.50656905\n",
      "Iteration 1787, loss = 0.50650872\n",
      "Iteration 1788, loss = 0.50644461\n",
      "Iteration 1789, loss = 0.50632537\n",
      "Iteration 1790, loss = 0.50615398\n",
      "Iteration 1791, loss = 0.50608696\n",
      "Iteration 1792, loss = 0.50608689\n",
      "Iteration 1793, loss = 0.50589557\n",
      "Iteration 1794, loss = 0.50585510\n",
      "Iteration 1795, loss = 0.50577469\n",
      "Iteration 1796, loss = 0.50586652\n",
      "Iteration 1797, loss = 0.50560297\n",
      "Iteration 1798, loss = 0.50557805\n",
      "Iteration 1799, loss = 0.50545879\n",
      "Iteration 1800, loss = 0.50551850\n",
      "Iteration 1801, loss = 0.50537159\n",
      "Iteration 1802, loss = 0.50538419\n",
      "Iteration 1803, loss = 0.50523844\n",
      "Iteration 1804, loss = 0.50515359\n",
      "Iteration 1805, loss = 0.50515765\n",
      "Iteration 1806, loss = 0.50504034\n",
      "Iteration 1807, loss = 0.50494044\n",
      "Iteration 1808, loss = 0.50490974\n",
      "Iteration 1809, loss = 0.50486100\n",
      "Iteration 1810, loss = 0.50462958\n",
      "Iteration 1811, loss = 0.50457749\n",
      "Iteration 1812, loss = 0.50447340\n",
      "Iteration 1813, loss = 0.50442866\n",
      "Iteration 1814, loss = 0.50433632\n",
      "Iteration 1815, loss = 0.50433248\n",
      "Iteration 1816, loss = 0.50432297\n",
      "Iteration 1817, loss = 0.50423710\n",
      "Iteration 1818, loss = 0.50404547\n",
      "Iteration 1819, loss = 0.50415123\n",
      "Iteration 1820, loss = 0.50425519\n",
      "Iteration 1821, loss = 0.50389458\n",
      "Iteration 1822, loss = 0.50374450\n",
      "Iteration 1823, loss = 0.50368797\n",
      "Iteration 1824, loss = 0.50363867\n",
      "Iteration 1825, loss = 0.50360006\n",
      "Iteration 1826, loss = 0.50347466\n",
      "Iteration 1827, loss = 0.50345537\n",
      "Iteration 1828, loss = 0.50329873\n",
      "Iteration 1829, loss = 0.50329596\n",
      "Iteration 1830, loss = 0.50329491\n",
      "Iteration 1831, loss = 0.50312178\n",
      "Iteration 1832, loss = 0.50307895\n",
      "Iteration 1833, loss = 0.50302113\n",
      "Iteration 1834, loss = 0.50283071\n",
      "Iteration 1835, loss = 0.50274228\n",
      "Iteration 1836, loss = 0.50276248\n",
      "Iteration 1837, loss = 0.50266769\n",
      "Iteration 1838, loss = 0.50254538\n",
      "Iteration 1839, loss = 0.50249759\n",
      "Iteration 1840, loss = 0.50242311\n",
      "Iteration 1841, loss = 0.50242364\n",
      "Iteration 1842, loss = 0.50241123\n",
      "Iteration 1843, loss = 0.50215940\n",
      "Iteration 1844, loss = 0.50208692\n",
      "Iteration 1845, loss = 0.50201581\n",
      "Iteration 1846, loss = 0.50218442\n",
      "Iteration 1847, loss = 0.50204609\n",
      "Iteration 1848, loss = 0.50184525\n",
      "Iteration 1849, loss = 0.50179751\n",
      "Iteration 1850, loss = 0.50170446\n",
      "Iteration 1851, loss = 0.50160028\n",
      "Iteration 1852, loss = 0.50157571\n",
      "Iteration 1853, loss = 0.50137920\n",
      "Iteration 1854, loss = 0.50139641\n",
      "Iteration 1855, loss = 0.50132086\n",
      "Iteration 1856, loss = 0.50126302\n",
      "Iteration 1857, loss = 0.50117824\n",
      "Iteration 1858, loss = 0.50107094\n",
      "Iteration 1859, loss = 0.50103870\n",
      "Iteration 1860, loss = 0.50089406\n",
      "Iteration 1861, loss = 0.50093665\n",
      "Iteration 1862, loss = 0.50073233\n",
      "Iteration 1863, loss = 0.50063727\n",
      "Iteration 1864, loss = 0.50068126\n",
      "Iteration 1865, loss = 0.50062489\n",
      "Iteration 1866, loss = 0.50039949\n",
      "Iteration 1867, loss = 0.50044774\n",
      "Iteration 1868, loss = 0.50030667\n",
      "Iteration 1869, loss = 0.50033668\n",
      "Iteration 1870, loss = 0.50012261\n",
      "Iteration 1871, loss = 0.50002776\n",
      "Iteration 1872, loss = 0.50015560\n",
      "Iteration 1873, loss = 0.50024095\n",
      "Iteration 1874, loss = 0.49982137\n",
      "Iteration 1875, loss = 0.49979810\n",
      "Iteration 1876, loss = 0.49966402\n",
      "Iteration 1877, loss = 0.49953151\n",
      "Iteration 1878, loss = 0.49955497\n",
      "Iteration 1879, loss = 0.49939657\n",
      "Iteration 1880, loss = 0.49941810\n",
      "Iteration 1881, loss = 0.49927782\n",
      "Iteration 1882, loss = 0.49918640\n",
      "Iteration 1883, loss = 0.49924654\n",
      "Iteration 1884, loss = 0.49906914\n",
      "Iteration 1885, loss = 0.49920829\n",
      "Iteration 1886, loss = 0.49907021\n",
      "Iteration 1887, loss = 0.49875945\n",
      "Iteration 1888, loss = 0.49879948\n",
      "Iteration 1889, loss = 0.49867813\n",
      "Iteration 1890, loss = 0.49874059\n",
      "Iteration 1891, loss = 0.49850262\n",
      "Iteration 1892, loss = 0.49840419\n",
      "Iteration 1893, loss = 0.49829385\n",
      "Iteration 1894, loss = 0.49829909\n",
      "Iteration 1895, loss = 0.49832796\n",
      "Iteration 1896, loss = 0.49813028\n",
      "Iteration 1897, loss = 0.49806617\n",
      "Iteration 1898, loss = 0.49812366\n",
      "Iteration 1899, loss = 0.49790379\n",
      "Iteration 1900, loss = 0.49782794\n",
      "Iteration 1901, loss = 0.49771989\n",
      "Iteration 1902, loss = 0.49760346\n",
      "Iteration 1903, loss = 0.49758335\n",
      "Iteration 1904, loss = 0.49753727\n",
      "Iteration 1905, loss = 0.49742635\n",
      "Iteration 1906, loss = 0.49755702\n",
      "Iteration 1907, loss = 0.49728891\n",
      "Iteration 1908, loss = 0.49728951\n",
      "Iteration 1909, loss = 0.49708627\n",
      "Iteration 1910, loss = 0.49705033\n",
      "Iteration 1911, loss = 0.49693785\n",
      "Iteration 1912, loss = 0.49689620\n",
      "Iteration 1913, loss = 0.49682424\n",
      "Iteration 1914, loss = 0.49679073\n",
      "Iteration 1915, loss = 0.49671278\n",
      "Iteration 1916, loss = 0.49676717\n",
      "Iteration 1917, loss = 0.49662741\n",
      "Iteration 1918, loss = 0.49653470\n",
      "Iteration 1919, loss = 0.49628891\n",
      "Iteration 1920, loss = 0.49626513\n",
      "Iteration 1921, loss = 0.49614193\n",
      "Iteration 1922, loss = 0.49619428\n",
      "Iteration 1923, loss = 0.49608664\n",
      "Iteration 1924, loss = 0.49595696\n",
      "Iteration 1925, loss = 0.49597684\n",
      "Iteration 1926, loss = 0.49581860\n",
      "Iteration 1927, loss = 0.49580344\n",
      "Iteration 1928, loss = 0.49568476\n",
      "Iteration 1929, loss = 0.49564523\n",
      "Iteration 1930, loss = 0.49547714\n",
      "Iteration 1931, loss = 0.49547319\n",
      "Iteration 1932, loss = 0.49545765\n",
      "Iteration 1933, loss = 0.49527972\n",
      "Iteration 1934, loss = 0.49526259\n",
      "Iteration 1935, loss = 0.49514829\n",
      "Iteration 1936, loss = 0.49508782\n",
      "Iteration 1937, loss = 0.49503147\n",
      "Iteration 1938, loss = 0.49495169\n",
      "Iteration 1939, loss = 0.49489889\n",
      "Iteration 1940, loss = 0.49475064\n",
      "Iteration 1941, loss = 0.49467562\n",
      "Iteration 1942, loss = 0.49466547\n",
      "Iteration 1943, loss = 0.49455101\n",
      "Iteration 1944, loss = 0.49443832\n",
      "Iteration 1945, loss = 0.49448047\n",
      "Iteration 1946, loss = 0.49435450\n",
      "Iteration 1947, loss = 0.49418449\n",
      "Iteration 1948, loss = 0.49442980\n",
      "Iteration 1949, loss = 0.49410327\n",
      "Iteration 1950, loss = 0.49415055\n",
      "Iteration 1951, loss = 0.49405984\n",
      "Iteration 1952, loss = 0.49379412\n",
      "Iteration 1953, loss = 0.49372724\n",
      "Iteration 1954, loss = 0.49371718\n",
      "Iteration 1955, loss = 0.49372083\n",
      "Iteration 1956, loss = 0.49385097\n",
      "Iteration 1957, loss = 0.49355185\n",
      "Iteration 1958, loss = 0.49353953\n",
      "Iteration 1959, loss = 0.49328533\n",
      "Iteration 1960, loss = 0.49315848\n",
      "Iteration 1961, loss = 0.49315955\n",
      "Iteration 1962, loss = 0.49300409\n",
      "Iteration 1963, loss = 0.49293473\n",
      "Iteration 1964, loss = 0.49322855\n",
      "Iteration 1965, loss = 0.49278737\n",
      "Iteration 1966, loss = 0.49293648\n",
      "Iteration 1967, loss = 0.49268330\n",
      "Iteration 1968, loss = 0.49260005\n",
      "Iteration 1969, loss = 0.49254171\n",
      "Iteration 1970, loss = 0.49255117\n",
      "Iteration 1971, loss = 0.49275955\n",
      "Iteration 1972, loss = 0.49234307\n",
      "Iteration 1973, loss = 0.49225249\n",
      "Iteration 1974, loss = 0.49210963\n",
      "Iteration 1975, loss = 0.49202536\n",
      "Iteration 1976, loss = 0.49194215\n",
      "Iteration 1977, loss = 0.49191608\n",
      "Iteration 1978, loss = 0.49209522\n",
      "Iteration 1979, loss = 0.49182786\n",
      "Iteration 1980, loss = 0.49197620\n",
      "Iteration 1981, loss = 0.49163168\n",
      "Iteration 1982, loss = 0.49155434\n",
      "Iteration 1983, loss = 0.49147271\n",
      "Iteration 1984, loss = 0.49137811\n",
      "Iteration 1985, loss = 0.49129752\n",
      "Iteration 1986, loss = 0.49132069\n",
      "Iteration 1987, loss = 0.49118077\n",
      "Iteration 1988, loss = 0.49106936\n",
      "Iteration 1989, loss = 0.49108128\n",
      "Iteration 1990, loss = 0.49090500\n",
      "Iteration 1991, loss = 0.49086158\n",
      "Iteration 1992, loss = 0.49074819\n",
      "Iteration 1993, loss = 0.49073179\n",
      "Iteration 1994, loss = 0.49061311\n",
      "Iteration 1995, loss = 0.49052738\n",
      "Iteration 1996, loss = 0.49047385\n",
      "Iteration 1997, loss = 0.49033454\n",
      "Iteration 1998, loss = 0.49026480\n",
      "Iteration 1999, loss = 0.49033720\n",
      "Iteration 2000, loss = 0.49016474\n",
      "Iteration 2001, loss = 0.49012125\n",
      "Iteration 2002, loss = 0.49001027\n",
      "Iteration 2003, loss = 0.49009074\n",
      "Iteration 2004, loss = 0.48999628\n",
      "Iteration 2005, loss = 0.48974542\n",
      "Iteration 2006, loss = 0.48998290\n",
      "Iteration 2007, loss = 0.48968000\n",
      "Iteration 2008, loss = 0.48962707\n",
      "Iteration 2009, loss = 0.48970514\n",
      "Iteration 2010, loss = 0.48944107\n",
      "Iteration 2011, loss = 0.48935086\n",
      "Iteration 2012, loss = 0.48946912\n",
      "Iteration 2013, loss = 0.48914444\n",
      "Iteration 2014, loss = 0.48910880\n",
      "Iteration 2015, loss = 0.48904245\n",
      "Iteration 2016, loss = 0.48895439\n",
      "Iteration 2017, loss = 0.48884937\n",
      "Iteration 2018, loss = 0.48899075\n",
      "Iteration 2019, loss = 0.48865368\n",
      "Iteration 2020, loss = 0.48870567\n",
      "Iteration 2021, loss = 0.48851867\n",
      "Iteration 2022, loss = 0.48848966\n",
      "Iteration 2023, loss = 0.48840048\n",
      "Iteration 2024, loss = 0.48831652\n",
      "Iteration 2025, loss = 0.48839690\n",
      "Iteration 2026, loss = 0.48854642\n",
      "Iteration 2027, loss = 0.48812117\n",
      "Iteration 2028, loss = 0.48820125\n",
      "Iteration 2029, loss = 0.48786156\n",
      "Iteration 2030, loss = 0.48782449\n",
      "Iteration 2031, loss = 0.48780538\n",
      "Iteration 2032, loss = 0.48777787\n",
      "Iteration 2033, loss = 0.48764465\n",
      "Iteration 2034, loss = 0.48765523\n",
      "Iteration 2035, loss = 0.48745148\n",
      "Iteration 2036, loss = 0.48753699\n",
      "Iteration 2037, loss = 0.48733513\n",
      "Iteration 2038, loss = 0.48727346\n",
      "Iteration 2039, loss = 0.48715404\n",
      "Iteration 2040, loss = 0.48715056\n",
      "Iteration 2041, loss = 0.48715737\n",
      "Iteration 2042, loss = 0.48697474\n",
      "Iteration 2043, loss = 0.48718865\n",
      "Iteration 2044, loss = 0.48674500\n",
      "Iteration 2045, loss = 0.48679006\n",
      "Iteration 2046, loss = 0.48660281\n",
      "Iteration 2047, loss = 0.48653478\n",
      "Iteration 2048, loss = 0.48642001\n",
      "Iteration 2049, loss = 0.48642224\n",
      "Iteration 2050, loss = 0.48638835\n",
      "Iteration 2051, loss = 0.48650748\n",
      "Iteration 2052, loss = 0.48631126\n",
      "Iteration 2053, loss = 0.48606512\n",
      "Iteration 2054, loss = 0.48597827\n",
      "Iteration 2055, loss = 0.48588084\n",
      "Iteration 2056, loss = 0.48593523\n",
      "Iteration 2057, loss = 0.48573042\n",
      "Iteration 2058, loss = 0.48584405\n",
      "Iteration 2059, loss = 0.48573878\n",
      "Iteration 2060, loss = 0.48550951\n",
      "Iteration 2061, loss = 0.48542045\n",
      "Iteration 2062, loss = 0.48563937\n",
      "Iteration 2063, loss = 0.48533876\n",
      "Iteration 2064, loss = 0.48537509\n",
      "Iteration 2065, loss = 0.48520883\n",
      "Iteration 2066, loss = 0.48522456\n",
      "Iteration 2067, loss = 0.48499414\n",
      "Iteration 2068, loss = 0.48489415\n",
      "Iteration 2069, loss = 0.48500413\n",
      "Iteration 2070, loss = 0.48480082\n",
      "Iteration 2071, loss = 0.48469177\n",
      "Iteration 2072, loss = 0.48462379\n",
      "Iteration 2073, loss = 0.48449444\n",
      "Iteration 2074, loss = 0.48444943\n",
      "Iteration 2075, loss = 0.48435290\n",
      "Iteration 2076, loss = 0.48430149\n",
      "Iteration 2077, loss = 0.48421603\n",
      "Iteration 2078, loss = 0.48435322\n",
      "Iteration 2079, loss = 0.48405510\n",
      "Iteration 2080, loss = 0.48395482\n",
      "Iteration 2081, loss = 0.48388397\n",
      "Iteration 2082, loss = 0.48399675\n",
      "Iteration 2083, loss = 0.48375583\n",
      "Iteration 2084, loss = 0.48373766\n",
      "Iteration 2085, loss = 0.48373782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2086, loss = 0.48350322\n",
      "Iteration 2087, loss = 0.48344435\n",
      "Iteration 2088, loss = 0.48333493\n",
      "Iteration 2089, loss = 0.48334415\n",
      "Iteration 2090, loss = 0.48318789\n",
      "Iteration 2091, loss = 0.48326116\n",
      "Iteration 2092, loss = 0.48303632\n",
      "Iteration 2093, loss = 0.48303718\n",
      "Iteration 2094, loss = 0.48295899\n",
      "Iteration 2095, loss = 0.48280368\n",
      "Iteration 2096, loss = 0.48281628\n",
      "Iteration 2097, loss = 0.48269018\n",
      "Iteration 2098, loss = 0.48256284\n",
      "Iteration 2099, loss = 0.48286545\n",
      "Iteration 2100, loss = 0.48241258\n",
      "Iteration 2101, loss = 0.48233856\n",
      "Iteration 2102, loss = 0.48247987\n",
      "Iteration 2103, loss = 0.48220911\n",
      "Iteration 2104, loss = 0.48212976\n",
      "Iteration 2105, loss = 0.48235171\n",
      "Iteration 2106, loss = 0.48205262\n",
      "Iteration 2107, loss = 0.48195838\n",
      "Iteration 2108, loss = 0.48220376\n",
      "Iteration 2109, loss = 0.48181478\n",
      "Iteration 2110, loss = 0.48165108\n",
      "Iteration 2111, loss = 0.48154880\n",
      "Iteration 2112, loss = 0.48160204\n",
      "Iteration 2113, loss = 0.48149397\n",
      "Iteration 2114, loss = 0.48135173\n",
      "Iteration 2115, loss = 0.48135585\n",
      "Iteration 2116, loss = 0.48126814\n",
      "Iteration 2117, loss = 0.48117976\n",
      "Iteration 2118, loss = 0.48106790\n",
      "Iteration 2119, loss = 0.48095901\n",
      "Iteration 2120, loss = 0.48089516\n",
      "Iteration 2121, loss = 0.48086440\n",
      "Iteration 2122, loss = 0.48079590\n",
      "Iteration 2123, loss = 0.48065873\n",
      "Iteration 2124, loss = 0.48057061\n",
      "Iteration 2125, loss = 0.48071952\n",
      "Iteration 2126, loss = 0.48041797\n",
      "Iteration 2127, loss = 0.48037275\n",
      "Iteration 2128, loss = 0.48025290\n",
      "Iteration 2129, loss = 0.48018515\n",
      "Iteration 2130, loss = 0.48015372\n",
      "Iteration 2131, loss = 0.48018728\n",
      "Iteration 2132, loss = 0.48000833\n",
      "Iteration 2133, loss = 0.48003352\n",
      "Iteration 2134, loss = 0.47988437\n",
      "Iteration 2135, loss = 0.47977568\n",
      "Iteration 2136, loss = 0.47972248\n",
      "Iteration 2137, loss = 0.47963964\n",
      "Iteration 2138, loss = 0.47971694\n",
      "Iteration 2139, loss = 0.47953763\n",
      "Iteration 2140, loss = 0.47934782\n",
      "Iteration 2141, loss = 0.47924578\n",
      "Iteration 2142, loss = 0.47930110\n",
      "Iteration 2143, loss = 0.47912114\n",
      "Iteration 2144, loss = 0.47902233\n",
      "Iteration 2145, loss = 0.47899683\n",
      "Iteration 2146, loss = 0.47896527\n",
      "Iteration 2147, loss = 0.47887042\n",
      "Iteration 2148, loss = 0.47881258\n",
      "Iteration 2149, loss = 0.47862175\n",
      "Iteration 2150, loss = 0.47858113\n",
      "Iteration 2151, loss = 0.47850329\n",
      "Iteration 2152, loss = 0.47839158\n",
      "Iteration 2153, loss = 0.47850099\n",
      "Iteration 2154, loss = 0.47825785\n",
      "Iteration 2155, loss = 0.47814056\n",
      "Iteration 2156, loss = 0.47828370\n",
      "Iteration 2157, loss = 0.47801148\n",
      "Iteration 2158, loss = 0.47790692\n",
      "Iteration 2159, loss = 0.47790192\n",
      "Iteration 2160, loss = 0.47775707\n",
      "Iteration 2161, loss = 0.47773651\n",
      "Iteration 2162, loss = 0.47763840\n",
      "Iteration 2163, loss = 0.47763929\n",
      "Iteration 2164, loss = 0.47760675\n",
      "Iteration 2165, loss = 0.47738047\n",
      "Iteration 2166, loss = 0.47760683\n",
      "Iteration 2167, loss = 0.47728186\n",
      "Iteration 2168, loss = 0.47728387\n",
      "Iteration 2169, loss = 0.47709302\n",
      "Iteration 2170, loss = 0.47713683\n",
      "Iteration 2171, loss = 0.47705542\n",
      "Iteration 2172, loss = 0.47691150\n",
      "Iteration 2173, loss = 0.47686095\n",
      "Iteration 2174, loss = 0.47669464\n",
      "Iteration 2175, loss = 0.47673734\n",
      "Iteration 2176, loss = 0.47664538\n",
      "Iteration 2177, loss = 0.47649320\n",
      "Iteration 2178, loss = 0.47636672\n",
      "Iteration 2179, loss = 0.47646149\n",
      "Iteration 2180, loss = 0.47631204\n",
      "Iteration 2181, loss = 0.47641192\n",
      "Iteration 2182, loss = 0.47619819\n",
      "Iteration 2183, loss = 0.47612473\n",
      "Iteration 2184, loss = 0.47594080\n",
      "Iteration 2185, loss = 0.47592998\n",
      "Iteration 2186, loss = 0.47579996\n",
      "Iteration 2187, loss = 0.47575083\n",
      "Iteration 2188, loss = 0.47573903\n",
      "Iteration 2189, loss = 0.47567531\n",
      "Iteration 2190, loss = 0.47556864\n",
      "Iteration 2191, loss = 0.47539878\n",
      "Iteration 2192, loss = 0.47553564\n",
      "Iteration 2193, loss = 0.47523077\n",
      "Iteration 2194, loss = 0.47521704\n",
      "Iteration 2195, loss = 0.47507657\n",
      "Iteration 2196, loss = 0.47532110\n",
      "Iteration 2197, loss = 0.47503520\n",
      "Iteration 2198, loss = 0.47490436\n",
      "Iteration 2199, loss = 0.47496888\n",
      "Iteration 2200, loss = 0.47478195\n",
      "Iteration 2201, loss = 0.47478436\n",
      "Iteration 2202, loss = 0.47457746\n",
      "Iteration 2203, loss = 0.47450477\n",
      "Iteration 2204, loss = 0.47440470\n",
      "Iteration 2205, loss = 0.47439382\n",
      "Iteration 2206, loss = 0.47423546\n",
      "Iteration 2207, loss = 0.47418284\n",
      "Iteration 2208, loss = 0.47416846\n",
      "Iteration 2209, loss = 0.47406592\n",
      "Iteration 2210, loss = 0.47401952\n",
      "Iteration 2211, loss = 0.47383188\n",
      "Iteration 2212, loss = 0.47377064\n",
      "Iteration 2213, loss = 0.47368787\n",
      "Iteration 2214, loss = 0.47363990\n",
      "Iteration 2215, loss = 0.47368428\n",
      "Iteration 2216, loss = 0.47346814\n",
      "Iteration 2217, loss = 0.47340386\n",
      "Iteration 2218, loss = 0.47341935\n",
      "Iteration 2219, loss = 0.47334377\n",
      "Iteration 2220, loss = 0.47317708\n",
      "Iteration 2221, loss = 0.47319651\n",
      "Iteration 2222, loss = 0.47309075\n",
      "Iteration 2223, loss = 0.47294532\n",
      "Iteration 2224, loss = 0.47293549\n",
      "Iteration 2225, loss = 0.47281063\n",
      "Iteration 2226, loss = 0.47279041\n",
      "Iteration 2227, loss = 0.47281590\n",
      "Iteration 2228, loss = 0.47272931\n",
      "Iteration 2229, loss = 0.47250057\n",
      "Iteration 2230, loss = 0.47257293\n",
      "Iteration 2231, loss = 0.47234928\n",
      "Iteration 2232, loss = 0.47224313\n",
      "Iteration 2233, loss = 0.47218266\n",
      "Iteration 2234, loss = 0.47226318\n",
      "Iteration 2235, loss = 0.47203618\n",
      "Iteration 2236, loss = 0.47208613\n",
      "Iteration 2237, loss = 0.47194459\n",
      "Iteration 2238, loss = 0.47181940\n",
      "Iteration 2239, loss = 0.47195503\n",
      "Iteration 2240, loss = 0.47163800\n",
      "Iteration 2241, loss = 0.47158925\n",
      "Iteration 2242, loss = 0.47151756\n",
      "Iteration 2243, loss = 0.47148099\n",
      "Iteration 2244, loss = 0.47132767\n",
      "Iteration 2245, loss = 0.47134561\n",
      "Iteration 2246, loss = 0.47136489\n",
      "Iteration 2247, loss = 0.47121209\n",
      "Iteration 2248, loss = 0.47109520\n",
      "Iteration 2249, loss = 0.47095291\n",
      "Iteration 2250, loss = 0.47123539\n",
      "Iteration 2251, loss = 0.47094718\n",
      "Iteration 2252, loss = 0.47072932\n",
      "Iteration 2253, loss = 0.47067520\n",
      "Iteration 2254, loss = 0.47056591\n",
      "Iteration 2255, loss = 0.47053623\n",
      "Iteration 2256, loss = 0.47043414\n",
      "Iteration 2257, loss = 0.47088147\n",
      "Iteration 2258, loss = 0.47045689\n",
      "Iteration 2259, loss = 0.47039033\n",
      "Iteration 2260, loss = 0.47033567\n",
      "Iteration 2261, loss = 0.47020822\n",
      "Iteration 2262, loss = 0.46995254\n",
      "Iteration 2263, loss = 0.47008883\n",
      "Iteration 2264, loss = 0.47011171\n",
      "Iteration 2265, loss = 0.46974965\n",
      "Iteration 2266, loss = 0.46969495\n",
      "Iteration 2267, loss = 0.46970035\n",
      "Iteration 2268, loss = 0.46959586\n",
      "Iteration 2269, loss = 0.46977838\n",
      "Iteration 2270, loss = 0.46950054\n",
      "Iteration 2271, loss = 0.46920085\n",
      "Iteration 2272, loss = 0.46927876\n",
      "Iteration 2273, loss = 0.46917676\n",
      "Iteration 2274, loss = 0.46910308\n",
      "Iteration 2275, loss = 0.46908013\n",
      "Iteration 2276, loss = 0.46901130\n",
      "Iteration 2277, loss = 0.46881301\n",
      "Iteration 2278, loss = 0.46869379\n",
      "Iteration 2279, loss = 0.46867695\n",
      "Iteration 2280, loss = 0.46873954\n",
      "Iteration 2281, loss = 0.46852478\n",
      "Iteration 2282, loss = 0.46841886\n",
      "Iteration 2283, loss = 0.46857600\n",
      "Iteration 2284, loss = 0.46849308\n",
      "Iteration 2285, loss = 0.46829663\n",
      "Iteration 2286, loss = 0.46819198\n",
      "Iteration 2287, loss = 0.46811285\n",
      "Iteration 2288, loss = 0.46795536\n",
      "Iteration 2289, loss = 0.46793645\n",
      "Iteration 2290, loss = 0.46794300\n",
      "Iteration 2291, loss = 0.46782753\n",
      "Iteration 2292, loss = 0.46774538\n",
      "Iteration 2293, loss = 0.46778898\n",
      "Iteration 2294, loss = 0.46756161\n",
      "Iteration 2295, loss = 0.46746491\n",
      "Iteration 2296, loss = 0.46747204\n",
      "Iteration 2297, loss = 0.46737037\n",
      "Iteration 2298, loss = 0.46721971\n",
      "Iteration 2299, loss = 0.46718489\n",
      "Iteration 2300, loss = 0.46707111\n",
      "Iteration 2301, loss = 0.46710356\n",
      "Iteration 2302, loss = 0.46703219\n",
      "Iteration 2303, loss = 0.46692624\n",
      "Iteration 2304, loss = 0.46676048\n",
      "Iteration 2305, loss = 0.46671340\n",
      "Iteration 2306, loss = 0.46669091\n",
      "Iteration 2307, loss = 0.46652294\n",
      "Iteration 2308, loss = 0.46655340\n",
      "Iteration 2309, loss = 0.46643961\n",
      "Iteration 2310, loss = 0.46637313\n",
      "Iteration 2311, loss = 0.46644016\n",
      "Iteration 2312, loss = 0.46616405\n",
      "Iteration 2313, loss = 0.46613547\n",
      "Iteration 2314, loss = 0.46605218\n",
      "Iteration 2315, loss = 0.46601373\n",
      "Iteration 2316, loss = 0.46601831\n",
      "Iteration 2317, loss = 0.46611323\n",
      "Iteration 2318, loss = 0.46576236\n",
      "Iteration 2319, loss = 0.46574422\n",
      "Iteration 2320, loss = 0.46560592\n",
      "Iteration 2321, loss = 0.46596650\n",
      "Iteration 2322, loss = 0.46545248\n",
      "Iteration 2323, loss = 0.46547287\n",
      "Iteration 2324, loss = 0.46533596\n",
      "Iteration 2325, loss = 0.46526040\n",
      "Iteration 2326, loss = 0.46522452\n",
      "Iteration 2327, loss = 0.46507693\n",
      "Iteration 2328, loss = 0.46503528\n",
      "Iteration 2329, loss = 0.46502074\n",
      "Iteration 2330, loss = 0.46495245\n",
      "Iteration 2331, loss = 0.46523085\n",
      "Iteration 2332, loss = 0.46470388\n",
      "Iteration 2333, loss = 0.46459459\n",
      "Iteration 2334, loss = 0.46454315\n",
      "Iteration 2335, loss = 0.46442423\n",
      "Iteration 2336, loss = 0.46439883\n",
      "Iteration 2337, loss = 0.46444302\n",
      "Iteration 2338, loss = 0.46446837\n",
      "Iteration 2339, loss = 0.46427460\n",
      "Iteration 2340, loss = 0.46409230\n",
      "Iteration 2341, loss = 0.46401861\n",
      "Iteration 2342, loss = 0.46393582\n",
      "Iteration 2343, loss = 0.46406718\n",
      "Iteration 2344, loss = 0.46390278\n",
      "Iteration 2345, loss = 0.46372660\n",
      "Iteration 2346, loss = 0.46412934\n",
      "Iteration 2347, loss = 0.46426844\n",
      "Iteration 2348, loss = 0.46368094\n",
      "Iteration 2349, loss = 0.46360337\n",
      "Iteration 2350, loss = 0.46345792\n",
      "Iteration 2351, loss = 0.46330581\n",
      "Iteration 2352, loss = 0.46328706\n",
      "Iteration 2353, loss = 0.46329032\n",
      "Iteration 2354, loss = 0.46303892\n",
      "Iteration 2355, loss = 0.46296890\n",
      "Iteration 2356, loss = 0.46301692\n",
      "Iteration 2357, loss = 0.46291201\n",
      "Iteration 2358, loss = 0.46296809\n",
      "Iteration 2359, loss = 0.46269562\n",
      "Iteration 2360, loss = 0.46258355\n",
      "Iteration 2361, loss = 0.46255752\n",
      "Iteration 2362, loss = 0.46250595\n",
      "Iteration 2363, loss = 0.46239763\n",
      "Iteration 2364, loss = 0.46256009\n",
      "Iteration 2365, loss = 0.46221894\n",
      "Iteration 2366, loss = 0.46222356\n",
      "Iteration 2367, loss = 0.46212874\n",
      "Iteration 2368, loss = 0.46207977\n",
      "Iteration 2369, loss = 0.46190681\n",
      "Iteration 2370, loss = 0.46218676\n",
      "Iteration 2371, loss = 0.46198700\n",
      "Iteration 2372, loss = 0.46182689\n",
      "Iteration 2373, loss = 0.46161479\n",
      "Iteration 2374, loss = 0.46158628\n",
      "Iteration 2375, loss = 0.46148877\n",
      "Iteration 2376, loss = 0.46152553\n",
      "Iteration 2377, loss = 0.46138376\n",
      "Iteration 2378, loss = 0.46130983\n",
      "Iteration 2379, loss = 0.46127998\n",
      "Iteration 2380, loss = 0.46108934\n",
      "Iteration 2381, loss = 0.46106906\n",
      "Iteration 2382, loss = 0.46095677\n",
      "Iteration 2383, loss = 0.46099203\n",
      "Iteration 2384, loss = 0.46082288\n",
      "Iteration 2385, loss = 0.46080661\n",
      "Iteration 2386, loss = 0.46082533\n",
      "Iteration 2387, loss = 0.46056759\n",
      "Iteration 2388, loss = 0.46052416\n",
      "Iteration 2389, loss = 0.46094830\n",
      "Iteration 2390, loss = 0.46056587\n",
      "Iteration 2391, loss = 0.46038048\n",
      "Iteration 2392, loss = 0.46020130\n",
      "Iteration 2393, loss = 0.46012681\n",
      "Iteration 2394, loss = 0.46013871\n",
      "Iteration 2395, loss = 0.46009747\n",
      "Iteration 2396, loss = 0.45993996\n",
      "Iteration 2397, loss = 0.45992920\n",
      "Iteration 2398, loss = 0.45977620\n",
      "Iteration 2399, loss = 0.45978465\n",
      "Iteration 2400, loss = 0.45992165\n",
      "Iteration 2401, loss = 0.45960567\n",
      "Iteration 2402, loss = 0.45954030\n",
      "Iteration 2403, loss = 0.45947796\n",
      "Iteration 2404, loss = 0.45936480\n",
      "Iteration 2405, loss = 0.45925675\n",
      "Iteration 2406, loss = 0.45920950\n",
      "Iteration 2407, loss = 0.45911428\n",
      "Iteration 2408, loss = 0.45907981\n",
      "Iteration 2409, loss = 0.45900269\n",
      "Iteration 2410, loss = 0.45888413\n",
      "Iteration 2411, loss = 0.45923852\n",
      "Iteration 2412, loss = 0.45872544\n",
      "Iteration 2413, loss = 0.45890907\n",
      "Iteration 2414, loss = 0.45862661\n",
      "Iteration 2415, loss = 0.45857583\n",
      "Iteration 2416, loss = 0.45856008\n",
      "Iteration 2417, loss = 0.45843264\n",
      "Iteration 2418, loss = 0.45840041\n",
      "Iteration 2419, loss = 0.45838886\n",
      "Iteration 2420, loss = 0.45818749\n",
      "Iteration 2421, loss = 0.45822167\n",
      "Iteration 2422, loss = 0.45802964\n",
      "Iteration 2423, loss = 0.45796128\n",
      "Iteration 2424, loss = 0.45794137\n",
      "Iteration 2425, loss = 0.45793129\n",
      "Iteration 2426, loss = 0.45792154\n",
      "Iteration 2427, loss = 0.45771320\n",
      "Iteration 2428, loss = 0.45762865\n",
      "Iteration 2429, loss = 0.45751668\n",
      "Iteration 2430, loss = 0.45758989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2431, loss = 0.45743464\n",
      "Iteration 2432, loss = 0.45734231\n",
      "Iteration 2433, loss = 0.45727259\n",
      "Iteration 2434, loss = 0.45722455\n",
      "Iteration 2435, loss = 0.45704495\n",
      "Iteration 2436, loss = 0.45707531\n",
      "Iteration 2437, loss = 0.45711826\n",
      "Iteration 2438, loss = 0.45694075\n",
      "Iteration 2439, loss = 0.45691603\n",
      "Iteration 2440, loss = 0.45670014\n",
      "Iteration 2441, loss = 0.45665743\n",
      "Iteration 2442, loss = 0.45660073\n",
      "Iteration 2443, loss = 0.45651873\n",
      "Iteration 2444, loss = 0.45651112\n",
      "Iteration 2445, loss = 0.45652019\n",
      "Iteration 2446, loss = 0.45643244\n",
      "Iteration 2447, loss = 0.45621060\n",
      "Iteration 2448, loss = 0.45637846\n",
      "Iteration 2449, loss = 0.45608729\n",
      "Iteration 2450, loss = 0.45622874\n",
      "Iteration 2451, loss = 0.45592086\n",
      "Iteration 2452, loss = 0.45619699\n",
      "Iteration 2453, loss = 0.45582097\n",
      "Iteration 2454, loss = 0.45570862\n",
      "Iteration 2455, loss = 0.45565994\n",
      "Iteration 2456, loss = 0.45555509\n",
      "Iteration 2457, loss = 0.45553142\n",
      "Iteration 2458, loss = 0.45541133\n",
      "Iteration 2459, loss = 0.45545033\n",
      "Iteration 2460, loss = 0.45538961\n",
      "Iteration 2461, loss = 0.45521204\n",
      "Iteration 2462, loss = 0.45529539\n",
      "Iteration 2463, loss = 0.45513984\n",
      "Iteration 2464, loss = 0.45507640\n",
      "Iteration 2465, loss = 0.45490486\n",
      "Iteration 2466, loss = 0.45489522\n",
      "Iteration 2467, loss = 0.45485276\n",
      "Iteration 2468, loss = 0.45496544\n",
      "Iteration 2469, loss = 0.45472985\n",
      "Iteration 2470, loss = 0.45458137\n",
      "Iteration 2471, loss = 0.45466582\n",
      "Iteration 2472, loss = 0.45448245\n",
      "Iteration 2473, loss = 0.45433908\n",
      "Iteration 2474, loss = 0.45432381\n",
      "Iteration 2475, loss = 0.45441114\n",
      "Iteration 2476, loss = 0.45419332\n",
      "Iteration 2477, loss = 0.45473505\n",
      "Iteration 2478, loss = 0.45405888\n",
      "Iteration 2479, loss = 0.45396885\n",
      "Iteration 2480, loss = 0.45402079\n",
      "Iteration 2481, loss = 0.45380126\n",
      "Iteration 2482, loss = 0.45367426\n",
      "Iteration 2483, loss = 0.45363587\n",
      "Iteration 2484, loss = 0.45358958\n",
      "Iteration 2485, loss = 0.45352458\n",
      "Iteration 2486, loss = 0.45341107\n",
      "Iteration 2487, loss = 0.45371125\n",
      "Iteration 2488, loss = 0.45356385\n",
      "Iteration 2489, loss = 0.45322560\n",
      "Iteration 2490, loss = 0.45317960\n",
      "Iteration 2491, loss = 0.45323834\n",
      "Iteration 2492, loss = 0.45304291\n",
      "Iteration 2493, loss = 0.45285931\n",
      "Iteration 2494, loss = 0.45310242\n",
      "Iteration 2495, loss = 0.45274694\n",
      "Iteration 2496, loss = 0.45268388\n",
      "Iteration 2497, loss = 0.45261033\n",
      "Iteration 2498, loss = 0.45255292\n",
      "Iteration 2499, loss = 0.45253813\n",
      "Iteration 2500, loss = 0.45245873\n",
      "Iteration 2501, loss = 0.45265701\n",
      "Iteration 2502, loss = 0.45243013\n",
      "Iteration 2503, loss = 0.45242609\n",
      "Iteration 2504, loss = 0.45262044\n",
      "Iteration 2505, loss = 0.45216716\n",
      "Iteration 2506, loss = 0.45198160\n",
      "Iteration 2507, loss = 0.45195952\n",
      "Iteration 2508, loss = 0.45183386\n",
      "Iteration 2509, loss = 0.45180239\n",
      "Iteration 2510, loss = 0.45169924\n",
      "Iteration 2511, loss = 0.45191241\n",
      "Iteration 2512, loss = 0.45155990\n",
      "Iteration 2513, loss = 0.45143831\n",
      "Iteration 2514, loss = 0.45155911\n",
      "Iteration 2515, loss = 0.45131452\n",
      "Iteration 2516, loss = 0.45127350\n",
      "Iteration 2517, loss = 0.45122934\n",
      "Iteration 2518, loss = 0.45140444\n",
      "Iteration 2519, loss = 0.45113765\n",
      "Iteration 2520, loss = 0.45127091\n",
      "Iteration 2521, loss = 0.45089696\n",
      "Iteration 2522, loss = 0.45082328\n",
      "Iteration 2523, loss = 0.45092723\n",
      "Iteration 2524, loss = 0.45073856\n",
      "Iteration 2525, loss = 0.45069578\n",
      "Iteration 2526, loss = 0.45062104\n",
      "Iteration 2527, loss = 0.45062481\n",
      "Iteration 2528, loss = 0.45087839\n",
      "Iteration 2529, loss = 0.45045302\n",
      "Iteration 2530, loss = 0.45030859\n",
      "Iteration 2531, loss = 0.45051632\n",
      "Iteration 2532, loss = 0.45018708\n",
      "Iteration 2533, loss = 0.45007001\n",
      "Iteration 2534, loss = 0.45017475\n",
      "Iteration 2535, loss = 0.45002979\n",
      "Iteration 2536, loss = 0.45003637\n",
      "Iteration 2537, loss = 0.45006101\n",
      "Iteration 2538, loss = 0.44969956\n",
      "Iteration 2539, loss = 0.44969989\n",
      "Iteration 2540, loss = 0.44958984\n",
      "Iteration 2541, loss = 0.44949475\n",
      "Iteration 2542, loss = 0.44950454\n",
      "Iteration 2543, loss = 0.44969076\n",
      "Iteration 2544, loss = 0.44946765\n",
      "Iteration 2545, loss = 0.44917907\n",
      "Iteration 2546, loss = 0.44945741\n",
      "Iteration 2547, loss = 0.44918549\n",
      "Iteration 2548, loss = 0.44911159\n",
      "Iteration 2549, loss = 0.44894006\n",
      "Iteration 2550, loss = 0.44896256\n",
      "Iteration 2551, loss = 0.44919463\n",
      "Iteration 2552, loss = 0.44900949\n",
      "Iteration 2553, loss = 0.44883807\n",
      "Iteration 2554, loss = 0.44873379\n",
      "Iteration 2555, loss = 0.44867168\n",
      "Iteration 2556, loss = 0.44884819\n",
      "Iteration 2557, loss = 0.44850684\n",
      "Iteration 2558, loss = 0.44835346\n",
      "Iteration 2559, loss = 0.44829309\n",
      "Iteration 2560, loss = 0.44822141\n",
      "Iteration 2561, loss = 0.44827546\n",
      "Iteration 2562, loss = 0.44826775\n",
      "Iteration 2563, loss = 0.44800532\n",
      "Iteration 2564, loss = 0.44795993\n",
      "Iteration 2565, loss = 0.44787295\n",
      "Iteration 2566, loss = 0.44784164\n",
      "Iteration 2567, loss = 0.44771694\n",
      "Iteration 2568, loss = 0.44767430\n",
      "Iteration 2569, loss = 0.44757217\n",
      "Iteration 2570, loss = 0.44757487\n",
      "Iteration 2571, loss = 0.44749809\n",
      "Iteration 2572, loss = 0.44738862\n",
      "Iteration 2573, loss = 0.44768631\n",
      "Iteration 2574, loss = 0.44738771\n",
      "Iteration 2575, loss = 0.44721290\n",
      "Iteration 2576, loss = 0.44717097\n",
      "Iteration 2577, loss = 0.44735420\n",
      "Iteration 2578, loss = 0.44713195\n",
      "Iteration 2579, loss = 0.44690698\n",
      "Iteration 2580, loss = 0.44682024\n",
      "Iteration 2581, loss = 0.44675203\n",
      "Iteration 2582, loss = 0.44676303\n",
      "Iteration 2583, loss = 0.44675525\n",
      "Iteration 2584, loss = 0.44674129\n",
      "Iteration 2585, loss = 0.44646503\n",
      "Iteration 2586, loss = 0.44644398\n",
      "Iteration 2587, loss = 0.44650380\n",
      "Iteration 2588, loss = 0.44657565\n",
      "Iteration 2589, loss = 0.44623510\n",
      "Iteration 2590, loss = 0.44613200\n",
      "Iteration 2591, loss = 0.44615473\n",
      "Iteration 2592, loss = 0.44604454\n",
      "Iteration 2593, loss = 0.44597582\n",
      "Iteration 2594, loss = 0.44590751\n",
      "Iteration 2595, loss = 0.44583178\n",
      "Iteration 2596, loss = 0.44573674\n",
      "Iteration 2597, loss = 0.44576912\n",
      "Iteration 2598, loss = 0.44580066\n",
      "Iteration 2599, loss = 0.44548289\n",
      "Iteration 2600, loss = 0.44566004\n",
      "Iteration 2601, loss = 0.44540317\n",
      "Iteration 2602, loss = 0.44557694\n",
      "Iteration 2603, loss = 0.44524233\n",
      "Iteration 2604, loss = 0.44522564\n",
      "Iteration 2605, loss = 0.44504952\n",
      "Iteration 2606, loss = 0.44541327\n",
      "Iteration 2607, loss = 0.44511765\n",
      "Iteration 2608, loss = 0.44495849\n",
      "Iteration 2609, loss = 0.44479584\n",
      "Iteration 2610, loss = 0.44488668\n",
      "Iteration 2611, loss = 0.44477756\n",
      "Iteration 2612, loss = 0.44470061\n",
      "Iteration 2613, loss = 0.44461960\n",
      "Iteration 2614, loss = 0.44450089\n",
      "Iteration 2615, loss = 0.44454406\n",
      "Iteration 2616, loss = 0.44446508\n",
      "Iteration 2617, loss = 0.44434230\n",
      "Iteration 2618, loss = 0.44436290\n",
      "Iteration 2619, loss = 0.44422395\n",
      "Iteration 2620, loss = 0.44406825\n",
      "Iteration 2621, loss = 0.44411664\n",
      "Iteration 2622, loss = 0.44390379\n",
      "Iteration 2623, loss = 0.44404330\n",
      "Iteration 2624, loss = 0.44385673\n",
      "Iteration 2625, loss = 0.44377304\n",
      "Iteration 2626, loss = 0.44390658\n",
      "Iteration 2627, loss = 0.44367190\n",
      "Iteration 2628, loss = 0.44362024\n",
      "Iteration 2629, loss = 0.44353209\n",
      "Iteration 2630, loss = 0.44340414\n",
      "Iteration 2631, loss = 0.44337001\n",
      "Iteration 2632, loss = 0.44324152\n",
      "Iteration 2633, loss = 0.44335915\n",
      "Iteration 2634, loss = 0.44344409\n",
      "Iteration 2635, loss = 0.44315817\n",
      "Iteration 2636, loss = 0.44301155\n",
      "Iteration 2637, loss = 0.44301295\n",
      "Iteration 2638, loss = 0.44301110\n",
      "Iteration 2639, loss = 0.44283961\n",
      "Iteration 2640, loss = 0.44290473\n",
      "Iteration 2641, loss = 0.44271513\n",
      "Iteration 2642, loss = 0.44266768\n",
      "Iteration 2643, loss = 0.44253211\n",
      "Iteration 2644, loss = 0.44249189\n",
      "Iteration 2645, loss = 0.44244042\n",
      "Iteration 2646, loss = 0.44277028\n",
      "Iteration 2647, loss = 0.44232524\n",
      "Iteration 2648, loss = 0.44236271\n",
      "Iteration 2649, loss = 0.44215146\n",
      "Iteration 2650, loss = 0.44214237\n",
      "Iteration 2651, loss = 0.44216020\n",
      "Iteration 2652, loss = 0.44199301\n",
      "Iteration 2653, loss = 0.44193643\n",
      "Iteration 2654, loss = 0.44189533\n",
      "Iteration 2655, loss = 0.44180381\n",
      "Iteration 2656, loss = 0.44176329\n",
      "Iteration 2657, loss = 0.44177948\n",
      "Iteration 2658, loss = 0.44168913\n",
      "Iteration 2659, loss = 0.44167289\n",
      "Iteration 2660, loss = 0.44143740\n",
      "Iteration 2661, loss = 0.44134227\n",
      "Iteration 2662, loss = 0.44133033\n",
      "Iteration 2663, loss = 0.44120147\n",
      "Iteration 2664, loss = 0.44115373\n",
      "Iteration 2665, loss = 0.44109933\n",
      "Iteration 2666, loss = 0.44113485\n",
      "Iteration 2667, loss = 0.44110589\n",
      "Iteration 2668, loss = 0.44111774\n",
      "Iteration 2669, loss = 0.44099984\n",
      "Iteration 2670, loss = 0.44090803\n",
      "Iteration 2671, loss = 0.44072074\n",
      "Iteration 2672, loss = 0.44071609\n",
      "Iteration 2673, loss = 0.44061731\n",
      "Iteration 2674, loss = 0.44072612\n",
      "Iteration 2675, loss = 0.44052751\n",
      "Iteration 2676, loss = 0.44042982\n",
      "Iteration 2677, loss = 0.44032140\n",
      "Iteration 2678, loss = 0.44055572\n",
      "Iteration 2679, loss = 0.44030100\n",
      "Iteration 2680, loss = 0.44017405\n",
      "Iteration 2681, loss = 0.44007696\n",
      "Iteration 2682, loss = 0.44014663\n",
      "Iteration 2683, loss = 0.43996887\n",
      "Iteration 2684, loss = 0.44000727\n",
      "Iteration 2685, loss = 0.43993039\n",
      "Iteration 2686, loss = 0.43979131\n",
      "Iteration 2687, loss = 0.43978304\n",
      "Iteration 2688, loss = 0.43965814\n",
      "Iteration 2689, loss = 0.43985923\n",
      "Iteration 2690, loss = 0.43950654\n",
      "Iteration 2691, loss = 0.43967406\n",
      "Iteration 2692, loss = 0.43943540\n",
      "Iteration 2693, loss = 0.43937575\n",
      "Iteration 2694, loss = 0.43925260\n",
      "Iteration 2695, loss = 0.43928470\n",
      "Iteration 2696, loss = 0.43910303\n",
      "Iteration 2697, loss = 0.43903146\n",
      "Iteration 2698, loss = 0.43921037\n",
      "Iteration 2699, loss = 0.43895493\n",
      "Iteration 2700, loss = 0.43891382\n",
      "Iteration 2701, loss = 0.43873421\n",
      "Iteration 2702, loss = 0.43886542\n",
      "Iteration 2703, loss = 0.43867067\n",
      "Iteration 2704, loss = 0.43918964\n",
      "Iteration 2705, loss = 0.43943255\n",
      "Iteration 2706, loss = 0.43846283\n",
      "Iteration 2707, loss = 0.43843079\n",
      "Iteration 2708, loss = 0.43830738\n",
      "Iteration 2709, loss = 0.43843112\n",
      "Iteration 2710, loss = 0.43840545\n",
      "Iteration 2711, loss = 0.43814414\n",
      "Iteration 2712, loss = 0.43821393\n",
      "Iteration 2713, loss = 0.43798118\n",
      "Iteration 2714, loss = 0.43792683\n",
      "Iteration 2715, loss = 0.43801906\n",
      "Iteration 2716, loss = 0.43787494\n",
      "Iteration 2717, loss = 0.43789034\n",
      "Iteration 2718, loss = 0.43769852\n",
      "Iteration 2719, loss = 0.43765672\n",
      "Iteration 2720, loss = 0.43755923\n",
      "Iteration 2721, loss = 0.43771835\n",
      "Iteration 2722, loss = 0.43749104\n",
      "Iteration 2723, loss = 0.43746412\n",
      "Iteration 2724, loss = 0.43745082\n",
      "Iteration 2725, loss = 0.43757573\n",
      "Iteration 2726, loss = 0.43728811\n",
      "Iteration 2727, loss = 0.43729681\n",
      "Iteration 2728, loss = 0.43713819\n",
      "Iteration 2729, loss = 0.43710899\n",
      "Iteration 2730, loss = 0.43696765\n",
      "Iteration 2731, loss = 0.43695210\n",
      "Iteration 2732, loss = 0.43693703\n",
      "Iteration 2733, loss = 0.43694944\n",
      "Iteration 2734, loss = 0.43672609\n",
      "Iteration 2735, loss = 0.43669860\n",
      "Iteration 2736, loss = 0.43659093\n",
      "Iteration 2737, loss = 0.43672600\n",
      "Iteration 2738, loss = 0.43650676\n",
      "Iteration 2739, loss = 0.43651774\n",
      "Iteration 2740, loss = 0.43628495\n",
      "Iteration 2741, loss = 0.43629851\n",
      "Iteration 2742, loss = 0.43632775\n",
      "Iteration 2743, loss = 0.43622591\n",
      "Iteration 2744, loss = 0.43612534\n",
      "Iteration 2745, loss = 0.43594955\n",
      "Iteration 2746, loss = 0.43612305\n",
      "Iteration 2747, loss = 0.43624977\n",
      "Iteration 2748, loss = 0.43588336\n",
      "Iteration 2749, loss = 0.43599008\n",
      "Iteration 2750, loss = 0.43572367\n",
      "Iteration 2751, loss = 0.43557540\n",
      "Iteration 2752, loss = 0.43580817\n",
      "Iteration 2753, loss = 0.43547805\n",
      "Iteration 2754, loss = 0.43573047\n",
      "Iteration 2755, loss = 0.43560402\n",
      "Iteration 2756, loss = 0.43536653\n",
      "Iteration 2757, loss = 0.43540741\n",
      "Iteration 2758, loss = 0.43521780\n",
      "Iteration 2759, loss = 0.43523746\n",
      "Iteration 2760, loss = 0.43527683\n",
      "Iteration 2761, loss = 0.43506123\n",
      "Iteration 2762, loss = 0.43492091\n",
      "Iteration 2763, loss = 0.43582800\n",
      "Iteration 2764, loss = 0.43498978\n",
      "Iteration 2765, loss = 0.43480114\n",
      "Iteration 2766, loss = 0.43480176\n",
      "Iteration 2767, loss = 0.43468937\n",
      "Iteration 2768, loss = 0.43455724\n",
      "Iteration 2769, loss = 0.43459927\n",
      "Iteration 2770, loss = 0.43492886\n",
      "Iteration 2771, loss = 0.43461631\n",
      "Iteration 2772, loss = 0.43439616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2773, loss = 0.43465424\n",
      "Iteration 2774, loss = 0.43429616\n",
      "Iteration 2775, loss = 0.43418902\n",
      "Iteration 2776, loss = 0.43425014\n",
      "Iteration 2777, loss = 0.43414900\n",
      "Iteration 2778, loss = 0.43399003\n",
      "Iteration 2779, loss = 0.43405749\n",
      "Iteration 2780, loss = 0.43406084\n",
      "Iteration 2781, loss = 0.43385687\n",
      "Iteration 2782, loss = 0.43371986\n",
      "Iteration 2783, loss = 0.43371899\n",
      "Iteration 2784, loss = 0.43376401\n",
      "Iteration 2785, loss = 0.43364113\n",
      "Iteration 2786, loss = 0.43364249\n",
      "Iteration 2787, loss = 0.43385468\n",
      "Iteration 2788, loss = 0.43346785\n",
      "Iteration 2789, loss = 0.43329454\n",
      "Iteration 2790, loss = 0.43324275\n",
      "Iteration 2791, loss = 0.43323197\n",
      "Iteration 2792, loss = 0.43320611\n",
      "Iteration 2793, loss = 0.43314498\n",
      "Iteration 2794, loss = 0.43309258\n",
      "Iteration 2795, loss = 0.43297752\n",
      "Iteration 2796, loss = 0.43296077\n",
      "Iteration 2797, loss = 0.43353092\n",
      "Iteration 2798, loss = 0.43290621\n",
      "Iteration 2799, loss = 0.43295712\n",
      "Iteration 2800, loss = 0.43264058\n",
      "Iteration 2801, loss = 0.43267971\n",
      "Iteration 2802, loss = 0.43276953\n",
      "Iteration 2803, loss = 0.43294892\n",
      "Iteration 2804, loss = 0.43255414\n",
      "Iteration 2805, loss = 0.43265293\n",
      "Iteration 2806, loss = 0.43229858\n",
      "Iteration 2807, loss = 0.43240005\n",
      "Iteration 2808, loss = 0.43225553\n",
      "Iteration 2809, loss = 0.43220861\n",
      "Iteration 2810, loss = 0.43212209\n",
      "Iteration 2811, loss = 0.43205010\n",
      "Iteration 2812, loss = 0.43204765\n",
      "Iteration 2813, loss = 0.43215156\n",
      "Iteration 2814, loss = 0.43196155\n",
      "Iteration 2815, loss = 0.43176207\n",
      "Iteration 2816, loss = 0.43176483\n",
      "Iteration 2817, loss = 0.43179160\n",
      "Iteration 2818, loss = 0.43164177\n",
      "Iteration 2819, loss = 0.43162856\n",
      "Iteration 2820, loss = 0.43146429\n",
      "Iteration 2821, loss = 0.43186013\n",
      "Iteration 2822, loss = 0.43143955\n",
      "Iteration 2823, loss = 0.43129992\n",
      "Iteration 2824, loss = 0.43203115\n",
      "Iteration 2825, loss = 0.43127274\n",
      "Iteration 2826, loss = 0.43125893\n",
      "Iteration 2827, loss = 0.43117014\n",
      "Iteration 2828, loss = 0.43097006\n",
      "Iteration 2829, loss = 0.43109921\n",
      "Iteration 2830, loss = 0.43099643\n",
      "Iteration 2831, loss = 0.43092493\n",
      "Iteration 2832, loss = 0.43080390\n",
      "Iteration 2833, loss = 0.43070832\n",
      "Iteration 2834, loss = 0.43092354\n",
      "Iteration 2835, loss = 0.43075361\n",
      "Iteration 2836, loss = 0.43056875\n",
      "Iteration 2837, loss = 0.43064899\n",
      "Iteration 2838, loss = 0.43066113\n",
      "Iteration 2839, loss = 0.43034642\n",
      "Iteration 2840, loss = 0.43039413\n",
      "Iteration 2841, loss = 0.43036237\n",
      "Iteration 2842, loss = 0.43016530\n",
      "Iteration 2843, loss = 0.43034050\n",
      "Iteration 2844, loss = 0.43044750\n",
      "Iteration 2845, loss = 0.43012279\n",
      "Iteration 2846, loss = 0.42996113\n",
      "Iteration 2847, loss = 0.42988447\n",
      "Iteration 2848, loss = 0.42988287\n",
      "Iteration 2849, loss = 0.42974586\n",
      "Iteration 2850, loss = 0.42974709\n",
      "Iteration 2851, loss = 0.43003124\n",
      "Iteration 2852, loss = 0.42958961\n",
      "Iteration 2853, loss = 0.42951033\n",
      "Iteration 2854, loss = 0.42953836\n",
      "Iteration 2855, loss = 0.42958183\n",
      "Iteration 2856, loss = 0.42942691\n",
      "Iteration 2857, loss = 0.42929252\n",
      "Iteration 2858, loss = 0.42933199\n",
      "Iteration 2859, loss = 0.42931995\n",
      "Iteration 2860, loss = 0.42915654\n",
      "Iteration 2861, loss = 0.42904253\n",
      "Iteration 2862, loss = 0.42917120\n",
      "Iteration 2863, loss = 0.42891628\n",
      "Iteration 2864, loss = 0.42894413\n",
      "Iteration 2865, loss = 0.42879733\n",
      "Iteration 2866, loss = 0.42893264\n",
      "Iteration 2867, loss = 0.42874029\n",
      "Iteration 2868, loss = 0.42863198\n",
      "Iteration 2869, loss = 0.42860667\n",
      "Iteration 2870, loss = 0.42864188\n",
      "Iteration 2871, loss = 0.42849533\n",
      "Iteration 2872, loss = 0.42841227\n",
      "Iteration 2873, loss = 0.42843958\n",
      "Iteration 2874, loss = 0.42830214\n",
      "Iteration 2875, loss = 0.42831816\n",
      "Iteration 2876, loss = 0.42838068\n",
      "Iteration 2877, loss = 0.42819451\n",
      "Iteration 2878, loss = 0.42853008\n",
      "Iteration 2879, loss = 0.42804465\n",
      "Iteration 2880, loss = 0.42813531\n",
      "Iteration 2881, loss = 0.42804235\n",
      "Iteration 2882, loss = 0.42802950\n",
      "Iteration 2883, loss = 0.42778886\n",
      "Iteration 2884, loss = 0.42793847\n",
      "Iteration 2885, loss = 0.42796236\n",
      "Iteration 2886, loss = 0.42778931\n",
      "Iteration 2887, loss = 0.42765003\n",
      "Iteration 2888, loss = 0.42752981\n",
      "Iteration 2889, loss = 0.42766247\n",
      "Iteration 2890, loss = 0.42755153\n",
      "Iteration 2891, loss = 0.42769203\n",
      "Iteration 2892, loss = 0.42740019\n",
      "Iteration 2893, loss = 0.42733922\n",
      "Iteration 2894, loss = 0.42723152\n",
      "Iteration 2895, loss = 0.42711529\n",
      "Iteration 2896, loss = 0.42711669\n",
      "Iteration 2897, loss = 0.42723002\n",
      "Iteration 2898, loss = 0.42711802\n",
      "Iteration 2899, loss = 0.42692363\n",
      "Iteration 2900, loss = 0.42714647\n",
      "Iteration 2901, loss = 0.42678183\n",
      "Iteration 2902, loss = 0.42686735\n",
      "Iteration 2903, loss = 0.42702244\n",
      "Iteration 2904, loss = 0.42681632\n",
      "Iteration 2905, loss = 0.42666281\n",
      "Iteration 2906, loss = 0.42651522\n",
      "Iteration 2907, loss = 0.42650926\n",
      "Iteration 2908, loss = 0.42645177\n",
      "Iteration 2909, loss = 0.42643807\n",
      "Iteration 2910, loss = 0.42639762\n",
      "Iteration 2911, loss = 0.42626120\n",
      "Iteration 2912, loss = 0.42626523\n",
      "Iteration 2913, loss = 0.42611822\n",
      "Iteration 2914, loss = 0.42611135\n",
      "Iteration 2915, loss = 0.42628695\n",
      "Iteration 2916, loss = 0.42620617\n",
      "Iteration 2917, loss = 0.42600806\n",
      "Iteration 2918, loss = 0.42581146\n",
      "Iteration 2919, loss = 0.42578803\n",
      "Iteration 2920, loss = 0.42591601\n",
      "Iteration 2921, loss = 0.42578512\n",
      "Iteration 2922, loss = 0.42587338\n",
      "Iteration 2923, loss = 0.42562940\n",
      "Iteration 2924, loss = 0.42560136\n",
      "Iteration 2925, loss = 0.42560661\n",
      "Iteration 2926, loss = 0.42558288\n",
      "Iteration 2927, loss = 0.42560142\n",
      "Iteration 2928, loss = 0.42536542\n",
      "Iteration 2929, loss = 0.42531070\n",
      "Iteration 2930, loss = 0.42538954\n",
      "Iteration 2931, loss = 0.42538520\n",
      "Iteration 2932, loss = 0.42513746\n",
      "Iteration 2933, loss = 0.42527583\n",
      "Iteration 2934, loss = 0.42504047\n",
      "Iteration 2935, loss = 0.42495970\n",
      "Iteration 2936, loss = 0.42509508\n",
      "Iteration 2937, loss = 0.42497428\n",
      "Iteration 2938, loss = 0.42482619\n",
      "Iteration 2939, loss = 0.42473320\n",
      "Iteration 2940, loss = 0.42464823\n",
      "Iteration 2941, loss = 0.42465943\n",
      "Iteration 2942, loss = 0.42461464\n",
      "Iteration 2943, loss = 0.42466937\n",
      "Iteration 2944, loss = 0.42451393\n",
      "Iteration 2945, loss = 0.42450631\n",
      "Iteration 2946, loss = 0.42431425\n",
      "Iteration 2947, loss = 0.42436633\n",
      "Iteration 2948, loss = 0.42425627\n",
      "Iteration 2949, loss = 0.42425666\n",
      "Iteration 2950, loss = 0.42437366\n",
      "Iteration 2951, loss = 0.42430841\n",
      "Iteration 2952, loss = 0.42431636\n",
      "Iteration 2953, loss = 0.42405866\n",
      "Iteration 2954, loss = 0.42391796\n",
      "Iteration 2955, loss = 0.42388783\n",
      "Iteration 2956, loss = 0.42411621\n",
      "Iteration 2957, loss = 0.42420210\n",
      "Iteration 2958, loss = 0.42398415\n",
      "Iteration 2959, loss = 0.42383131\n",
      "Iteration 2960, loss = 0.42365821\n",
      "Iteration 2961, loss = 0.42362417\n",
      "Iteration 2962, loss = 0.42385008\n",
      "Iteration 2963, loss = 0.42360304\n",
      "Iteration 2964, loss = 0.42348726\n",
      "Iteration 2965, loss = 0.42359899\n",
      "Iteration 2966, loss = 0.42332425\n",
      "Iteration 2967, loss = 0.42326929\n",
      "Iteration 2968, loss = 0.42332087\n",
      "Iteration 2969, loss = 0.42313455\n",
      "Iteration 2970, loss = 0.42311491\n",
      "Iteration 2971, loss = 0.42304467\n",
      "Iteration 2972, loss = 0.42297846\n",
      "Iteration 2973, loss = 0.42309244\n",
      "Iteration 2974, loss = 0.42291220\n",
      "Iteration 2975, loss = 0.42288243\n",
      "Iteration 2976, loss = 0.42284493\n",
      "Iteration 2977, loss = 0.42278700\n",
      "Iteration 2978, loss = 0.42291699\n",
      "Iteration 2979, loss = 0.42266819\n",
      "Iteration 2980, loss = 0.42270814\n",
      "Iteration 2981, loss = 0.42251436\n",
      "Iteration 2982, loss = 0.42247379\n",
      "Iteration 2983, loss = 0.42269678\n",
      "Iteration 2984, loss = 0.42268578\n",
      "Iteration 2985, loss = 0.42231139\n",
      "Iteration 2986, loss = 0.42226512\n",
      "Iteration 2987, loss = 0.42229801\n",
      "Iteration 2988, loss = 0.42216329\n",
      "Iteration 2989, loss = 0.42211986\n",
      "Iteration 2990, loss = 0.42203343\n",
      "Iteration 2991, loss = 0.42198905\n",
      "Iteration 2992, loss = 0.42190662\n",
      "Iteration 2993, loss = 0.42204934\n",
      "Iteration 2994, loss = 0.42204874\n",
      "Iteration 2995, loss = 0.42179980\n",
      "Iteration 2996, loss = 0.42184821\n",
      "Iteration 2997, loss = 0.42176259\n",
      "Iteration 2998, loss = 0.42182076\n",
      "Iteration 2999, loss = 0.42160929\n",
      "Iteration 3000, loss = 0.42166397\n",
      "Iteration 3001, loss = 0.42155844\n",
      "Iteration 3002, loss = 0.42164752\n",
      "Iteration 3003, loss = 0.42146789\n",
      "Iteration 3004, loss = 0.42135951\n",
      "Iteration 3005, loss = 0.42124311\n",
      "Iteration 3006, loss = 0.42129039\n",
      "Iteration 3007, loss = 0.42120965\n",
      "Iteration 3008, loss = 0.42112232\n",
      "Iteration 3009, loss = 0.42108221\n",
      "Iteration 3010, loss = 0.42104480\n",
      "Iteration 3011, loss = 0.42122326\n",
      "Iteration 3012, loss = 0.42091742\n",
      "Iteration 3013, loss = 0.42123542\n",
      "Iteration 3014, loss = 0.42100354\n",
      "Iteration 3015, loss = 0.42080932\n",
      "Iteration 3016, loss = 0.42082603\n",
      "Iteration 3017, loss = 0.42072392\n",
      "Iteration 3018, loss = 0.42060074\n",
      "Iteration 3019, loss = 0.42072757\n",
      "Iteration 3020, loss = 0.42094047\n",
      "Iteration 3021, loss = 0.42053938\n",
      "Iteration 3022, loss = 0.42039508\n",
      "Iteration 3023, loss = 0.42046918\n",
      "Iteration 3024, loss = 0.42038451\n",
      "Iteration 3025, loss = 0.42030461\n",
      "Iteration 3026, loss = 0.42026992\n",
      "Iteration 3027, loss = 0.42058463\n",
      "Iteration 3028, loss = 0.42013234\n",
      "Iteration 3029, loss = 0.42013473\n",
      "Iteration 3030, loss = 0.42004115\n",
      "Iteration 3031, loss = 0.41997372\n",
      "Iteration 3032, loss = 0.41992619\n",
      "Iteration 3033, loss = 0.41995557\n",
      "Iteration 3034, loss = 0.41993908\n",
      "Iteration 3035, loss = 0.41992629\n",
      "Iteration 3036, loss = 0.41972875\n",
      "Iteration 3037, loss = 0.41967301\n",
      "Iteration 3038, loss = 0.41965346\n",
      "Iteration 3039, loss = 0.41958562\n",
      "Iteration 3040, loss = 0.41970520\n",
      "Iteration 3041, loss = 0.41950453\n",
      "Iteration 3042, loss = 0.41947324\n",
      "Iteration 3043, loss = 0.41936175\n",
      "Iteration 3044, loss = 0.41936674\n",
      "Iteration 3045, loss = 0.41940086\n",
      "Iteration 3046, loss = 0.41948864\n",
      "Iteration 3047, loss = 0.41926217\n",
      "Iteration 3048, loss = 0.41913278\n",
      "Iteration 3049, loss = 0.41905178\n",
      "Iteration 3050, loss = 0.41927916\n",
      "Iteration 3051, loss = 0.41919184\n",
      "Iteration 3052, loss = 0.41904840\n",
      "Iteration 3053, loss = 0.41889928\n",
      "Iteration 3054, loss = 0.41896784\n",
      "Iteration 3055, loss = 0.41892826\n",
      "Iteration 3056, loss = 0.41872761\n",
      "Iteration 3057, loss = 0.41873278\n",
      "Iteration 3058, loss = 0.41861652\n",
      "Iteration 3059, loss = 0.41862504\n",
      "Iteration 3060, loss = 0.41858086\n",
      "Iteration 3061, loss = 0.41858025\n",
      "Iteration 3062, loss = 0.41854346\n",
      "Iteration 3063, loss = 0.41841592\n",
      "Iteration 3064, loss = 0.41829042\n",
      "Iteration 3065, loss = 0.41833207\n",
      "Iteration 3066, loss = 0.41820331\n",
      "Iteration 3067, loss = 0.41854159\n",
      "Iteration 3068, loss = 0.41819437\n",
      "Iteration 3069, loss = 0.41804719\n",
      "Iteration 3070, loss = 0.41815760\n",
      "Iteration 3071, loss = 0.41802627\n",
      "Iteration 3072, loss = 0.41796467\n",
      "Iteration 3073, loss = 0.41792363\n",
      "Iteration 3074, loss = 0.41786819\n",
      "Iteration 3075, loss = 0.41783166\n",
      "Iteration 3076, loss = 0.41788366\n",
      "Iteration 3077, loss = 0.41781484\n",
      "Iteration 3078, loss = 0.41773299\n",
      "Iteration 3079, loss = 0.41757434\n",
      "Iteration 3080, loss = 0.41777647\n",
      "Iteration 3081, loss = 0.41757078\n",
      "Iteration 3082, loss = 0.41745398\n",
      "Iteration 3083, loss = 0.41739512\n",
      "Iteration 3084, loss = 0.41743387\n",
      "Iteration 3085, loss = 0.41732344\n",
      "Iteration 3086, loss = 0.41748016\n",
      "Iteration 3087, loss = 0.41740080\n",
      "Iteration 3088, loss = 0.41714934\n",
      "Iteration 3089, loss = 0.41718841\n",
      "Iteration 3090, loss = 0.41706777\n",
      "Iteration 3091, loss = 0.41705883\n",
      "Iteration 3092, loss = 0.41697109\n",
      "Iteration 3093, loss = 0.41690437\n",
      "Iteration 3094, loss = 0.41685636\n",
      "Iteration 3095, loss = 0.41688296\n",
      "Iteration 3096, loss = 0.41680315\n",
      "Iteration 3097, loss = 0.41676552\n",
      "Iteration 3098, loss = 0.41703084\n",
      "Iteration 3099, loss = 0.41675133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=10000, verbose=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural=MLPClassifier(max_iter=10000, verbose=True,tol=0.0001, activation='relu')\n",
    "rede_neural.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c11b5237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84375"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57751cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  6,  1,  0],\n",
       "       [ 6, 18,  0,  0],\n",
       "       [ 0,  0, 16,  0],\n",
       "       [ 0,  2,  0, 23]], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = rede_neural.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92e9af11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(42.0, 0.5, 'Truth')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize=(7,5))\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "59d34884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   BallFault       0.80      0.77      0.79        31\n",
      "     Healthy       0.69      0.75      0.72        24\n",
      "  InnerFault       0.94      1.00      0.97        16\n",
      "  OuterFault       1.00      0.92      0.96        25\n",
      "\n",
      "    accuracy                           0.84        96\n",
      "   macro avg       0.86      0.86      0.86        96\n",
      "weighted avg       0.85      0.84      0.85        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3428721b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Healthy'], dtype='<U10')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural.predict([[-0.12,-0.02]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7361502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = dataset_process[:200]\n",
    "df1 = dataset_process[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d34e5d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "#plt.scatter(X[:,0],X[:,2])\n",
    "plt.scatter(df0['principal component 1'], df0['Label'],color=\"green\",marker='+')\n",
    "plt.scatter(df1['principal component 1'], df1['Label'],color=\"blue\",marker='.')\n",
    "plt.ylabel('Label')\n",
    "plt.xlabel('principle component 1')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273a4df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
